<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xinyi Shen</title>
    <link>http://ssxy00.github.io/</link>
    <description>Recent content on Xinyi Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 12 Nov 2020 12:57:25 +0800</lastBuildDate>
    
	<atom:link href="http://ssxy00.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Arxiv Daily</title>
      <link>http://ssxy00.github.io/posts/arxiv-daily/</link>
      <pubDate>Thu, 12 Nov 2020 12:57:25 +0800</pubDate>
      
      <guid>http://ssxy00.github.io/posts/arxiv-daily/</guid>
      <description>最近订阅了 Arxiv cs.CL，希望能坚持每天刷一刷，了解一下 NLP 领域的最新文献。在这篇博客中我会记录每天刷到的感兴趣的工作。
​
16 Nov 20 &amp;ndash; 17 Nov 20 更新于 19 Nov 2020
Neural Semi-supervised Learning for Text Classification Under Large-Scale Pretraining
 Author: Zĳun Sun, Chun Fan, Xiaofei Sun, Yuxian Meng, Fei Wu, Jiwei Li  这篇工作研究的是：在 large-scale pre-training 广泛被使用的场景下，利用大量无标注 in-domain 语料做 semi-supervised learning 是否还能带来好处，应该选用什么样的 semi-supervised learning 算法，具体应该如何设计。
最近接连看到好几篇研究 NLP 中 semi-supervised learning 和 pre-training 做对比的 paper，之后再来读一下这一篇。
​
Where Are You? Localization from Embodied Dialog</description>
    </item>
    
  </channel>
</rss>