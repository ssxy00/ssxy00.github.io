<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Feb 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Feb 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2102/" />
<meta property="article:published_time" content="2021-03-09T18:57:54+08:00" />
<meta property="article:modified_time" content="2021-03-09T18:57:54+08:00" />


    <title>
  Arxiv Daily | Feb 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2102/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Feb 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-03-09T18:57:54&#43;08:00'>
                March 9, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              14-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 2 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="26-feb-21----1-mar-21">26 Feb 21 &ndash; 1 Mar 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.00820.pdf">Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Hung Le, Nancy F. Chen, Steven C.H. Hoi</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01075.pdf">OmniNet: Omnidirectional Representations from Transformers</a></p>
<ul>
<li><strong>Author</strong>: Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2003.01200.pdf">Natural Language Processing Advancements By Deep Learning: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Amirsina Torf, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf, Edward A. Fox</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08377.pdf">CARE: Commonsense-Aware Emotional Response Generation with Latent Concepts</a></p>
<ul>
<li><strong>Author</strong>: Peixiang Zhong, Di Wang, Pengfei Li, Chen Zhang, Hao Wang, Chunyan Miao</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.09763.pdf">Analysing the Noise Model Error for Realistic Noisy Label Data</a></p>
<ul>
<li><strong>Author</strong>: Michael A. Hedderich, Dawei Zhu, Dietrich Klakow</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="25-feb-21----26-feb-21">25 Feb 21 &ndash; 26 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.13549.pdf">Gradient-guided Loss Masking for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Xinyi Wang, Ankur Bapna, Melvin Johnson, Orhan Firat</li>
</ul>
<p>Our method has a natural intuition: good training data should update the model parameters in a similar direction as the clean data.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.13589.pdf">Evaluate On-the-job Learning Dialogue Systems and a Case Study for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Mathilde Veron, Sophie Rosset, Olivier Galibert, Guillaume Bernard</li>
</ul>
<p>On-the-job learning consists in continuously learning while being used in production, in an open environment, meaning that the system has to deal on its own with situations and elements never seen before.</p>
<p>​</p>
<p>​</p>
<h2 id="24-feb-21----25-feb-21">24 Feb 21 &ndash; 25 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.12634.pdf">Automatic Story Generation: Challenges and Attempts</a></p>
<ul>
<li><strong>Author</strong>: Amal Alabdulkarim, Siyan Li, Xiangyu Peng</li>
</ul>
<p>Survey</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.12702.pdf">LazyFormer: Self Attention with Lazy Update</a></p>
<ul>
<li><strong>Author</strong>: Chengxuan Ying, Guolin Ke, Di He, Tie-Yan Liu</li>
</ul>
<p>LazyFormer composes of multiple lazy blocks, each of which contains multiple Transformer layers. In each lazy block, the self-attention distribution is only computed once in the first layer and then is reused in all upper layers. In this way, the cost of computation could be largely saved.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.12982.pdf">A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives</a></p>
<ul>
<li><strong>Author</strong>: Nils Rethmeier, Isabelle Augenstein</li>
</ul>
<p>In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.11207.pdf">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></p>
<ul>
<li><strong>Author</strong>: Yaru Hao, Li Dong, Furu Wei, Ke Xu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.10603.pdf">Improve Variational Autoencoder for Text Generation with Discrete Latent Bottleneck</a></p>
<ul>
<li><strong>Author</strong>: Yang Zhao, Ping Yu, Suchismit Mahapatra, Qinliang Su, Changyou Chen</li>
</ul>
<p>In this paper, we propose a principled approach to alleviate this issue by applying a discretized bottleneck to enforce an implicit latent feature matching in a more compact latent space.</p>
<p>​</p>
<p>​</p>
<h2 id="23-feb-21----24-feb-21">23 Feb 21 &ndash; 24 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.12266.pdf">Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding Learning</a></p>
<ul>
<li><strong>Author</strong>: Gordon Buck, Andreas Vlachos</li>
<li><strong>Comments</strong>: EACL workshop Adapt-NLP 2021</li>
</ul>
<p>用 Meta-Learning 处理 OOV 问题</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.11972.pdf"><strong>Do Transformer Modifications Transfer Across Implementations and Applications?</strong></a></p>
<ul>
<li><strong>Author</strong>: Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel</li>
</ul>
<p>In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.10750.pdf">Lifelong Learning Dialogue Systems: Chatbots that Self-Learn On the Job</a></p>
<ul>
<li><strong>Author</strong>: Bing Liu, Sahisnu Mazumder</li>
</ul>
<p>An extended and revised version of <a href="https://www.cs.uic.edu/~liub/publications/LINC_paper_AAAI_2021_camera_ready.pdf">Lifelong and Continual Learning Dialogue Systems: Learning during Conversation</a></p>
<p>​</p>
<p>​</p>
<h2 id="22-feb-21----23-feb-21">22 Feb 21 &ndash; 23 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.11402.pdf">MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture</a></p>
<ul>
<li><strong>Author</strong>: Wancong Zhang, Ieshan Vaidya</li>
</ul>
<p>In this study, we propose MixUp methods at the Input, Manifold, and sentence embedding levels for the transformer architecture, and apply them to finetune the BERT model for a diverse set of NLU tasks.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.11403.pdf">Exploring Supervised and Unsupervised Rewards in Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Julia Ive, Zixu Wang, Marina Fomicheva, Lucia Specia</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.02727.pdf">Understanding Knowledge Distillation in Non-autoregressive Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Chunting Zhou, Jiatao Gu, Graham Neubig</li>
<li><strong>Comments</strong>: ICLR 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="19-feb-21----22-feb-21">19 Feb 21 &ndash; 22 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.10242.pdf">Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach</a></p>
<ul>
<li><strong>Author</strong>: Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, Wei Wei</li>
</ul>
<p>We propose a new framework named ENIGMA for estimating human evaluation scores based on recent advances of off-policy evaluation in reinforcement learning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.10780.pdf">Multi-View Feature Representation for Dialogue Generation with Bidirectional Distillation</a></p>
<ul>
<li><strong>Author</strong>: Shaoxiong Feng, Xuancheng Ren, Kan Li, Xu Sun</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.10934.pdf">Using Prior Knowledge to Guide BERT’s Attention in Semantic Textual Matching Tasks</a></p>
<ul>
<li><strong>Author</strong>: Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang</li>
<li><strong>Comments</strong>: WWW'21</li>
</ul>
<p>Instead of using prior knowledge to create a new training task for fine-tuning BERT, we directly inject knowledge into BERT’s multi-head attention mechanism.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.11008.pdf">On Efficient Training, Controllability and Compositional Generalization of Insertion-based Language Generators</a></p>
<ul>
<li><strong>Author</strong>: Sidi Lu, Nanyun Peng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.11090.pdf">Position Information in Transformers: An Overview</a></p>
<ul>
<li><strong>Author</strong>: Philipp Dufter, Martin Schmitt, Hinrich Schutze</li>
</ul>
<p>In this paper, we provide an overview of common methods to incorporate position information into Transformer models.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.10772.pdf">Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer</a></p>
<ul>
<li><strong>Author</strong>: Ronghang Hu, Amanpreet Singh</li>
</ul>
<p>We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to language understanding and multimodal reasoning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.12756.pdf">Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval</a></p>
<ul>
<li><strong>Author</strong>: Wenhan Xiong, Xiang Lorraine, Srinivasan Iyer, Jingfei Du, Patrick Lewis, William Wang, Yashar Mehdad, Wen-tau Yih, Sebastian Riedel, Douwe Kiela, Barlas Oğuz</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="18-feb-21----19-feb-21">18 Feb 21 &ndash; 19 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.09690.pdf">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a></p>
<ul>
<li><strong>Author</strong>: Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.09727.pdf">Learning Dynamic BERT via Trainable Gate Variables and a Bi-modal Regularizer</a></p>
<ul>
<li><strong>Author</strong>: Seohyeong Jeong, Nojun Kwak</li>
</ul>
<p>Our method shows reduced computational cost on the GLUE dataset with a minimal performance drop.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.10094.pdf">Formal Language Theory Meets Modern NLP</a></p>
<ul>
<li><strong>Author</strong>: William Merrill</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00038.pdf">Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Wenhan Xiong, Hong Wang, William Yang Wang</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.06814.pdf">Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System</a></p>
<ul>
<li><strong>Author</strong>: Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.04708.pdf">Conversational Query Rewriting with Self-supervised Learning</a></p>
<ul>
<li><strong>Author</strong>: Hang Liu, Meng Chen, Youzheng Wu, Xiaodong He, Bowen Zhou</li>
<li><strong>Comments</strong>: ICASSP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="17-feb-21----18-feb-21">17 Feb 21 &ndash; 18 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.09282.pdf">Learning to Select Context in a Hierarchical and Global Perspective for Open-domain Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Lei Shen, Haolan Zhan, Xin Shen, Yang Feng</li>
<li><strong>Comments</strong>: ICASSP 2021</li>
</ul>
<p>In this paper, we propose a novel model with hierarchical self-attention mechanism and distant supervision to not only detect relevant words and utterances in short and long distances, but also discern related information globally when decoding.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.08678.pdf">Curriculum CycleGAN for Textual Sentiment Domain Adaptation with Multiple Sources</a></p>
<ul>
<li><strong>Author</strong>: Sicheng Zhao, Yang Xiao, Jiang Guo, Xiangyu Yue, Jufeng Yang, Ravi Krishna, Pengfei Xu, Kurt Keutzer</li>
<li><strong>Comments</strong>: WWW 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13633.pdf">CoRe: An Efficient Coarse-refined Training Framework for BERT</a></p>
<ul>
<li><strong>Author</strong>: Cheng Yang, Shengnan Wang, Yuechuan Li, Chao Yang, Ming Yan, Jingqiao Zhang, Fangquan Lin</li>
</ul>
<p>Two-Stage Training</p>
<p>Experimental results show that the proposed CoRe framework can greatly reduce the training time without reducing the performance.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.15525.pdf">BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining</a></p>
<ul>
<li><strong>Author</strong>: Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, Nan Duan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.00154.pdf">DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He</li>
<li><strong>Comments</strong>: WWW 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-feb-21----17-feb-21">16 Feb 21 &ndash; 17 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.08473.pdf">COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</a></p>
<ul>
<li><strong>Author</strong>: Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, Xia Song</li>
</ul>
<p>PLM</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.08597.pdf">Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters</a></p>
<ul>
<li><strong>Author</strong>: Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, Jie Fu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.13796.pdf">TextGAIL: Generative Adversarial Imitation Learning for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Qingyang Wu, Lei Li, Zhou Yu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>We propose a generative adversarial imitation learning framework for text generation that uses large pre-trained language models to provide more reliable reward guidance.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00181.pdf">Sparse, Dense, and Attentional Representations for Text Retrieval</a></p>
<ul>
<li><strong>Author</strong>: Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins</li>
<li><strong>Comments</strong>: TACL 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-feb-21----16-feb-21">15 Feb 21 &ndash; 16 Feb 21</h2>
<p>更新于 23 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.07847.pdf">Meta Back-translation</a></p>
<ul>
<li><strong>Author</strong>: Hieu Pham, Xinyi Wang, Yiming Yang, Graham Neubig</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.08220.pdf">Non-Autoregressive Text Generation with Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, Nigel Collier</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.08357.pdf">Revisiting Language Encoding in Learning Multilingual Representations</a></p>
<ul>
<li><strong>Author</strong>: Shengjie Luo, Kaiyuan Gao, Shuxin Zheng, Guolin Ke, Di He, Liwei Wang, Tie-Yan Liu</li>
</ul>
<p>To process multilingual sentences in the model, a learnable vector is usually assigned to each language, which is called “language embedding”.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.07988.pdf">TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models</a></p>
<ul>
<li><strong>Author</strong>: Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, Ion Stoica</li>
</ul>
<p>We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-theart model-parallel methods.</p>
<p>​</p>
<p>[GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training](GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training)</p>
<ul>
<li><strong>Author</strong>: Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W. Ronny Huang, Tom Goldstein</li>
</ul>
<p>It also enables training the original Post-LN Transformer for machine translation without learning rate warmup under a wide range of learning rates and momentum coefficients.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.08322.pdf">A Cooperative Memory Network for Personalized Task-oriented Dialogue Systems with Incomplete User Profiles</a></p>
<ul>
<li><strong>Author</strong>: Jiahuan Pei, Pengjie Ren, Maarten de Rijke</li>
<li><strong>Comments</strong>: WWW &lsquo;21</li>
</ul>
<p>We propose a Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually enrich user profiles as dialogues progress and to simultaneously improve response selection based on the enriched profiles.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2003.00576.pdf">StructSum: Summarization via Structured Representations</a></p>
<ul>
<li><strong>Author</strong>: Vidhisha Balachandran, Artidoro Pagnoni, Jay Yoon Lee, Dheeraj Rajagopal, Jaime Carbonell, Yulia Tsvetkov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.11527.pdf">Memory Transformer</a></p>
<ul>
<li><strong>Author</strong>: Mikhail S. Burtsev, Yuri Kuratov, Anton Peganov, Grigory V. Sapunov</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="12-feb-21----15-feb-21">12 Feb 21 &ndash; 15 Feb 21</h2>
<p>更新于 22 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.07024.pdf">Interactive Learning from Activity Description</a></p>
<ul>
<li><strong>Author</strong>: Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dudík, Patrick Shafto</li>
</ul>
<p>We present a novel interactive learning protocol that enables training request-fulfilling agents by verbally describing their activities.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.07033.pdf">PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them</a></p>
<ul>
<li><strong>Author</strong>: Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, Sebastian Riedel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2001.09309.pdf">BERT’s output layer recognizes all hidden layers? Some Intriguing Phenomena and a simple way to boost BERT</a></p>
<ul>
<li><strong>Author</strong>: Wei-Tsung Kao, Tsung-Han Wu,  Po-Han Chi, Chun-Cheng, Hsieh Hung-Yi Lee</li>
</ul>
<p>In this paper, we found that surprisingly the output layer of BERT can reconstruct the input sentence by directly taking each layer of BERT as input, even though the output layer has never seen the input other than the final hidden layer.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2003.02645.pdf"><strong>SentenceMIM: A Latent Variable Language Model</strong></a></p>
<ul>
<li><strong>Author</strong>: Micha Livne, Kevin Swersky, David J. Fleet</li>
</ul>
<p>MIM learning encourages high mutual information between observations and latent variables, and is robust against posterior collapse.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00036.pdf">Improving Factual Consistency Between a Response and Persona Facts</a></p>
<ul>
<li><strong>Author</strong>: Mohsen Mesgar, Edwin Simpson, Iryna Gurevych</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>We propose to finetune these models by reinforcement learning and an efficient reward function that explicitly captures the consistency between a response and persona facts as well as semantic plausibility.</p>
<p>在 PersonaChat 上做了实验</p>
<p>​</p>
<p>​</p>
<h2 id="11-feb-21----12-feb-21">11 Feb 21 &ndash; 12 Feb 21</h2>
<p>更新于 22 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2008.00623.pdf">DeLighT: Deep and Light-weight Transformer</a></p>
<ul>
<li><strong>Author</strong>: Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations.</p>
<p>​</p>
<p>​</p>
<h2 id="10-feb-21----11-feb-21">10 Feb 21 &ndash; 11 Feb 21</h2>
<p>更新于 22 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.05951.pdf">Text Compression-aided Transformer Encoding</a></p>
<ul>
<li><strong>Author</strong>: Zuchao Li, Zhuosheng Zhang, Hai Zhao, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita</li>
</ul>
<h2 id="4-feb-21----5-feb-21">4 Feb 21 &ndash; 5 Feb 21</h2>
<p>更新于 22 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.03315.pdf">Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge</a></p>
<ul>
<li><strong>Author</strong>: Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, Peter Clark</li>
</ul>
<p>ARC-DA is one of the first DA datasets of natural questions that often require reasoning, and where appropriate question decompositions are not evident from the questions themselves.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.12061.pdf">An Unsupervised Sentence Embedding Method by Mutual Information Maximization</a></p>
<ul>
<li><strong>Author</strong>: Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, Lidong Bing</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.11931.pdf">Evaluating Commonsense in Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks.</p>
<p>​</p>
<p>​</p>
<h2 id="3-feb-21----4-feb-21">3 Feb 21 &ndash; 4 Feb 21</h2>
<p>更新于 10 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.02435.pdf">Converse, Focus and Guess - Towards Multi-Document Driven Dialogue</a></p>
<ul>
<li><strong>Author</strong>: Han Liu, Caixia Yuan, Xiaojie Wang, Yushu Yang, Huixing Jiang, Zhongyuan Wang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>Task - Multi-Document Driven Dialogue (MD3): an agent can guess the target document that the user is interested in by leading a dialogue.</p>
<p>Dataset - GuessMovie: contains 16,881 documents, each describing a movie, and associated 13,434 dialogues.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.02503.pdf">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli</li>
</ul>
<p>Discussion Summary</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.02574.pdf">Incremental Beam Manipulation for Natural Language Generation</a></p>
<ul>
<li><strong>Author</strong>: James Hargreaves, Andreas Vlachos, Guy Emerson</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>This paper proposes incremental beam manipulation, i.e. reranking the hypotheses in the beam during decoding instead of only at the end.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.02779.pdf">Unifying Vision-and-Language Tasks via Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal</li>
</ul>
<p>We propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.02178.pdf">IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization</a></p>
<ul>
<li><strong>Author</strong>: Wenxuan Zhou, Bill Yuchen Lin, Xiang Ren</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning by dynamically penalizing dominating principal components.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2007.00655.pdf">Knowledge-Aware Language Model Pretraining</a></p>
<ul>
<li><strong>Author</strong>: Corby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, Saurabh Tiwary</li>
</ul>
<p>In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entityextended tokenizer; and at the output, with an additional entity prediction task.</p>
<p>​</p>
<p>​</p>
<h2 id="2-feb-21----3-feb-21">2 Feb 21 &ndash; 3 Feb 21</h2>
<p>更新于 9 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2102.01951.pdf">Pitfalls of Static Language Modelling</a></p>
<ul>
<li><strong>Author</strong>: Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liška, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d’Autume, Sebastian Ruder, Dani Yogatama, Kris Cao, Tomas Kocisky, Susannah Young, Phil Blunsom</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.02096.pdf">Learning to Select External Knowledge with Multi-Scale Negative Sampling</a></p>
<ul>
<li><strong>Author</strong>: Huang He, Hua Lu, Siqi Bao, Fan Wang, Hua Wu, Zhengyu Niu, Haifeng Wang</li>
<li><strong>Comments</strong>: AAAI-21 DSTC9 Workshop</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.02191.pdf">DiSCoL: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Sarik Ghazarian, Zixi Liu, Tuhin Chakrabarty, Xuezhe Ma, Aram Galstyan, Nanyun Peng</li>
</ul>
<ol>
<li>predicting relevant and informative convlines for dialogue contexts and 2) generating high-quality responses conditioned on the predicted convlines.</li>
</ol>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2007.01282.pdf">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Gautier Izacard, Edouard Grave</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.02329.pdf">InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective</a></p>
<ul>
<li><strong>Author</strong>: Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) an Anchored Feature regularizer, which increases the mutual information between local stable features and global features.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.01672.pdf"><strong>The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</strong></a></p>
<ul>
<li><strong>Author</strong>: Sebastian Gehrmann et al. （太多了）</li>
</ul>
<p>NLG 评估</p>
<p>​</p>
<p>​</p>
<h2 id="1-feb-21----2-feb-21">1 Feb 21 &ndash; 2 Feb 21</h2>
<p>更新于 9 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.03125.pdf">Advances in Multi-turn Dialogue Comprehension: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Zhuosheng Zhang, Hai Zhao</li>
</ul>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
