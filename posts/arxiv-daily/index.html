<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily/" />
<meta property="article:published_time" content="2020-11-12T12:57:25+08:00" />
<meta property="article:modified_time" content="2020-11-12T12:57:25+08:00" />


    <title>
  Arxiv Daily · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-11-12T12:57:25&#43;08:00'>
                November 12, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              4-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>最近订阅了 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a>，希望能坚持每天刷一刷，了解一下 NLP 领域的最新文献。在这篇博客中我会记录每天刷到的感兴趣的工作。</p>
<h2 id="12-nov-20----13-nov-20">12 Nov 20 &ndash; 13 Nov 20</h2>
<p>更新于 17 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.06623.pdf">doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset</a></p>
<ul>
<li><strong>Author</strong>: Song Feng, Hui Wan, Chulaka Gunasekara, Siva Sankalp Patel, Sachindra Joshi, Luis A. Lastras</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作贡献了一个 document-grounded 对话数据集。&lsquo;The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.06868.pdf">EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</a></p>
<ul>
<li><strong>Author</strong>: Weijia Xu, Marine Carpuat</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>这篇是基于 <a href="https://arxiv.org/pdf/1905.11006.pdf">Levenshtein Transformer</a> 的改进工作，将原来的  <code>deletion</code>  操作替换为 <code>reposition</code>。相比于 Levenshtein Transformer，他们的模型可以达到更好的效果以及跟快的推理速度。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07013.pdf">Deconstructing word embedding algorithms</a></p>
<ul>
<li><strong>Author</strong>: Kian Kenyon-Dean, Edward Newell, , Jackie Chi Kit Cheung</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作对经典的 Uncontextualized word embedding 方法做了理论分析。（很短，只有 6 页）</p>
<p>​</p>
<p>​</p>
<h2 id="11-nov-20----12-nov-20">11 Nov 20 &ndash; 12 Nov 20</h2>
<p>更新于 14 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.06132.pdf">Incorporating a Local Translation Mechanism into Non-autoregressive Translation</a></p>
<ul>
<li><strong>Author</strong>: Xiang Kong, Zhisong Zhang, Eduard Hovy</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Non-autoregressive</li>
</ul>
<p>Non-autoregressive 相比于 Autoregressive 模型的好处是计算快，因为每个位置的 token 可以被并行预测。但与此同时带来的缺点是每个预测 token 的独立出的，所以整体预测结果质量较差。这篇文章对于两种模型取了折中：在每个位置预测一个片段，然后设计了一种有效的融合方式将每个位置的片段处理为最终的输出。相比于 Baseline (<a href="https://arxiv.org/pdf/2011.06132.pdf">CMLM</a>)，他们的模型用更少的迭代就达到可比或者更好的结果。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.02496.pdf">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a></p>
<ul>
<li><strong>Author</strong>: Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: BERT, Span-based dynamic convolution</li>
</ul>
<p>在 BERT 中，attention 的计算会占据很大的计算量，因为序列中的每个单词需要 attend 到整个序列。这篇工作发现，对于 attention 层的多个注意力头，其实有的头只需要去关注局部信息，因此现有的计算是存在很大的冗余的。因此，他们提出用 span-based dynamic convolution 去替代其中冗余的头，只关注局部信息。替换后的混合 attention 机制不仅提升了表现，而且减少了计算代价。</p>
<p>比较好奇是怎么发现一些注意力头冗余的，以及这个 span-based dynamic convolution 到底是怎么计算的。</p>
<p>想到了一篇相关的工作 <a href="https://arxiv.org/pdf/1911.02150.pdf">Fast Transformer Decoding: One Write-Head is All You Need</a></p>
<p>​</p>
<p>​</p>
<h2 id="10-nov-20----11-nov-20">10 Nov 20 &ndash; 11 Nov 20</h2>
<p>更新于 13 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.05864.pdf">On the Sentence Embeddings from Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Sentence Embedding, BERT, Normalizing Flow</li>
</ul>
<p>这篇文章首先观察到一个现象：预训练模型的 sentence embedding 如果不做 finetune，难以很好地概括句子的语义。他们对 BERT MLM 的预训练任务和语义相似度任务之间的关联做了理论分析，发现 BERT 总是会推导出一个非平滑的各向异性的语义空间，而这会对语义相似度任务有负面影响。为了解决这个问题，他们提出利用 Normalizing Flow 将这一各向异性的语义空间转换为各向同性的高斯分布，在各种语义相似度任务上得到显著提升。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05910.pdf">Audrey: A Personalized Open-Domain Conversational Bot</a></p>
<ul>
<li><strong>Author</strong>: Chung Hoon Hong, Yuan Liang, Sagnik Sinha Roy, Arushi Jain, Vihang Agarwal, Ryan Draves , Zhizhuo Zhou, William Chen, Yujian Liu, Martha Miracky, Lily Ge, Nikola Banovic, David Jurgens</li>
</ul>
<p>介绍了 University of Michigan 参赛 Alexa Prize 2019 的对话模型 Audrey。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.12744.pdf">Augmenting Transformers with KNN-Based Composite Memory for Dialog</a></p>
<ul>
<li><strong>Author</strong>: Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes</li>
</ul>
<p>NLP 的许多任务会受益于丰富的外部知识，例如 QA, Chatbot, Reasoning。这篇工作为对话模型设计了一个基于 KNN 的外部知识提取模块。</p>
<p><em>感兴趣的理由是在对话领域，越长的 context 和越丰富的外部知识一定会有利于更好的对话生成，但这些信息很难记忆在隐空间中，因此这种与一个知识库交互的信息提取的做法可以了解一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.01460.pdf">Situated and Interactive Multimodal Conversations</a></p>
<ul>
<li><strong>Author</strong>: Seungwhan Moon, Satwik Kottur, Paul A. Crook, Ankita De, Shivani Poddar, Theodore Levin, David Whitney, Daniel Difranco, Ahmad Beirami Eunjoon Cho, Rajen Subba, Alborz Geramifard</li>
</ul>
<p>新一代虚拟助手（尤其是客服）需要应对多模态的 context 以及做出多模态的回复。Facebook 提出了一个新的任务  Situated Interactive MultiModal Conversations (SIMMC) 来训练模型向这一目标努力。他们发布了两个购物相关的数据集。这个任务也是 <a href="https://sites.google.com/dstc.community/dstc9/home">DSTC9</a> 的赛道之一。</p>
<p>​</p>
<p>​</p>
<h2 id="9-nov-20----10-nov-20">9 Nov 20 &ndash; 10 Nov 20</h2>
<p>更新于 12 Nov 20</p>
<p><a href="https://arxiv.org/pdf/2011.04823.pdf">Language Through a Prism: A Spectral Approach for Multiscale Language Representations</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Dan Jurafsky, Noah Goodman</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>自然语言中是包含不同尺度的信息的 (character-level, word-level, document-level, \cdots)，而不同的下游任务会用到不同尺度的信息（例如文档分类会用到篇章级别的信息）。如果可以将不同尺度的信息分离出来，那么显然是对做特定尺度的任务有好处的。这篇工作借用信号处理中「光谱分析 (spectral analysis)」的做法，成功提取出特定尺度的信息，并基于此设计了一种新的模型组件: <em>prism layer</em>。<em>prism layer</em> + BERT 能更好地在 MLM (Masked Language Modeling) 任务中利用 long-range context。</p>
<p><em>题目中的 Prism 起的很有灵魂了。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.04946.pdf">When Do You Need Billions of Words of Pretraining Data?</a></p>
<ul>
<li><strong>Author</strong>: Yian Zhang, Alex Warstadt,  Haau-Sing Li, Samuel R. Bowman</li>
</ul>
<p>近几年来借助大规模预训练模型，许多工作在下游任务上频繁刷榜。预训练模型在海量无标注数据上学到了丰富的知识，而这些知识显然是有利于下游任务的学习的。但预训练模型使用的数据量通常非常庞大，因此训练起来代价是很高的。因此值得思考的是：这些预训练模型学到的语言学知识中，哪些必须要见过足够庞大的数据量才能学到，而哪些只需少量数据就可以学的很好？这篇文章设计了 4 种 Probing 任务来研究在四种数据规模上预训练的 RoBERTa。结论是大部分句法和语义的知识只需较少的数据就可以学到了，而更大的数据量可以让模型学到下游任务需要的 commense knowledge。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05268.pdf">Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: Natural language explanation, Latent variables</li>
</ul>
<p><em>看到 Interpretable 和 Latent variables 就觉得应该值得一读，摘要没有太看明白，不知道 Natural language explanation 是在做什么，之后好好读一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14975.pdf">Investigating Transferability in Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah Goodman</li>
<li><strong>Comments</strong>:  Findings of EMNLP 2020</li>
</ul>
<p>这篇工作的是 pre-trained models 迁移到 downstream tasks 的能力。具体做法是，在 transfer task 上训练 pre-trained models 时，随机将不同的层替换为初始化参数，检验替换前后模型的表现。结论是：</p>
<ul>
<li>&lsquo;layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks.&rsquo;</li>
<li>&lsquo;parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings.&rsquo;</li>
</ul>
<p><em>看摘要觉得这篇工作做的似乎挺简单的，以及 Transfer Learning 确实是个很值得研究的主题。</em></p>
<p>​</p>
<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.655.pdf">Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity</a></p>
<ul>
<li><strong>Author</strong>: Pedro Rodriguez, Paul Crook, Seungwhan Moon, Zhiguang Wang</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作围绕一个关于对话的猜想：&lsquo;engagement increases when users are presented with facts related to what they know.&rsquo; 设计了一个有标注的数据集 (规模是 14K dialogs, 181K utterances)。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09427.pdf">Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data</a></p>
<ul>
<li><strong>Author</strong>: Rongsheng Zhang, Yinhe Zheng, Jianzhi Shao, Xiaoxi Mao, Yadong Xi, Minlie Huang</li>
<li><strong>Comments</strong>: EMNLP 2020, Long paper</li>
</ul>
<p>使用 unpaired data 来增强对话模型。</p>
<p><em>最近也在考虑的问题，之后来好好读一下这篇文章的具体做法。</em></p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
