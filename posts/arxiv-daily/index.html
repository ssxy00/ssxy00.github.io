<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily/" />
<meta property="article:published_time" content="2020-11-12T12:57:25+08:00" />
<meta property="article:modified_time" content="2020-11-12T12:57:25+08:00" />


    <title>
  Arxiv Daily · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-11-12T12:57:25&#43;08:00'>
                November 12, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              14-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>最近订阅了 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a>，希望能坚持每天刷一刷，了解一下 NLP 领域的最新文献。在这篇博客中我会记录每天刷到的感兴趣的工作。</p>
<h2 id="27-nov-20----30-nov-20">27 Nov 20 &ndash; 30 Nov 20</h2>
<p>更新于 2 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2011.14244.pdf"><strong>Latent Template Induction with Gumbel-CRFs</strong></a></p>
<ul>
<li><strong>Author</strong>: Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, Alexander M. Rush</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14344.pdf">Generative Pre-training for Paraphrase Generation by Representing and Predicting Spans in Exemplars</a></p>
<ul>
<li><strong>Author</strong>: Tien-Cuong Bui, Van-Duc Le, Hai-Thien To, Sang Kyun Cha</li>
<li><strong>Comments</strong>: IEEE International Conference on Big Data and Smart Computing 2021</li>
<li><strong>Keywords</strong>: masking, predicting</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14459.pdf">Improved Semantic Role Labeling using Parameterized Neighborhood Memory Adaptation</a></p>
<ul>
<li><strong>Author</strong>: Ishan Jindala, Ranit Aharonova, Siddhartha Brahmab, Huaiyu Zhua, Yunyao Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14496.pdf">Blind signal decomposition of various word embeddings based on join and individual variance explained (JIVE)</a></p>
<ul>
<li><strong>Author</strong>: Yikai Wang, Weijian Li</li>
</ul>
<p>&lsquo;In this paper, we propose to use a novel joint signal separation method - JIVE to jointly decompose various trained word embeddings into joint and individual components.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14608.pdf">Dynamic Curriculum Learning for Low-Resource Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Chen Xu, Bojie Hu, Yufan Jiang, Kai Feng, Zeyang Wang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>这篇工作设计了一种 Curriculum Learning 的方法在 low-resource MT 任务上取得了不错的效果。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14874.pdf">A Simple and Effective Approach to Robust Unsupervised Bilingual Dictionary Induction</a></p>
<ul>
<li><strong>Author</strong>: Yanyang Li, Yingfeng Luo, Ye Lin, Quan Du, Huizhen Wang, Shujian Huang, Tong Xiao, Jingbo Zhu</li>
<li><strong>Comments</strong>: COLING 2020</li>
<li><strong>Keywords</strong>: Unsupervised Bilingual Dictionary Induction, distant language pairs</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.15124.pdf">Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs</a></p>
<ul>
<li><strong>Author</strong>: Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, Desmond Elliott</li>
</ul>
<p>这篇工作为 vision BERT 和 language BERT 提供了一个统一的研究框架。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.14203.pdf">EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP</a></p>
<ul>
<li><strong>Author</strong>: Thierry Tambe, Coleman Hooper, Lillian Pentecost, En-Yu Yang, Marco Donato, Victor Sanh, Alexander M. Rush, David Brooks, Gu-Yeon Wei</li>
</ul>
<p>BERT 加速</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.03705.pdf">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, Xiang Ren</li>
<li><strong>Comments</strong>: EMNLP Findings 2020</li>
</ul>
<p>这篇工作提出了一个关注 commonsense reasoning 的文本生成任务和数据集：给定若干个 concepts (e.g., {dog, frisbee, catch, throw})，要求模型生成一个符合现实场景的句子 (e.g., “a man throws a frisbee and his dog catches it”)。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2002.07845.pdf">Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths</a></p>
<ul>
<li><strong>Author</strong>: Ritesh Sarkhel, Moniba Keymanesh, Arnab Nandi, Srinivasan Parthasarathy</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00932.pdf">Improving Non-autoregressive Neural Machine Translation with Monolingual Data</a></p>
<ul>
<li><strong>Author</strong>: Jiawei Zhou, Phillip Keung</li>
<li><strong>Comments</strong>: ACL 2020</li>
</ul>
<p>&lsquo;We leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.11538.pdf">Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training</a></p>
<ul>
<li><strong>Author</strong>: Hai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou Ng, Lidong Bing</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: pre-trained LM, unsupervised domain adaptation, self-training, class-aware feature self-distillation</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12872.pdf">Learning TO DECEIVE KNOWLEDGE GRAPH AUGMENTED MODELS VIA TARGETED PERTURBATION</a></p>
<ul>
<li><strong>Author</strong>: Mrigank Raman, Siddhant Agarwal, Peifeng Wang, Aaron Chan, Hansen Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren</li>
</ul>
<p>KG-augmented models 是否真正利用了 KG 中的信息？</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.00678.pdf">Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Shuhao Gu, Yang Feng</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>&lsquo;Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters (neurons). '</p>
<p>​</p>
<p>​</p>
<h2 id="25-nov-20----27-nov-20">25 Nov 20 &ndash; 27 Nov 20</h2>
<p>更新于 1 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2011.13137.pdf">Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction</a></p>
<ul>
<li><strong>Author</strong>: Yifan Gao, Henghui Zhu, Patrick Ng, Cicero Nogueira dos Santos, Zhiguo Wang, Feng Nan, Dejiao Zhang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang</li>
<li><strong>Keywords</strong>: QA, ambiguous question, evdience fusion, round-trip prediction</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13200.pdf">Unsupervised Word Translation Pairing using Refinement based Point Set Registration</a></p>
<ul>
<li><strong>Author</strong>: Silviu Oprea, Sourav Dutta, Haytham Assem</li>
<li><strong>Keywords</strong>: unsupervised word translation, adversarial initialization, refinement procedure , point set registration algorithm</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13231.pdf">NLPStatTest: A Toolkit for Comparing NLP System Performance</a></p>
<ul>
<li><strong>Author</strong>: Haotian Zhu, Denise Mak, Jesse Gioannini, Fei Xia</li>
<li><strong>Comments</strong>: AACL-IJCNLP 2020</li>
<li><strong>Keywords</strong>: NLP system comparsion toolkit</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13477.pdf">Decoding and Diversity in Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Nicholas Roberts, Davis Liang, Graham Neubig, Zachary C. Lipton</li>
<li><strong>Comments</strong>: Resistance AI Workshop, NeurIPS 2020</li>
</ul>
<p><em>&lsquo;Our study implicates search as a salient source of known bias when translating gender pronouns.'</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13527.pdf"><strong>TaylorGAN: Neighbor-Augmented Policy Update for Sample-Efficient Natural Language Generation</strong></a></p>
<ul>
<li><strong>Author</strong>: Chun-Hsing, Lin Siang-Ruei, Wu Hung-Yi Lee, Yun-Nung Chen</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: NLG, REINFORCE, sample efficiency, first-order Taylor expansion</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13534.pdf">A Survey of Deep Learning Approaches for OCR and Document Understanding</a></p>
<ul>
<li><strong>Author</strong>: Nishant Subramani, Alexandre Matton, Malcolm Greaves, Adrian Lam</li>
<li><strong>Comments</strong>: ML-RSA Workshop, NeurIPS 2020</li>
</ul>
<p><em>不太了解这个方向，有缘再读</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13633.pdf">CoRe: An Efficient Coarse-refined Training Framework for BERT</a></p>
<ul>
<li><strong>Author</strong>: Cheng Yang, Shengnan Wang, Yuechuan Li, Chao Yang, Ming Yan, Jingqiao Zhang, Fangquan Lin</li>
</ul>
<p>这篇工作关注的是降低 BERT 预训练的代价。他们提出的训练框架 CoRe 在训练初期通过 fast attention mechanism 和 decomposing the large parameters 用更短的时间来得到一个较好的参数初始化，然后再恢复为标准的 BERT 继续训练。他们的训练框架在大大缩短了训练时间的同时结果不降。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.13635.pdf">Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup</a></p>
<ul>
<li><strong>Author</strong>: Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, Jingqiao Zhang</li>
</ul>
<p>另一篇关注 BERT 预训练效率的文章。这篇文章提出一种多阶段训练 BERT 的方式，开始只从较少的层数训练，而后逐渐加深层数（靠近输出端），并且每次只训练新加的层，之前的层参数固定。这一方法可以达到超过 110% 的加速，同时模型效果方面没有显著的下降。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.12086.pdf">Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer</a></p>
<ul>
<li><strong>Author</strong>: Joosung Lee</li>
<li><strong>Comments</strong>: INLG 2020</li>
</ul>
<p>Two-stage 文本风格迁移：第一阶段删除句子中属性相关的词，第二阶段结合第一阶段的删除结果和目标属性生成风格迁移后的句子。</p>
<p>​</p>
<p>​</p>
<h2 id="24-nov-20----25-nov-20">24 Nov 20 &ndash; 25 Nov 20</h2>
<p>更新于 27 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.12334.pdf">Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced Monte-Carlo Approach</a></p>
<ul>
<li><strong>Author</strong>: Maosen Zhang, Nan Jiang, Lei Li, Yexiang Xue</li>
<li><strong>Keywords</strong>: controllable text generation, Pretrained-LM, Markov chain Monte Carlo (MCMC)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.12771.pdf">Learning to Expand: Reinforced Pseudo-relevance Feedback Selection for Information-seeking Conversations</a></p>
<ul>
<li><strong>Author</strong>: Haojie Pan, Cen Chen, Minghui Qiu, Liu Yang, Feng Ji, Jun Huang, Haiqing Chen</li>
<li><strong>Keywords</strong>: Pseudo-relevance feedback selection, Reinforcement learning, BERT, Info-seeking conversations</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="23-nov-20----24-nov-20">23 Nov 20 &ndash; 24 Nov 20</h2>
<p>更新于 26 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.11928.pdf">GLGE: A New General Language Generation Evaluation Benchmark</a></p>
<ul>
<li><strong>Author</strong>: Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong, Pengcheng Wang, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Ruofei Zhang, Winnie Wu, Ming Zhou, Nan Duan</li>
</ul>
<p>这篇工作提出了一个针对 Natural Language Generation (NLG) 的 benchmark，覆盖 8 个任务和 3 个难度级别，共 24 个子任务。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.08178.pdf">Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</a></p>
<ul>
<li><strong>Author</strong>: Yekun Chai, Shuo Jin, Xinwen Hou</li>
<li><strong>Comments</strong>: ACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09708.pdf">Towards Empathetic Dialogue Generation over Multi-type Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Qintong Li, Piji Li, Zhumin Chen, Zhaochun Ren</li>
<li><strong>Keywords</strong>: Empathetic dialogue generation, emotional context graph, cross-attention</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.05345.pdf">Do Language Embeddings Capture Scales?</a></p>
<ul>
<li><strong>Author</strong>: Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth</li>
<li><strong>Comments</strong>: EMNLP Findings 2020</li>
<li><strong>Keywords</strong>: PLM, scalar magnitude, probing</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="20-nov-20----23-nov-20">20 Nov 20 &ndash; 23 Nov 20</h2>
<p>更新于 25 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.10731.pdf">LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan Tur</li>
<li><strong>Comments</strong>: NeurIPS KR2ML 2020</li>
</ul>
<p>这篇文章关注的是主流的 VQA 模型的工作模式缺乏可解释性。他们提出 [[Look, Read, Think, Answer] 的推理框架，让模型按照人类的思考方式，一步一步解决问题，并在每一步给出人类可理解的推理依据。他们的模型在 GQA 数据集上大幅度超过 SOTA。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.10683.pdf">Athena: Constructing Dialogues Dynamically with Discourse Constraints</a></p>
<ul>
<li><strong>Author</strong>: Vrindavan Harrison, Juraj Juraska, Wen Cui, Lena Reed, Kevin K. Bowden, Jiaqi Wu, Brian Schwarzmann, Abteen Ebrahimi, Rishi Rajasekaran, Nikhil Varghese, Max Wechsler-Azen, Steve Whittaker, Jeffrey Flanigan, Marilyn Walker</li>
</ul>
<p>University of California 在 Alexa Prize 2019 的参赛模型。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.11499.pdf">Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model</a></p>
<ul>
<li><strong>Author</strong>: Juntao Li, Ruidan He, Hai Ye, Hwee Tou Ng, Lidong Bing, Rui Yan</li>
<li><strong>Comments</strong>:  IJCAI-PRICAI2020</li>
</ul>
<p>这篇工作研究的是跨语言预训练模型在跨领域 (cross-lingual and cross-domain, CLCD) 场景下的使用。他们提出了一种无监督的特征分解方法，将预训练模型生成的特征表示分解为领域相关 (domain-specific) 和领域无关 (domain-invariant) 的部分。实验表明，他们提出的方法在 CLCD 场景下显著超过 SOTA。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.06136.pdf">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a></p>
<ul>
<li><strong>Author</strong>: Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>KE + PLM</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.08698.pdf">EmpDG: Multi-resolution Interactive Empathetic Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Qintong Li, Hongshen Chen, Zhaochun Ren, Pengjie Ren, Zhaopeng Tu, Zhumin Chen</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>这篇工作关注的问题是 empathetic dialogue generation，同时关注 dialogue-level 和 token-level 的 emotion，并通过 adversarial learning 的框架利用用户反馈来训练模型。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2001.11453.pdf">Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages</a></p>
<ul>
<li><strong>Author</strong>: Edoardo M. Ponti, Ivan Vulić, Ryan Cotterell, Marinela Parović, Roi Reichart, Anna Korhonen</li>
</ul>
<p>NLP 中有很多常用的任务以及很多需要研究的语言，而其中一些语言和任务的组合是缺乏数据的，所以很自然想到的是：假设我们有英语 NER 的数据和德语做 POS tagging 的数据，能不能得到一个做德语 NER 任务的 zero-shot 模型？这篇文章提出了一种针对参数空间的贝叶斯生成模型，借助 latent variable 来表示语言和任务。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.13310.pdf">Self-Attention with Cross-Lingual Position Representation</a></p>
<ul>
<li><strong>Author</strong>: Liang Ding, Longyue Wang, Dacheng Tao</li>
<li><strong>Comments</strong>: ACL 2020</li>
<li><strong>Keywords</strong>: Self-Attention, Position Encoding, Cross-lingual, Bracketing transduction grammar</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1908.05787.pdf">Integrating Multimodal Information in Large Pretrained Transformers</a></p>
<ul>
<li><strong>Author</strong>: Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, Ehsan Hoque</li>
</ul>
<p>这篇工作为 BERT 和 XLNet 设计了一个挂件：Multimodal Adaptation Gate (MAG)。MAG 使得 BERT 和 XLNet 在 fine-tuning 过程中可以接受多模态输入。在 CMU-MOSI 数据集上，MAG-XLNet 首次达到人类级别的多模态情感分析水平。</p>
<p>​</p>
<p>​</p>
<h2 id="19-nov-20----20-nov-20">19 Nov 20 &ndash; 20 Nov 20</h2>
<p>更新于 24 Nov 2020</p>
<p>今天没看到感兴趣的 paper。</p>
<p>​</p>
<p>​</p>
<h2 id="18-nov-20----19-nov-20">18 Nov 20 &ndash; 19 Nov 20</h2>
<p>更新于 23 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.09708.pdf">Are Pre-trained Language Models Knowledgeable to Ground Open Domain Dialogues?</a></p>
<ul>
<li><strong>Author</strong>: Yufan Zhao, Wei Wu, Can Xu</li>
</ul>
<p>这篇工作研究了一个问题：对于基于预训练模型的 open-domain knowledge grounded chatbot，参数中储存的知识是否足够呢，或是外部知识库是不可缺少的呢？他们在几个知识型的对话数据集上做微调，发现可以达到比依赖外部知识库的 SOTA 模型达到更好的性能。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.03895.pdf">A Bilingual Generative Transformer for Semantic Sentence Embedding</a></p>
<ul>
<li><strong>Author</strong>: John Wieting, Graham Neubig, Taylor Berg-Kirkpatrick</li>
<li><strong>Comments</strong>: EMNLP 2020 long paper</li>
<li><strong>Keywords</strong>: sentence embedding, bilingual data, latent variable</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1912.13283.pdf">oLMpics - On what Language Model Pre-training Captures</a></p>
<ul>
<li>
<p><strong>Author</strong>: Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant</p>
</li>
<li>
<p><strong>Comments</strong>: TACL 2020</p>
</li>
</ul>
<p>这篇工作设计了 8 个推理任务，来研究预训练模型到底学会了什么。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.05516.pdf">When Does Unsupervised Machine Translation Work?</a></p>
<ul>
<li><strong>Author</strong>: Kelly Marchisio, Kevin Duh, Philipp Koehn</li>
<li><strong>Comments</strong>: WMT20</li>
</ul>
<p>尽管 Unsupervised MT 有许多成功的例子，但我们并不知道这种方式成功和场景需要满足什么条件。这篇工作利用不相似的语言、不相似的领域和多样性的数据集来检测评估 Unsupervised MT systems，指出它们会在何种情形下失败，以及何种范式更有研究前景。</p>
<p>​</p>
<p>​</p>
<h2 id="17-nov-20----18-nov-20">17 Nov 20 &ndash; 18 Nov 20</h2>
<p>更新于 20 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.08906.pdf">Gunrock 2.0: A User Adaptive Social Conversational System</a></p>
<ul>
<li><strong>Author</strong>: Kai-Hui Liang, Austin Chau, Yu Li, Xueyuan Lu, Dian Yu, Mingyang Zhou, Ishan Jain, Sam Davidson, Josh Arnold, Minh Nguyen, Zhou Yu</li>
</ul>
<p>University of California 在 Alexa Prize 2020 的参赛模型。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.09039.pdf">Sequence-Level Mixed Sample Data Augmentation</a></p>
<ul>
<li><strong>Author</strong>: Demi Guo, Yoon Kim, Alexander M. Rush</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Data Augmentation</li>
</ul>
<p>这篇工作提出了一种针对 seq2seq 模型的数据增强方法。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.09140.pdf">Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs</a></p>
<ul>
<li><strong>Author</strong>: Shogo Fujita, Tomohide Shibata, Manabu Okumura</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>在社区问答 (Community Question Answering, CQA) 平台上，一个问题通常有许多个回答，而用户遍历这些回答是很花时间的。已有的工作研究如何将这些回答排序，但用户仍需仔细阅读 Top-N 的回答（并且仍有可能遗漏有用的信息）。这篇文章设计了一个新的任务：从回答集合中选择出多样性且非冗余的若干回答提供给用户。他们的做法利用了 <strong>determinantal point processes (DPPs)</strong>。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.09378.pdf"><strong>LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue Policy Optimization</strong></a></p>
<ul>
<li><strong>Author</strong>: Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-chin Lin, Marco Moresi, Carel van Niekerk, Milica Gašić</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>VAE 的隐空间作为 RL 中的动作空间，并设计了三种辅助任务来指导动作空间分布的优化。</p>
<p><em>VAE + RL 我觉得是很好的做法，之后来好好读一下。</em></p>
<p>​</p>
<p>​</p>
<h2 id="16-nov-20----17-nov-20">16 Nov 20 &ndash; 17 Nov 20</h2>
<p>更新于 19 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.08626.pdf">Neural Semi-supervised Learning for Text Classification Under Large-Scale Pretraining</a></p>
<ul>
<li><strong>Author</strong>: Zĳun Sun, Chun Fan, Xiaofei Sun, Yuxian Meng, Fei Wu,  Jiwei Li</li>
</ul>
<p>这篇工作研究的是：在 large-scale pre-training 广泛被使用的场景下，利用大量无标注 in-domain 语料做 semi-supervised learning 是否还能带来好处，应该选用什么样的 semi-supervised learning 算法，具体应该如何设计。</p>
<p><em>最近接连看到好几篇研究 NLP 中 semi-supervised learning 和 pre-training 做对比的 paper，之后再来读一下这一篇。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.08277.pdf">Where Are You? Localization from Embodied Dialog</a></p>
<ul>
<li><strong>Author</strong>: Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, Peter Anderson</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作贡献了一个数据集 WAY (~6k dialogs)：two humans – an Observer and a Locator – complete a cooperative localization task.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.08558.pdf">Generating universal language adversarial examples by understanding and enhancing the transferability across neural models</a></p>
<ul>
<li><strong>Author</strong>: Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang</li>
<li><strong>Comments</strong>: ICLR 2021 (under review)</li>
<li><strong>Keywords</strong>: adversarial attack</li>
</ul>
<p>这篇工作研究了对抗样本的迁移能力。他们在 text classification 这一任务下研究模型的不同特性（模型结构、输入形式等）是如何影响对抗样本的通用性的，并基于此设计了一种 universal black-box attack 算法，可以成功攻击几乎所有已有的模型。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2002.03079.pdf">Blank Language Models</a></p>
<ul>
<li><strong>Author</strong>: Tianxiao Shen, Victor Quach, Regina Barzilay,  Tommi Jaakkola</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作设计了一种新的序列生成方法：不断生成空格并填充空格。</p>
<p><em>这种 non-autoregressive 的生成方式都比较有意思，不过之前应该有一些类似的工作，之后看一下创新点在哪里。以及文章中提到 BLM 在 ancient text restoration 上有很好的表现。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.11546.pdf">Generative Data Augmentation for Commonsense Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey</li>
<li><strong>Comments</strong>: Findings of EMNLP 2020</li>
<li><strong>Keywords</strong>: Commonsense Reasoning, Data Augmentation</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="13-nov-20----16-nov-20">13 Nov 20 &ndash; 16 Nov 20</h2>
<p>更新于 18 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.07164.pdf">Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling</a></p>
<ul>
<li>
<p><strong>Author</strong>: Shruti Bhosale, Kyra Yee, Sergey Edunov, Michael Auli</p>
</li>
<li>
<p><strong>Comments</strong>: WMT 2020</p>
</li>
<li>
<p><strong>Keywords</strong>: Noisy Channel Modeling</p>
</li>
</ul>
<p>对于 naïve noisy channel modeling 用于 seq2seq model 速度很慢，这篇工作提出一种有效的近似方法，在提高推理速度的同时还提高了准确性。他们在 Romanian-English 翻译任务上达到了新的 SOTA，表明 noise channel modeling 的方法可以达到比 pre-training 更好的效果。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07347.pdf">Conditioned Natural Language Generation using only Unconditioned Language Model: An Exploration</a></p>
<ul>
<li><strong>Author</strong>: Fan-Keng Sun, Cheng-I Lai</li>
</ul>
<p>用 Unconditioned LM 来完成 Conditioned NLG，即用 $p(x_t | x_{t-1}, \cdots, x_1)$ 来建模 $p(x_t, x_{t-1}, \cdots, x_1, c)$。这篇工作尝试了 4 种简单有效的方法。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07593.pdf">Morphologically Aware Word-Level Translation</a></p>
<ul>
<li><strong>Author</strong>: Paula Czarnowska, Sebastian Ruder, Ryan Cotterell, Ann Copestake</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>这篇工作针对双语词典推断任务（bilingual lexicon induction, BLI）提出一种新的建模方式，同时考虑 lexeme 和 inflectional morphology。他们的方法在 6 组语言对上取得显著提升。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07635.pdf">DORB: Dynamically Optimizing Multiple Rewards with Bandits</a></p>
<ul>
<li><strong>Author</strong>: Ramakanth Pasunuru,  Han Guo, Mohit Bansal</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: RL, MAB, NLG</li>
</ul>
<p>NLG 任务中一个比较常见的问题是训练和评估的指标不一致：训练时优化 MLE，而评估时是看 BLEU METEOR 之类的指标，因为许多我们实际关心的指标 non-differentiable，而强化学习的好处是可以直接优化 non-differentiable metrics。如果只优化单一指标，很有可能的现象是模型只在这一个指标上表现好，而其他指标上表现不佳，所以更好的是组合多个指标共同优化模型。但是不同指标在 loss 中的占比应该如何设计，以及随着训练的进行需要如何调整，这一点很难由人工来设计好。这篇工作提出用 Multi-Armed Bandit (MAB) 来组合多个指标，在每一轮决定根据哪个指标来优化模型。他们在两个 NLG 任务上 (question generation, data-to-text generation) 验证了这一方法的有效性。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07743.pdf">Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases</a></p>
<ul>
<li><strong>Author</strong>: Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, Yu Su</li>
<li><strong>Comments</strong>: EMNLP 2020 Workshop</li>
</ul>
<p>这篇工作</p>
<ul>
<li>
<p>提出 KBQA 模型需要有三个级别的泛化能力：i.i.d., compositional, zero-shot。</p>
</li>
<li>
<p>发布了一个新的数据集 GRAILQA (64,331 questions)，评估模型在这三个级别上的泛化能力。</p>
</li>
<li>
<p>提出了一个 BERT-based KBQA model。</p>
</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07916.pdf">Text Information Aggregation with Centrality Attention</a></p>
<ul>
<li><strong>Author</strong>: Jingjing Gong, Hang Yan, Yining Zheng, Qipeng Guo, Xipeng Qiu,  Xuanjing Huang</li>
<li><strong>Comments</strong>: SCIENCE CHINA Information Sciences</li>
</ul>
<p>这篇工作提出一种新的 sentence representation 方法：将句子表示成完全图，节点是句子中的每个单词，然后计算 eigen-centrality 作为每个词的注意力权重。相比于传统的 pooling 等方法在文本分类上得到了更好的效果。</p>
<p><em>看摘要感觉计算应该比较慢，不知道是否值得。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07956.pdf">Pre-training Text-to-Text Transformers for Concept-centric Common Sense</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam, Seyeon Lee, Bill Yuchen Lin, Xiang Ren</li>
<li><strong>Keywords</strong>: concept-centric commonsense knowledge, Pre-trained LM</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07960.pdf">Explicitly Modeling Syntax in Language Model improves Generalization</a></p>
<ul>
<li><strong>Author</strong>: Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron Courville</li>
<li><strong>Keywords</strong>: Syntax, LM, one-step look-ahead parser</li>
</ul>
<p>这篇工作提出了一种考虑句法的语言模型  Syntactic Ordered Memory (SOM)。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.02969.pdf">BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance</a></p>
<ul>
<li><strong>Author</strong>: R. Thomas McCoy,  Junghyun Min, Tal Linzen</li>
</ul>
<p>这篇工作发现：在 MNLI 数据集上微调 100 遍 BERT，在验证集上达到了相近的表现。而这 100 个模型却体现出了差异巨大的泛化能力。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.03863.pdf">Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks</a></p>
<ul>
<li><strong>Author</strong>: Trapit Bansal, Rishikesh Jha, Andrew McCallum</li>
<li><strong>Comments</strong>: COLING 2020</li>
<li><strong>Keywords</strong>: Meta-learning</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.08445.pdf">Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</a></p>
<ul>
<li><strong>Author</strong>: Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Self-supervised learning, Meta-learning</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.07437.pdf">Non-Autoregressive Machine Translation with Latent Alignments</a></p>
<ul>
<li><strong>Author</strong>: Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi</li>
<li><strong>Keywords</strong>: CTC, Imputer, Non-autoregressive model, discrete latent alignment variables</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="12-nov-20----13-nov-20">12 Nov 20 &ndash; 13 Nov 20</h2>
<p>更新于 17 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.06623.pdf">doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset</a></p>
<ul>
<li><strong>Author</strong>: Song Feng, Hui Wan, Chulaka Gunasekara, Siva Sankalp Patel, Sachindra Joshi, Luis A. Lastras</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作贡献了一个 document-grounded 对话数据集。&lsquo;The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.06868.pdf">EDITOR: an Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</a></p>
<ul>
<li><strong>Author</strong>: Weijia Xu, Marine Carpuat</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>这篇是基于 <a href="https://arxiv.org/pdf/1905.11006.pdf">Levenshtein Transformer</a> 的改进工作，将原来的  <code>deletion</code>  操作替换为 <code>reposition</code>。相比于 Levenshtein Transformer，他们的模型可以达到更好的效果以及跟快的推理速度。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07013.pdf">Deconstructing word embedding algorithms</a></p>
<ul>
<li><strong>Author</strong>: Kian Kenyon-Dean, Edward Newell, , Jackie Chi Kit Cheung</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作对经典的 Uncontextualized word embedding 方法做了理论分析。（很短，只有 6 页）</p>
<p>​</p>
<p>​</p>
<h2 id="11-nov-20----12-nov-20">11 Nov 20 &ndash; 12 Nov 20</h2>
<p>更新于 14 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.06132.pdf">Incorporating a Local Translation Mechanism into Non-autoregressive Translation</a></p>
<ul>
<li><strong>Author</strong>: Xiang Kong, Zhisong Zhang, Eduard Hovy</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Non-autoregressive</li>
</ul>
<p>Non-autoregressive 相比于 Autoregressive 模型的好处是计算快，因为每个位置的 token 可以被并行预测。但与此同时带来的缺点是每个预测 token 的独立出的，所以整体预测结果质量较差。这篇文章对于两种模型取了折中：在每个位置预测一个片段，然后设计了一种有效的融合方式将每个位置的片段处理为最终的输出。相比于 Baseline (<a href="https://arxiv.org/pdf/2011.06132.pdf">CMLM</a>)，他们的模型用更少的迭代就达到可比或者更好的结果。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.02496.pdf">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a></p>
<ul>
<li><strong>Author</strong>: Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: BERT, Span-based dynamic convolution</li>
</ul>
<p>在 BERT 中，attention 的计算会占据很大的计算量，因为序列中的每个单词需要 attend 到整个序列。这篇工作发现，对于 attention 层的多个注意力头，其实有的头只需要去关注局部信息，因此现有的计算是存在很大的冗余的。因此，他们提出用 span-based dynamic convolution 去替代其中冗余的头，只关注局部信息。替换后的混合 attention 机制不仅提升了表现，而且减少了计算代价。</p>
<p>比较好奇是怎么发现一些注意力头冗余的，以及这个 span-based dynamic convolution 到底是怎么计算的。</p>
<p>想到了一篇相关的工作 <a href="https://arxiv.org/pdf/1911.02150.pdf">Fast Transformer Decoding: One Write-Head is All You Need</a></p>
<p>​</p>
<p>​</p>
<h2 id="10-nov-20----11-nov-20">10 Nov 20 &ndash; 11 Nov 20</h2>
<p>更新于 13 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.05864.pdf">On the Sentence Embeddings from Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Sentence Embedding, BERT, Normalizing Flow</li>
</ul>
<p>这篇文章首先观察到一个现象：预训练模型的 sentence embedding 如果不做 finetune，难以很好地概括句子的语义。他们对 BERT MLM 的预训练任务和语义相似度任务之间的关联做了理论分析，发现 BERT 总是会推导出一个非平滑的各向异性的语义空间，而这会对语义相似度任务有负面影响。为了解决这个问题，他们提出利用 Normalizing Flow 将这一各向异性的语义空间转换为各向同性的高斯分布，在各种语义相似度任务上得到显著提升。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05910.pdf">Audrey: A Personalized Open-Domain Conversational Bot</a></p>
<ul>
<li><strong>Author</strong>: Chung Hoon Hong, Yuan Liang, Sagnik Sinha Roy, Arushi Jain, Vihang Agarwal, Ryan Draves , Zhizhuo Zhou, William Chen, Yujian Liu, Martha Miracky, Lily Ge, Nikola Banovic, David Jurgens</li>
</ul>
<p>介绍了 University of Michigan 参赛 Alexa Prize 2019 的对话模型 Audrey。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.12744.pdf">Augmenting Transformers with KNN-Based Composite Memory for Dialog</a></p>
<ul>
<li><strong>Author</strong>: Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes</li>
</ul>
<p>NLP 的许多任务会受益于丰富的外部知识，例如 QA, Chatbot, Reasoning。这篇工作为对话模型设计了一个基于 KNN 的外部知识提取模块。</p>
<p><em>感兴趣的理由是在对话领域，越长的 context 和越丰富的外部知识一定会有利于更好的对话生成，但这些信息很难记忆在隐空间中，因此这种与一个知识库交互的信息提取的做法可以了解一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.01460.pdf">Situated and Interactive Multimodal Conversations</a></p>
<ul>
<li><strong>Author</strong>: Seungwhan Moon, Satwik Kottur, Paul A. Crook, Ankita De, Shivani Poddar, Theodore Levin, David Whitney, Daniel Difranco, Ahmad Beirami Eunjoon Cho, Rajen Subba, Alborz Geramifard</li>
</ul>
<p>新一代虚拟助手（尤其是客服）需要应对多模态的 context 以及做出多模态的回复。Facebook 提出了一个新的任务  Situated Interactive MultiModal Conversations (SIMMC) 来训练模型向这一目标努力。他们发布了两个购物相关的数据集。这个任务也是 <a href="https://sites.google.com/dstc.community/dstc9/home">DSTC9</a> 的赛道之一。</p>
<p>​</p>
<p>​</p>
<h2 id="9-nov-20----10-nov-20">9 Nov 20 &ndash; 10 Nov 20</h2>
<p>更新于 12 Nov 20</p>
<p><a href="https://arxiv.org/pdf/2011.04823.pdf">Language Through a Prism: A Spectral Approach for Multiscale Language Representations</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Dan Jurafsky, Noah Goodman</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>自然语言中是包含不同尺度的信息的 (character-level, word-level, document-level, \cdots)，而不同的下游任务会用到不同尺度的信息（例如文档分类会用到篇章级别的信息）。如果可以将不同尺度的信息分离出来，那么显然是对做特定尺度的任务有好处的。这篇工作借用信号处理中「光谱分析 (spectral analysis)」的做法，成功提取出特定尺度的信息，并基于此设计了一种新的模型组件: <em>prism layer</em>。<em>prism layer</em> + BERT 能更好地在 MLM (Masked Language Modeling) 任务中利用 long-range context。</p>
<p><em>题目中的 Prism 起的很有灵魂了。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.04946.pdf">When Do You Need Billions of Words of Pretraining Data?</a></p>
<ul>
<li><strong>Author</strong>: Yian Zhang, Alex Warstadt,  Haau-Sing Li, Samuel R. Bowman</li>
</ul>
<p>近几年来借助大规模预训练模型，许多工作在下游任务上频繁刷榜。预训练模型在海量无标注数据上学到了丰富的知识，而这些知识显然是有利于下游任务的学习的。但预训练模型使用的数据量通常非常庞大，因此训练起来代价是很高的。因此值得思考的是：这些预训练模型学到的语言学知识中，哪些必须要见过足够庞大的数据量才能学到，而哪些只需少量数据就可以学的很好？这篇文章设计了 4 种 Probing 任务来研究在四种数据规模上预训练的 RoBERTa。结论是大部分句法和语义的知识只需较少的数据就可以学到了，而更大的数据量可以让模型学到下游任务需要的 commense knowledge。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05268.pdf">Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: Natural language explanation, Latent variables</li>
</ul>
<p><em>看到 Interpretable 和 Latent variables 就觉得应该值得一读，摘要没有太看明白，不知道 Natural language explanation 是在做什么，之后好好读一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14975.pdf">Investigating Transferability in Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah Goodman</li>
<li><strong>Comments</strong>:  Findings of EMNLP 2020</li>
</ul>
<p>这篇工作的是 pre-trained models 迁移到 downstream tasks 的能力。具体做法是，在 transfer task 上训练 pre-trained models 时，随机将不同的层替换为初始化参数，检验替换前后模型的表现。结论是：</p>
<ul>
<li>&lsquo;layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks.&rsquo;</li>
<li>&lsquo;parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings.&rsquo;</li>
</ul>
<p><em>看摘要觉得这篇工作做的似乎挺简单的，以及 Transfer Learning 确实是个很值得研究的主题。</em></p>
<p>​</p>
<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.655.pdf">Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity</a></p>
<ul>
<li><strong>Author</strong>: Pedro Rodriguez, Paul Crook, Seungwhan Moon, Zhiguang Wang</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作围绕一个关于对话的猜想：&lsquo;engagement increases when users are presented with facts related to what they know.&rsquo; 设计了一个有标注的数据集 (规模是 14K dialogs, 181K utterances)。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09427.pdf">Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data</a></p>
<ul>
<li><strong>Author</strong>: Rongsheng Zhang, Yinhe Zheng, Jianzhi Shao, Xiaoxi Mao, Yadong Xi, Minlie Huang</li>
<li><strong>Comments</strong>: EMNLP 2020, Long paper</li>
</ul>
<p>使用 unpaired data 来增强对话模型。</p>
<p><em>最近也在考虑的问题，之后来好好读一下这篇文章的具体做法。</em></p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
