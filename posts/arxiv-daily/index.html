<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily/" />
<meta property="article:published_time" content="2020-11-12T12:57:25+08:00" />
<meta property="article:modified_time" content="2020-11-12T12:57:25+08:00" />


    <title>
  Arxiv Daily · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-11-12T12:57:25&#43;08:00'>
                November 12, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              3-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>最近订阅了 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a>，希望能坚持每天刷一刷，了解一下 NLP 领域的最新文献。在这篇博客中我会记录每天刷到的感兴趣的工作。</p>
<h2 id="10-nov-20----11-nov-20">10 Nov 20 &ndash; 11 Nov 20</h2>
<p>更新于 13 Nov 2020</p>
<p><a href="https://arxiv.org/pdf/2011.05864.pdf">On the Sentence Embeddings from Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
<li><strong>Keywords</strong>: Sentence Embedding, BERT, Normalizing Flow</li>
</ul>
<p>这篇文章首先观察到一个现象：预训练模型的 sentence embedding 如果不做 finetune，难以很好地概括句子的语义。他们对 BERT MLM 的预训练任务和语义相似度任务之间的关联做了理论分析，发现 BERT 总是会推导出一个非平滑的各向异性的语义空间，而这会对语义相似度任务有负面影响。为了解决这个问题，他们提出利用 Normalizing Flow 将这一各向异性的语义空间转换为各向同性的高斯分布，在各种语义相似度任务上得到显著提升。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05910.pdf">Audrey: A Personalized Open-Domain Conversational Bot</a></p>
<ul>
<li><strong>Author</strong>: Chung Hoon Hong, Yuan Liang, Sagnik Sinha Roy, Arushi Jain, Vihang Agarwal, Ryan Draves , Zhizhuo Zhou, William Chen, Yujian Liu, Martha Miracky, Lily Ge, Nikola Banovic, David Jurgens</li>
</ul>
<p>介绍了 University of Michigan 参赛 Alexa Prize 2019 的对话模型 Audrey。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.12744.pdf">Augmenting Transformers with KNN-Based Composite Memory for Dialog</a></p>
<ul>
<li><strong>Author</strong>: Angela Fan, Claire Gardent, Chloe Braud, Antoine Bordes</li>
</ul>
<p>NLP 的许多任务会受益于丰富的外部知识，例如 QA, Chatbot, Reasoning。这篇工作为对话模型设计了一个基于 KNN 的外部知识提取模块。</p>
<p><em>感兴趣的理由是在对话领域，越长的 context 和越丰富的外部知识一定会有利于更好的对话生成，但这些信息很难记忆在隐空间中，因此这种与一个知识库交互的信息提取的做法可以了解一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.01460.pdf">Situated and Interactive Multimodal Conversations</a></p>
<ul>
<li><strong>Author</strong>: Seungwhan Moon, Satwik Kottur, Paul A. Crook, Ankita De, Shivani Poddar, Theodore Levin, David Whitney, Daniel Difranco, Ahmad Beirami Eunjoon Cho, Rajen Subba, Alborz Geramifard</li>
</ul>
<p>新一代虚拟助手（尤其是客服）需要应对多模态的 context 以及做出多模态的回复。Facebook 提出了一个新的任务  Situated Interactive MultiModal Conversations (SIMMC) 来训练模型向这一目标努力。他们发布了两个购物相关的数据集。这个任务也是 <a href="https://sites.google.com/dstc.community/dstc9/home">DSTC9</a> 的赛道之一。</p>
<h2 id="9-nov-20----10-nov-20">9 Nov 20 &ndash; 10 Nov 20</h2>
<p>更新于 12 Nov 20</p>
<p><a href="https://arxiv.org/pdf/2011.04823.pdf">Language Through a Prism: A Spectral Approach for Multiscale Language Representations</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Dan Jurafsky, Noah Goodman</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>自然语言中是包含不同尺度的信息的 (character-level, word-level, document-level, \cdots)，而不同的下游任务会用到不同尺度的信息（例如文档分类会用到篇章级别的信息）。如果可以将不同尺度的信息分离出来，那么显然是对做特定尺度的任务有好处的。这篇工作借用信号处理中「光谱分析 (spectral analysis)」的做法，成功提取出特定尺度的信息，并基于此设计了一种新的模型组件: <em>prism layer</em>。<em>prism layer</em> + BERT 能更好地在 MLM (Masked Language Modeling) 任务中利用 long-range context。</p>
<p><em>题目中的 Prism 起的很有灵魂了。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.04946.pdf">When Do You Need Billions of Words of Pretraining Data?</a></p>
<ul>
<li><strong>Author</strong>: Yian Zhang, Alex Warstadt,  Haau-Sing Li, Samuel R. Bowman</li>
</ul>
<p>近几年来借助大规模预训练模型，许多工作在下游任务上频繁刷榜。预训练模型在海量无标注数据上学到了丰富的知识，而这些知识显然是有利于下游任务的学习的。但预训练模型使用的数据量通常非常庞大，因此训练起来代价是很高的。因此值得思考的是：这些预训练模型学到的语言学知识中，哪些必须要见过足够庞大的数据量才能学到，而哪些只需少量数据就可以学的很好？这篇文章设计了 4 种 Probing 任务来研究在四种数据规模上预训练的 RoBERTa。结论是大部分句法和语义的知识只需较少的数据就可以学到了，而更大的数据量可以让模型学到下游任务需要的 commense knowledge。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05268.pdf">Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
<li><strong>Keywords</strong>: Natural language explanation, Latent variables</li>
</ul>
<p><em>看到 Interpretable 和 Latent variables 就觉得应该值得一读，摘要没有太看明白，不知道 Natural language explanation 是在做什么，之后好好读一下。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14975.pdf">Investigating Transferability in Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah Goodman</li>
<li><strong>Comments</strong>:  Findings of EMNLP 2020</li>
</ul>
<p>这篇工作的是 pre-trained models 迁移到 downstream tasks 的能力。具体做法是，在 transfer task 上训练 pre-trained models 时，随机将不同的层替换为初始化参数，检验替换前后模型的表现。结论是：</p>
<ul>
<li>&lsquo;layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks.&rsquo;</li>
<li>&lsquo;parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings.&rsquo;</li>
</ul>
<p><em>看摘要觉得这篇工作做的似乎挺简单的，以及 Transfer Learning 确实是个很值得研究的主题。</em></p>
<p>​</p>
<p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.655.pdf">Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity</a></p>
<ul>
<li><strong>Author</strong>: Pedro Rodriguez, Paul Crook, Seungwhan Moon, Zhiguang Wang</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>这篇工作围绕一个关于对话的猜想：&lsquo;engagement increases when users are presented with facts related to what they know.&rsquo; 设计了一个有标注的数据集 (规模是 14K dialogs, 181K utterances)。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09427.pdf">Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data</a></p>
<ul>
<li><strong>Author</strong>: Rongsheng Zhang, Yinhe Zheng, Jianzhi Shao, Xiaoxi Mao, Yadong Xi, Minlie Huang</li>
<li><strong>Comments</strong>: EMNLP 2020, Long paper</li>
</ul>
<p>使用 unpaired data 来增强对话模型。</p>
<p><em>最近也在考虑的问题，之后来好好读一下这篇文章的具体做法。</em></p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
