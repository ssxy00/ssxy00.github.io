<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Oct 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Oct 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2110/" />
<meta property="article:published_time" content="2021-10-12T14:53:16+08:00" />
<meta property="article:modified_time" content="2021-10-12T14:53:16+08:00" />


    <title>
  Arxiv Daily | Oct 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2110/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Oct 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-10-12T14:53:16&#43;08:00'>
                October 12, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              11-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 10 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="25-oct-21----26-oct-21">25 Oct 21 &ndash; 26 Oct 21</h2>
<p>更新于  28 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.13522.pdf">Probabilistic Entity Representation Model for Chain Reasoning over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, Chandan K. Reddy</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.13711.pdf"><strong>Hierarchical Transformers Are More Efficient Language Models</strong></a></p>
<ul>
<li><strong>Author</strong>: Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, Henryk Michalewski</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.13715.pdf">ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, Feng Wu</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.06891.pdf">Global-aware Beam Search for Neural Abstractive Summarization</a></p>
<ul>
<li><strong>Author</strong>: Ye Ma, Zixun Lan, Lu Zong, Kaizhu Huang</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.03831.pdf">Counterfactual Maximum Likelihood Estimation for Training Deep Networks</a></p>
<ul>
<li><strong>Author</strong>: Xinyi Wang, Wenhu Chen, Michael Saxon, William Yang Wang</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="22-oct-21----25-oct-21">22 Oct 21 &ndash; 25 Oct 21</h2>
<p>更新于  27 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2102.04081.pdf">VeeAlign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment</a></p>
<ul>
<li><strong>Author</strong>: Vivek Iyer, Arvind Agarwal, Harshit Kumar</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.12567.pdf">Alignment Attention by Matching Key and Query Distributions</a></p>
<ul>
<li><strong>Author</strong>: Shujian Zhang, Xinjie Fan, Huangjie Zheng, Korawat Tanwisuth, Mingyuan Zhou</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.12894.pdf">The Efficiency Misnomer</a></p>
<ul>
<li><strong>Author</strong>: Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, Yi Tay</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03888.pdf">M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining</a></p>
<ul>
<li><strong>Author</strong>: Junyang Lin, An Yang, Jinze Bai, Chang Zhou Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin Jingren Zhou, Hongxia Yang</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="21-oct-21----22-oct-21">21 Oct 21 &ndash; 22 Oct 21</h2>
<p>更新于  26 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.11514.pdf">SYNERGY: Building Task Bots at Scale Using Symbolic Knowledge and Machine Teaching</a></p>
<ul>
<li><strong>Author</strong>: Baolin Peng, Chunyuan Li, Zhu Zhang12, Jinchao Li, Chenguang Zhu, Jianfeng Gao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.11560.pdf">Adaptive Bridge between Training and Inference for Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Haoran Xu, Hainan Zhang, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Yanyan Lan</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.03808.pdf">Improving BERT with Self-Supervised Attention</a></p>
<ul>
<li><strong>Author</strong>: Yiren Chen, Xiaoyu Kou, Jiangang Bai, Yunhai Tong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.00200.pdf">Iterative Hierarchical Attention for Answering Complex Questions over Long Documents</a></p>
<ul>
<li><strong>Author</strong>: Haitian Sun, William W. Cohen, Ruslan Salakhutdinov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.05379.pdf">Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions</a></p>
<ul>
<li><strong>Author</strong>: Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="19-oct-21----21-oct-21">19 Oct 21 &ndash; 21 Oct 21</h2>
<p>更新于  22 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.10470.pdf">Interpreting Deep Learning Models in Natural Language Processing: A Review</a></p>
<ul>
<li><strong>Author</strong>: Xiaofei Sun, Diyi Yang, Xiaoya Li, Tianwei Zhang, Yuxian Meng, Qiu Han, Guoyin Wang, Eduard Hovy, Jiwei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.10746.pdf">Better than Average: Paired Evaluation of NLP Systems</a></p>
<ul>
<li><strong>Author</strong>: Maxime Peyrard, Wei Zhao, Steffen Eger, Robert West</li>
<li><strong>Comments</strong>: ACL 2021 (long paper)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.10778.pdf">Contrastive Document Representation Learning with Graph Attention Networks</a></p>
<ul>
<li><strong>Author</strong>: Peng Xu, Xinchi Chen, Xiaofei Ma, Zhiheng Huang, Bing Xiang</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.11115.pdf">Improving Non-autoregressive Generation with Mixup Training</a></p>
<ul>
<li><strong>Author</strong>: Ting Jiang, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Liangjie Zhang, Qi Zhang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.10963.pdf">Neuro-Symbolic Reinforcement Learning with First-Order Logic</a></p>
<ul>
<li><strong>Author</strong>: Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander Gray</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="18-oct-21----19-oct-21">18 Oct 21 &ndash; 19 Oct 21</h2>
<p>更新于  21 Oct 2021</p>
<p>None</p>
<p>​</p>
<p>​</p>
<h2 id="15-oct-21----18-oct-21">15 Oct 21 &ndash; 18 Oct 21</h2>
<p>更新于  20 Oct 2021</p>
<p>[Boosting coherence of language models](Boosting coherence of language models)</p>
<ul>
<li><strong>Author</strong>: Nikolay Malkin, Zhen Wang, Nebojsa Jojic</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08426.pdf">EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks</a></p>
<ul>
<li><strong>Author</strong>: Frederick Liu, Siamak Shakeri, Hongkun Yu, Jing Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08445.pdf">How Well Do You Know Your Audience? Reader-aware Question Generation</a></p>
<ul>
<li><strong>Author</strong>: Ian Stewart, Rada Mihalcea</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08455.pdf">Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey</a></p>
<ul>
<li><strong>Author</strong>: Xiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, Andrew Arnold</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08501.pdf">Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, Dilek Hakkani-Tur</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08544.pdf">Tackling Multi-Answer Open-Domain Questions via a Recall-then-Verify Framework</a></p>
<ul>
<li><strong>Author</strong>: Zhihong Shao, Minlie Huang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08743.pdf">GNN-LM: Language Modeling based on Global Contexts via GNN</a></p>
<ul>
<li><strong>Author</strong>: Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.09456.pdf">NormFormer: Improved Transformer Pretraining with Extra Normalization</a></p>
<ul>
<li><strong>Author</strong>: Sam Shleifer, Jason Weston, Myle Ott</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08323.pdf">On Learning the Transformer Kernel</a></p>
<ul>
<li><strong>Author</strong>: Sankalan Pal Chowdhury, Adamos Solomou, Avinava Dubey, Mrinmaya Sachan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08678.pdf">Transformer with a Mixture of Gaussian Keys</a></p>
<ul>
<li><strong>Author</strong>: Tam Nguyen†, Tan M. Nguyen, Dung Le, Khuong Nguyen, Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.10216.pdf">Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions</a></p>
<ul>
<li><strong>Author</strong>: Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07830.pdf">Reframing Instructional Prompts to GPTk’s Language</a></p>
<ul>
<li><strong>Author</strong>: Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="14-oct-21----15-oct-21">14 Oct 21 &ndash; 15 Oct 21</h2>
<p>更新于  18 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.07752.pdf">Hindsight: Posterior-guided training of retrievers for improved open-ended generation</a></p>
<ul>
<li><strong>Author</strong>: Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.07803.pdf">ContraQA: Question Answering under Contradicting Contexts</a></p>
<ul>
<li><strong>Author</strong>: Liangming Pan, Wenhu Chen, Min-Yen Kan, William Yang Wang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08018.pdf">Structural Modeling for Dialogue Disentanglement</a></p>
<ul>
<li><strong>Author</strong>: Xinbei Ma, Zhuosheng Zhang, Hai Zhao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08032.pdf">UniDS : A Unified Dialogue System for Chit-Chat and Task-oriented Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Xinyan Zhao, Bin He, Yasheng Wang, Yitong Li, Fei Mi, Yajiao Liu, Xin Jiang, Qun Liu, Huanhuan Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08118.pdf">Few-Shot Bot: Prompt-Based Learning for Dialogue Systems</a></p>
<ul>
<li><strong>Author</strong>: Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, Pascale Fung</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08222.pdf">DialFact: A Benchmark for Fact-Checking in Dialogue</a></p>
<ul>
<li><strong>Author</strong>: Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08207.pdf">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></p>
<ul>
<li><strong>Author</strong>: Victor Sanh et al.</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="13-oct-21----14-oct-21">13 Oct 21 &ndash; 14 Oct 21</h2>
<p>更新于  16 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.07002.pdf">Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Florian Mai, James Henderson</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.07143.pdf">bert2BERT: Towards Reusable Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, Qun Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.07515.pdf">Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision</a></p>
<ul>
<li><strong>Author</strong>: Chenyang Huang, Hao Zhou, Osmar R. Zaïane, Lili Mou, Lei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.07383.pdf">The Neglected Sibling: Isotropic Gaussian Posterior for VAE</a></p>
<ul>
<li><strong>Author</strong>: Lan Zhang, Wray Buntine, Ehsan Shareghi</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.00371.pdf">On-the-Fly Attention Modulation for Neural Generation</a></p>
<ul>
<li><strong>Author</strong>: Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine Bosselut, Jackie Chi Kit Cheung, Yejin Choi</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="12-oct-21----13-oct-21">12 Oct 21 &ndash; 13 Oct 21</h2>
<p>更新于  15 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.06274.pdf">LiST: Lite Self-training Makes Efficient Few-shot Learners</a></p>
<ul>
<li><strong>Author</strong>: Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06388.pdf">HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization</a></p>
<ul>
<li><strong>Author</strong>: Ye Liu, Jian-Guo Zhang, Yao Wan, Congying Xia, Lifang He, Philip S. Yu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06419.pdf">Federated Natural Language Generation for Personalized Dialogue System</a></p>
<ul>
<li><strong>Author</strong>: Yujie Lu, Chao Huang, Huanli Zhan, Yong Zhuang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06612.pdf">Exploring Dense Retrieval for Dialogue Response Selection</a></p>
<ul>
<li><strong>Author</strong>: Tian Lan, Deng Cai, Yan Wang, Yixuan Su, Xian-Ling Mao, Heyan Huang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06823.pdf">A Speaker-Aware Learning Framework for Improving Multi-turn Dialogue Coherence</a></p>
<ul>
<li><strong>Author</strong>: Zihao Wang, Ming Jiang, Junli Wang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06884.pdf">ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers</a></p>
<ul>
<li><strong>Author</strong>: Haitian Sun, William W. Cohen, Ruslan Salakhutdinov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06821.pdf"><strong>Leveraging redundancy in attention with Reuse Transformers</strong></a></p>
<ul>
<li><strong>Author</strong>: Srinadh Bhojanapalli, Ayan Chakrabarti, Andreas Veit, Michal Lukasik, Himanshu Jain, Frederick Liu, Yin-Wen Chang, Sanjiv Kumar</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="11-oct-21----12-oct-21">11 Oct 21 &ndash; 12 Oct 21</h2>
<p>更新于  14 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.05780.pdf">We’ve had this conversation before: A Novel Approach to Measuring Dialog Similarity</a></p>
<ul>
<li><strong>Author</strong>: Ofer Lavi, Ella Rabinovich, Segev Shlomov, David Boaz, Inbal Ronen, Ateret Anaby-Tavor</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.05999.pdf">DISCODVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer</a></p>
<ul>
<li><strong>Author</strong>: Haozhe Ji, Minlie Huang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.05838.pdf">Balancing Average and Worst-case Accuracy in Multitask Learning</a></p>
<ul>
<li><strong>Author</strong>: Paul Michel, Sebastian Ruder, Dani Yogatama</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-oct-21----11-oct-21">8 Oct 21 &ndash; 11 Oct 21</h2>
<p>更新于  13 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.04330.pdf">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, Michael Zeng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04366.pdf">Towards a Unified View of Parameter-Efficient Transfer Learning</a></p>
<ul>
<li><strong>Author</strong>: Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04374.pdf">A Few More Examples May Be Worth Billions of Parameters</a></p>
<ul>
<li><strong>Author</strong>: Yuval Kirstain, Patrick Lewis, Sebastian Riedel, Omer Levy</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2110.04541">The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design</a></p>
<ul>
<li><strong>Author</strong>: Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04614.pdf">Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality</a></p>
<ul>
<li><strong>Author</strong>: Jiashuo Wang, Wenjie Li, Peiqin Lin, Feiteng Mu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04821.pdf">DCT: Dynamic Compressive Transformer for Modeling Unbounded Sequence</a></p>
<ul>
<li><strong>Author</strong>: Kai-Po Chang, Wei-Yun Ma</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04845.pdf">What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study</a></p>
<ul>
<li><strong>Author</strong>: Mohamed Abdalla, Krishnapriya Vishnubhotla, Saif M. Mohammad</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.03125.pdf">Advances in Multi-turn Dialogue Comprehension: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Zhuosheng Zhang, Hai Zhao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.05448.pdf">Unsupervised Neural Machine Translation with Generative Language Models Only</a></p>
<ul>
<li><strong>Author</strong>: Igor Babuschkin, Harrison Edwards, Arvind Neelakantan, Tao Xu, Stanislas Polu, Alex Ray, Pranav Shyam, Aditya Ramesh, Alec Radford, Ilya Sutskever</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.05456.pdf">Rome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, Dilek Hakkani-Tur</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07601.pdf">Approximating How Single Head Attention Learns</a></p>
<ul>
<li><strong>Author</strong>: Charlie Snell, Ruiqi Zhong, Dan Klein, Jacob Steinhardt</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06378.pdf">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.09864.pdf">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></p>
<ul>
<li><strong>Author</strong>: Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="7-oct-21----8-oct-21">7 Oct 21 &ndash; 8 Oct 21</h2>
<p>更新于  13 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.03949.pdf">CheerBots: Chatbots toward Empathy and Emotion using Reinforcement Learning</a></p>
<ul>
<li><strong>Author</strong>: Jiun-Hao Jhan, Chao-Peng Liu, Shyh-Kang Jeng, Hung-Yi Lee</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2110.04260">Taming Sparsely Activated Transformer with Stochastic Experts</a></p>
<ul>
<li><strong>Author</strong>: Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, Jianfeng Gao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03848.pdf">Speeding up Deep Model Training by Sharing Weights and Then Unsharing</a></p>
<ul>
<li><strong>Author</strong>: Shuo Yang, Le Hou, Xiaodan Song, Qiang Liu, Denny Zhou</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04169.pdf">Iterative Decoding for Compositional Generalization in Transformers</a></p>
<ul>
<li><strong>Author</strong>: Luana Ruiz, Joshua Ainslie, Santiago Ontañón</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.10421.pdf">English Machine Reading Comprehension Datasets: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Daria Dzendzik, Carl Vogel, Jennifer Foster</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="6-oct-21----7-oct-21">6 Oct 21 &ndash; 7 Oct 21</h2>
<p>更新于  12 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.03215.pdf">Towards Continual Knowledge Learning of Language Models</a></p>
<ul>
<li><strong>Author</strong>: Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03389.pdf">Beam Search with Bidirectional Strategies for Neural Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Pierre Colombo, Chouchang (Jack) Yang, Giovanna Varni, Chloé Clavel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03618.pdf">Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP</a></p>
<ul>
<li><strong>Author</strong>: Zhijing Jin, Julius von Kügelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, Bernhard Schölkopf</li>
<li><strong>Comments</strong>:  EMNLP 2021 (Oral)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03192.pdf">GNN is a Counter? Revisiting GNN for Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Kuan Wang, Yuyu Zhang, Diyi Yang, Le Song, Tao Qin</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.03318.pdf">On the Latent Holes of VAEs for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Ruizhe Li, Xutan Peng, Chenghua Lin</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="5-oct-21----6-oct-21">5 Oct 21 &ndash; 6 Oct 21</h2>
<p>更新于  12 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.02442.pdf">PoNet: Pooling Network for Efficient Token Mixing in Long Sequences</a></p>
<ul>
<li><strong>Author</strong>: Chao-Hong Tan, Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Zhen-Hua Ling</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.02488.pdf">ABC: Attention with Bounded-memory Control</a></p>
<ul>
<li><strong>Author</strong>: Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, Noah A. Smith</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.02802.pdf">Self-conditioning pre-trained language models</a></p>
<ul>
<li><strong>Author</strong>: Xavier Suau, Luca Zappella,  Nicholas Apostoloff</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.02887.pdf">Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings</a></p>
<ul>
<li><strong>Author</strong>: Sawsan Alqahtani, Garima Lalwani, Yi Zhang, Salvatore Romeo, Saab Mansour</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07012.pdf">Sparse Attention with Linear Units</a></p>
<ul>
<li><strong>Author</strong>: Biao Zhang, Ivan Titov, Rico Sennrich</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2106.15078">Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Guangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan Liang, Zhen Li, Bowen Zhou, Shuguang Cui, Zhiting Hu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2108.13161">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a></p>
<ul>
<li><strong>Author</strong>: Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="4-oct-21----5-oct-21">4 Oct 21 &ndash; 5 Oct 21</h2>
<p>更新于  12 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.01804.pdf">A Survey On Neural Word Embeddings</a></p>
<ul>
<li><strong>Author</strong>: Erhan Sezerer, Selma Tekir</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.01811.pdf">On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, Zhaopeng Tu</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.01852.pdf">Data Augmentation Approaches in Natural Language Processing: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Bohan Li, Yutai Hou, Wanxiang Che</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.01895.pdf">Investigating the Impact of Pre-trained Language Models on Dialog Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhang, Luis Fernando D’Haro, Yiming Chen, Thomas Friedrichs, Haizhou Li</li>
<li><strong>Comments</strong>: IWSDS2021 (Long Paper)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.06829.pdf">Empathetic Dialog Generation with Fine-Grained Intents</a></p>
<ul>
<li><strong>Author</strong>: Yubo Xie, Pearl Pu</li>
<li><strong>Comments</strong>: CoNLL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-oct-21----4-oct-21">1 Oct 21 &ndash; 4 Oct 21</h2>
<p>更新于  12 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.00687.pdf"><strong>Investigating Robustness of Dialog Models to Popular Figurative Language Constructs</strong></a></p>
<ul>
<li><strong>Author</strong>: Harsh Jhamtani, Varun Gangal, Eduard Hovy, Taylor Berg-Kirkpatrick</li>
<li><strong>Comments</strong>: EMNLP 2021 Short Paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.00768.pdf">TopiOCQA: Open-domain Conversational Question Answering with Topic Switching</a></p>
<ul>
<li><strong>Author</strong>: Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, Siva Reddy</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.01256.pdf">Revisiting Self-Training for Few-Shot Learning of Language Model</a></p>
<ul>
<li><strong>Author</strong>: Yiming Chen, Yan Zhang, Chen Zhang, Grandee Lee, Ran Cheng, Haizhou Li</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.03654.pdf">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a></p>
<ul>
<li><strong>Author</strong>: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="30-sep-21----1-oct-21">30 Sep 21 &ndash; 1 Oct 21</h2>
<p>更新于  12 Oct 2021</p>
<p><a href="https://arxiv.org/pdf/2110.00159.pdf">Building an Efficient and Effective Retrieval-based Dialogue System via Mutual Learning</a></p>
<ul>
<li><strong>Author</strong>: Chongyang Tao, Jiazhan Feng, Chang Liu, Juntao Li, Xiubo Geng, Daxin Jiang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.00269.pdf">A Survey of Knowledge Enhanced Pre-trained Models</a></p>
<ul>
<li><strong>Author</strong>: Jian Yang, Gang Xiao, Yulong Shen, Wei Jiang, Xinyu Hu, Ying Zhang, Jinghui Peng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08840.pdf">On the Influence of Masking Policies in Intermediate Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma, Wen-tau Yih, Xiang Ren, Madian Khabsa</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
