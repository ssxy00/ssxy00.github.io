<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Jun 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Jun 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2106/" />
<meta property="article:published_time" content="2021-06-13T20:46:49+08:00" />
<meta property="article:modified_time" content="2021-06-13T20:46:49+08:00" />


    <title>
  Arxiv Daily | Jun 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2106/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Jun 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-06-13T20:46:49&#43;08:00'>
                June 13, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              10-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 6 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="30-jun-21----1-jul-21">30 Jun 21 &ndash; 1 Jul 21</h2>
<p>更新于  2 Aug 2021</p>
<p><a href="https://arxiv.org/pdf/2107.00061.pdf">All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text</a></p>
<ul>
<li><strong>Author</strong>: Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A. Smith</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.00152.pdf">Controllable Open-ended Question Generation with A New Question Type Ontology</a></p>
<ul>
<li><strong>Author</strong>: Shuyang Cao, Lu Wang</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.00440.pdf">CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Dong Wang, Ning Ding, Piji Li, Hai-Tao Zheng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.00395.pdf">GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph</a></p>
<ul>
<li><strong>Author</strong>: Yunxin Li, Yu Zhao, Baotian Hu, Qingcai Chen, Yang Xiang, Xiaolong Wang, Yuxin Ding, Lin Ma</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.15127.pdf">Improving Zero-Shot Translation by Disentangling Positional Information</a></p>
<ul>
<li><strong>Author</strong>: Danni Liu, Jan Niehues, James Cross, Francisco Guzmán, Xian Li</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01209.pdf">Generative Adversarial Transformers</a></p>
<ul>
<li><strong>Author</strong>: Drew A. Hudson, C. Lawrence Zitnick</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="29-jun-21----30-jun-21">29 Jun 21 &ndash; 30 Jun 21</h2>
<p>更新于 28 Jul 2021</p>
<p><a href="https://arxiv.org/pdf/2106.15846.pdf">Automatically Select Emotion for Response via Personality-affected Emotion Transition</a></p>
<ul>
<li><strong>Author</strong>: Zhiyuan Wen, Jiannong Cao, Ruosong Yang, Shuaiqi Liu, Jiaxing Shen</li>
<li><strong>Comments</strong>: Findings of ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.15880.pdf">Mixed Cross Entropy Loss for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Haoran Li, Wei Lu</li>
<li><strong>Comments</strong>: ICML2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.15903.pdf">Learning to Ask Conversational Questions by Optimizing Levenshtein Distance</a></p>
<ul>
<li><strong>Author</strong>: Zhongkun Liu, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Maarten de Rijke, Ming Zhou</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.16038.pdf">ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information</a></p>
<ul>
<li><strong>Author</strong>: Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, Jiwei Li</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.16213.pdf">On the Power of Saturated Transformers: A View from Circuit Complexity</a></p>
<ul>
<li><strong>Author</strong>: William Merrill, Yoav Goldberg, Roy Schwartz, Noah A. Smith</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.00510.pdf">CIDER: Commonsense Inference for Dialogue Explanation and Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Deepanway Ghosal, Pengfei Hong, Siqi Shen, Navonil Majumder, Rada Mihalcea, Soujanya Poria</li>
<li><strong>Comments</strong>:  SIGDIAL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="28-jun-21----29-jun-21">28 Jun 21 &ndash; 29 Jun 21</h2>
<p>更新于 12 Jul 2021</p>
<p><a href="https://arxiv.org/pdf/2106.15078.pdf">Don’t Take It Literally: An Edit-Invariant Sequence Loss for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Guangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan Liang, Zhen Li, Bowen Zhou, Shuguang Cui, Zhiting Hu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.15217.pdf">Rethinking the Evaluation of Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Jianhao Yan, Chenming Wu, Fandong Meng, Jie Zhou</li>
<li><strong>Comments</strong>: Submitted to NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.05268.pdf">Towards Interpretable Natural Language Understanding with Explanations as Latent Variables</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, Jian Tang</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.04252.pdf">Meta-Learning to Compositionally Generalize</a></p>
<ul>
<li><strong>Author</strong>: Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov</li>
<li><strong>Comments</strong>: ACL2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="25-jun-21----28-jun-21">25 Jun 21 &ndash; 28 Jun 21</h2>
<p>更新于 30 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.14614.pdf">Progressive Open-Domain Response Generation with Multiple Controllable Attributes</a></p>
<ul>
<li><strong>Author</strong>: Haiqin Yang, Xiaoyuan Yao, Yiqun Duan, Jianping Shen, Jie Zhong, Kun Zhang</li>
<li><strong>Comments</strong>: IJCAI'21</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="24-jun-21----25-jun-21">24 Jun 21 &ndash; 25 Jun 21</h2>
<p>更新于 30 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.13353.pdf">Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</a></p>
<ul>
<li><strong>Author</strong>: Robert L. Logan, Ivana Balažević, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.13627.pdf">Language Models are Good Translators</a></p>
<ul>
<li><strong>Author</strong>: Shuo Wang, Zhaopeng Tu, Zhixing Tan, Wenxuan Wang, Maosong Sun, Yang Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.13715.pdf">Learning to Sample Replacements for ELECTRA Pre-Training</a></p>
<ul>
<li><strong>Author</strong>: Yaru Hao, Li Dong, Hangbo Bao, Ke Xu, Furu Wei</li>
<li><strong>Comments</strong>: Findings of ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.10907.pdf">Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Elena Voita, Rico Sennrich, Ivan Titov</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="23-jun-21----24-jun-21">23 Jun 21 &ndash; 24 Jun 21</h2>
<p>更新于 26 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.12672.pdf">Charformer: Fast Character Transformers via Gradient-based Subword Tokenization</a></p>
<ul>
<li><strong>Author</strong>: Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri Zhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.13240.pdf">UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP</a></p>
<ul>
<li><strong>Author</strong>: M Saiful Bari, Tasnim Mohiuddin, Shafiq Joty</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.13985.pdf">Explaining NLP Models via Minimal Contrastive Editing (MICE)</a></p>
<ul>
<li><strong>Author</strong>: Alexis Ross, Ana Marasovic, Matthew E. Peters</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07766.pdf">Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann</li>
<li><strong>Comments</strong>: ESWC'2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.01006.pdf">SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying Nian Wu, Song-Chun Zhu</li>
<li><strong>Comments</strong>: Long paper (oral) accepted by ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="22-jun-21----23-jun-21">22 Jun 21 &ndash; 23 Jun 21</h2>
<p>更新于 26 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.12566.pdf">Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding</a></p>
<ul>
<li><strong>Author</strong>: Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="21-jun-21----22-jun-21">21 Jun 21 &ndash; 22 Jun 21</h2>
<p>更新于 23 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.11520.pdf">BARTSCORE: Evaluating Generated Text as Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Weizhe Yuan, Graham Neubig, Pengfei Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09147.pdf">Enhancing Dialogue Generation via Multi-Level Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Xin Li, Piji Li, Yan Wang, Xiaojiang Liu, Wai Lam</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.10199.pdf">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a></p>
<ul>
<li><strong>Author</strong>: Elad Ben-Zaken, Shauli Ravfogel, Yoav Goldberg</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="18-jun-21----21-jun-21">18 Jun 21 &ndash; 21 Jun 21</h2>
<p>更新于 23 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.10622.pdf">Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?</a></p>
<ul>
<li><strong>Author</strong>: Prasanna Parthasarathi, Joelle Pineau, Sarath Chandar</li>
<li><strong>Comments</strong>: SIGDial 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.10715.pdf">CPM-2: Large-scale Cost-effective Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.05092.pdf">Dialogue Relation Extraction with Document-Level Heterogeneous Graph Attention Networks</a></p>
<ul>
<li><strong>Author</strong>: Hui Chen, Pengfei Hong, Wei Han, Navonil Majumder, Soujanya Poria</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.09954.pdf">Improving Dialog Systems for Negotiation with Personality Modeling</a></p>
<ul>
<li><strong>Author</strong>: Runzhe Yang, Jingxiao Chen, Karthik Narasimhan</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="17-jun-21----18-jun-21">17 Jun 21 &ndash; 18 Jun 21</h2>
<p>更新于 21 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2103.01287.pdf">DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Ziming Li, Dookun Park, Julia Kiseleva, Young-Bum Kim, Sungjin Lee</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-jun-21----17-jun-21">16 Jun 21 &ndash; 17 Jun 21</h2>
<p>更新于 21 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.09063.pdf">Specializing Multilingual Language Models: An Empirical Study</a></p>
<ul>
<li><strong>Author</strong>: Ethan C. Chau, Noah A. Smith</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.09204.pdf">An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Xueqing Liu, Chi Wang</li>
<li><strong>Comments</strong>: ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.09231.pdf">Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases</a></p>
<ul>
<li><strong>Author</strong>: Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.09650.pdf">Multi-head or Single-head? An Empirical Comparison for Transformer Training</a></p>
<ul>
<li><strong>Author</strong>: Liyuan Liu, Jialu Liu, Jiawei Han</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.10088.pdf">Optimizing Data Usage via Differentiable Rewards</a></p>
<ul>
<li><strong>Author</strong>: Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, Graham Neubig</li>
<li><strong>Comments</strong>: ICML 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-jun-21----16-jun-21">15 Jun 21 &ndash; 16 Jun 21</h2>
<p>更新于 20 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.08364.pdf">Unsupervised Enrichment of Persona-grounded Dialog with Background Stories</a></p>
<ul>
<li><strong>Author</strong>: Bodhisattwa Prasad Majumder, Taylor Berg-Kirkpatrick, Julian McAuley, Harsh Jhamtani</li>
<li><strong>Comments</strong>: ACL 2021 oral</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08367.pdf">What Context Features Can Transformer Language Models Use?</a></p>
<ul>
<li><strong>Author</strong>: Joe O’Connor, Jacob Andreas</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08484.pdf">Generative Conversational Networks</a></p>
<ul>
<li><strong>Author</strong>: Alexandros Papangelis, Karthik Gopalakrishnan, Aishwarya Padmakumar, Seokhwan Kim, Gokhan Tur, Dilek Hakkani-Tur</li>
<li><strong>Comments</strong>: SIGDial 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08942.pdf">Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Samuel Kiegeland, Julia Kreutzer</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.08566.pdf">Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models</a></p>
<ul>
<li><strong>Author</strong>: Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang, Yejin Choi</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.01454.pdf">An Information Divergence Measure Between Neural Text and Human Text</a></p>
<ul>
<li><strong>Author</strong>: Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaid Harchaoui</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01345.pdf">Emotion Dynamics in Movie Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Will E. Hipson, Saif M. Mohammad</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.06762.pdf">DIALOGSUM: A Real-Life Scenario Dialogue Summarization Dataset</a></p>
<ul>
<li><strong>Author</strong>: Yulong Chen, Yang Liu, Liang Chen, Yue Zhang</li>
<li><strong>Comments</strong>: ACL findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.04426.pdf">Hash Layers For Large Sparse Models</a></p>
<ul>
<li><strong>Author</strong>: Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07728.pdf">Targeted Data Acquisition for Evolving Negotiation Agents</a></p>
<ul>
<li><strong>Author</strong>: Minae Kwon, Siddharth Karamcheti, Mariano-Florentino Cuellar, Dorsa Sadigh</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="14-jun-21----15-jun-21">14 Jun 21 &ndash; 15 Jun 21</h2>
<p>更新于 18 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.07704.pdf">Text Generation with Efficient (Soft) Q-Learning</a></p>
<ul>
<li><strong>Author</strong>: Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08007.pdf">Unsupervised Abstractive Opinion Summarization by Generating Sentences with Tree-Structured Topic Guidance</a></p>
<ul>
<li><strong>Author</strong>: Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08122.pdf">Sequence-Level Training for Non-Autoregressive Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, Jie Zhou</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.08190.pdf">Question Answering Infused Pre-training of General-Purpose Contextualized Representations</a></p>
<ul>
<li><strong>Author</strong>: Robin Jia, Mike Lewis, Luke Zettlemoyer</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01620.pdf">Disentangling Syntax and Semantics in the Brain with Deep Networks</a></p>
<ul>
<li><strong>Author</strong>: Charlotte Caucheteux, Alexandre Gramfort, Jean-Remi King</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.04554.pdf">A Survey of Transformers</a></p>
<ul>
<li><strong>Author</strong>: Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="11-jun-21----14-jun-21">11 Jun 21 &ndash; 14 Jun 21</h2>
<p>更新于 17 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.06899.pdf">Memory-efficient Transformers via Top-k Attention</a></p>
<ul>
<li><strong>Author</strong>: Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, Jonathan Berant</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07176.pdf">SAS: Self-Augmented Strategy for Language Model Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, Ying Nian Wu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07207.pdf">Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Xiang Lin, Simeng Han, Shafiq Joty</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07345.pdf">Self-Guided Contrastive Learning for BERT Sentence Representations</a></p>
<ul>
<li><strong>Author</strong>: Taeuk Kim, Kang Min Yoo, Sang-goo Lee</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07400.pdf"><strong>Determinantal Beam Search</strong></a></p>
<ul>
<li><strong>Author</strong>: Clara Meister, Martina Forster, Ryan Cotterell</li>
<li><strong>Comments</strong>: ACL-IJCNLP 2021</li>
</ul>
<p>DPP</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07499.pdf">An Empirical Survey of Data Augmentation for Limited Data Learning in NLP</a></p>
<ul>
<li><strong>Author</strong>: Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, Diyi Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06981.pdf">Thinking Like Transformers</a></p>
<ul>
<li><strong>Author</strong>: Gail Weiss, Yoav Goldberg, Eran Yahav</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07139.pdf"><strong>Pre-Trained Models: Past, Present and Future</strong></a></p>
<ul>
<li><strong>Author</strong>: Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, Jun Zhu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07250.pdf">Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding</a></p>
<ul>
<li><strong>Author</strong>: Hidetaka Kamigaito, Katsuhiko Hayashi</li>
<li><strong>Comments</strong>: ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00613.pdf">A Controllable Model of Grounded Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski, Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf, Bill Dolan</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.07003.pdf">Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search</a></p>
<ul>
<li><strong>Author</strong>: Gyuwan Kim, Kyunghyun Cho</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.06561.pdf">GENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, Daniel S. Weld</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.04941.pdf">InFillmore: Frame-Guided Language Generation with Bidirectional Context</a></p>
<ul>
<li><strong>Author</strong>: Jiefu Ou, Nathaniel Weir, Anton Belyy, Felix Yu, Benjamin Van Durme</li>
<li><strong>Comments</strong>: SEM 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.04779.pdf">EL-Attention: Memory Efficient Lossless Attention for Generation</a></p>
<ul>
<li><strong>Author</strong>: Yu Yan, Jiusheng Chen, Weizhen Qi, Nikhil Bhendawade, Yeyun Gong, Nan Duan, Ruofei Zhang</li>
<li><strong>Comments</strong>: ICML 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.04563.pdf">XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation</a></p>
<ul>
<li><strong>Author</strong>: Subhabrata Mukherjee, Ahmed Hassan Awadallah, Jianfeng Gao</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="10-jun-21----11-jun-21">10 Jun 21 &ndash; 11 Jun 21</h2>
<p>更新于 15 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.06038.pdf">Modeling Hierarchical Structures with Continuous Recursive Neural Networks</a></p>
<ul>
<li><strong>Author</strong>: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li><strong>Comments</strong>: ICML 2021 (long talk)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06090.pdf">Graph Neural Networks for Natural Language Processing: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Lingfei Wu, Yu Chen, Kai Shen, Xiaojie Guo, Hanning Gao, Shucheng Li, Jian Pei, Bo Long</li>
</ul>
<p>Survey</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06169.pdf"><strong>BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data</strong></a></p>
<ul>
<li><strong>Author</strong>: Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, Ting Liu</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06363.pdf">To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs</a></p>
<ul>
<li><strong>Author</strong>: Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06411.pdf">Zero-Shot Controlled Generation with Encoder-Decoder Transformers</a></p>
<ul>
<li><strong>Author</strong>: Devamanyu Hazarika, Mahdi Namazifar, Dilek Hakkani-Tür</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.06528.pdf">Local Explanation of Dialogue Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.00010.pdf">UnNatural Language Inference</a></p>
<ul>
<li><strong>Author</strong>: Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams</li>
<li><strong>Comments</strong>: ACL 2021 (Long Paper)</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="9-jun-21----10-jun-21">9 Jun 21 &ndash; 10 Jun 21</h2>
<p>更新于 14 Jun 2021</p>
<p><a href="https://arxiv.org/pdf/2106.05469.pdf">Variational Information Bottleneck for Effective Low-Resource Fine-Tuning</a></p>
<ul>
<li><strong>Author</strong>: Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.05505.pdf">Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Tyler A. Chang, Yifan Xu, Weijian Xu, Zhuowen Tu</li>
<li><strong>Comments</strong>: ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.05691.pdf">Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation</a></p>
<ul>
<li><strong>Author</strong>: Yuanxin Liu, Fandong Meng, Zheng Lin, Weiping Wang, Jie Zhou</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.05894.pdf">Synthesizing Adversarial Negative Responses for Robust Response Ranking and Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Prakhar Gupta, Yulia Tsvetkov, Jeffrey P. Bigham</li>
<li><strong>Comments</strong>: Findings of ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.07144.pdf"><strong>A Cognitive Regularizer for Language Modeling</strong></a></p>
<ul>
<li><strong>Author</strong>: Jason Wei, Clara Meister, Ryan Cotterell</li>
<li><strong>Comments</strong>: ACL 2021</li>
</ul>
<p>uniform information density (UID) hypothesis</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.08399.pdf"><strong>Relative Positional Encoding for Transformers with Linear Complexity</strong></a></p>
<ul>
<li><strong>Author</strong>: Antoine Liutkus, Ondřej Cífka, Shih-Lun Wu, Umut Şimşekli, Yi-Hsuan Yang, Gaël Richard</li>
<li><strong>Comments</strong>: ICML 2021 (long talk)</li>
</ul>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
