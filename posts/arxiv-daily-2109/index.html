<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Sep 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Sep 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2109/" />
<meta property="article:published_time" content="2021-09-06T11:35:11+08:00" />
<meta property="article:modified_time" content="2021-09-06T11:35:11+08:00" />


    <title>
  Arxiv Daily | Sep 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2109/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Sep 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-09-06T11:35:11&#43;08:00'>
                September 6, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              13-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 6 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="23-sep-21----24-sep-21">23 Sep 21 &ndash; 24 Sep 21</h2>
<p>更新于  27 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2101.00416.pdf">Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</a></p>
<ul>
<li><strong>Author</strong>: Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="22-sep-21----23-sep-21">22 Sep 21 &ndash; 23 Sep 21</h2>
<p>更新于  25 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.11034.pdf">Conditional Poisson Stochastic Beam Search</a></p>
<ul>
<li><strong>Author</strong>: Clara Meister, Afra Amini, Tim Vieira, Ryan Cotterell</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.11105.pdf">Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing</a></p>
<ul>
<li><strong>Author</strong>: Haoyu He, Xingjian Shi, Jonas Mueller, Sheng Zha, Mu Li, George Karypis</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.11295.pdf">Dynamic Knowledge Distillation for Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="21-sep-21----22-sep-21">21 Sep 21 &ndash; 22 Sep 21</h2>
<p>更新于  24 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.10480.pdf">DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder</a></p>
<ul>
<li><strong>Author</strong>: Zhenyu Zhang, Tao Guo, Meng Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.10510.pdf">FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Xu Wang, Hainan Zhang, Shuai Zhao, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Bo Cheng, Yanyan Lan</li>
<li><strong>Comments</strong>: EMNLP2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.10686.pdf">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers</a></p>
<ul>
<li><strong>Author</strong>: Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.13119.pdf">Predict-then-Decide: A Predictive Approach for Wait or Answer Task in Dialogue Systems</a></p>
<ul>
<li><strong>Author</strong>: Zehao Lin, Shaobo Cui, Guodun Li, Xiaoming Kang, Feng Ji, Fenglin Li, Zhongzhou Zhao, Haiqing Chen, Yin Zhang</li>
<li><strong>Comments</strong>: IEEE/ACM Transactions on Audio, Speech, and Language Processing</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.10242.pdf">Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach</a></p>
<ul>
<li><strong>Author</strong>: Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, Wei Wei</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.14103.pdf">An Attention Free Transformer</a></p>
<ul>
<li><strong>Author</strong>: Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="20-sep-21----21-sep-21">20 Sep 21 &ndash; 21 Sep 21</h2>
<p>更新于  23 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.10126.pdf">CONVFIT: Conversational Fine-Tuning of Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Ivan Vulić, Pei-Hao Su, Sam Coope, Daniela Gerz, Paweł Budzianowski, Iñigo Casanueva, Nikola Mrkšić, Tsung-Hsien Wen</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.11859.pdf">Not all parameters are born equal: Attention is mostly what you need</a></p>
<ul>
<li><strong>Author</strong>: Nikolay Bogoychev</li>
<li><strong>Comments</strong>: BlackboxNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="17-sep-21----20-sep-21">17 Sep 21 &ndash; 20 Sep 21</h2>
<p>更新于  23 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.08875.pdf">Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona</a></p>
<ul>
<li><strong>Author</strong>: Weixuan Wang, Xiaoling Cai, Chong Hsuan Huang, Haoran Wang, Haonan Lu, Ximing Liu, Wei Peng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.09115.pdf">Do Long-Range Language Models Actually Use Long-Range Context?</a></p>
<ul>
<li><strong>Author</strong>: Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, Mohit Iyyer</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.09519.pdf">PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zhengyu Niu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.09707.pdf">A Plug-and-Play Method for Controlled Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Damián Pascual, Béni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.13076.pdf">Finetuning Pretrained Transformers into RNNs</a></p>
<ul>
<li><strong>Author</strong>: Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-sep-21----17-sep-21">16 Sep 21 &ndash; 17 Sep 21</h2>
<p>更新于  23 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.08460.pdf">Neural Unification for Logic Reasoning over Natural Language</a></p>
<ul>
<li><strong>Author</strong>: Gabriele Picco, Hoang Thanh Lam, Marco Luca Sbodio, Vanessa Lopez Garcia</li>
<li><strong>Comments</strong>: EMNLP2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.08678.pdf">RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Caiming Xiong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.08544.pdf">Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules</a></p>
<ul>
<li><strong>Author</strong>: Forough Arabshahi, Jennifer Lee, Antoine Bosselut, Yejin Choi, Tom Mitchell</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-sep-21----16-sep-21">15 Sep 21 &ndash; 16 Sep 21</h2>
<p>更新于  23 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.07684.pdf">Language Models are Few-shot Multilingual Learners</a></p>
<ul>
<li><strong>Author</strong>: Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07713.pdf">Transferable Persona-Grounded Dialogues via Grounded Minimal Edits</a></p>
<ul>
<li><strong>Author</strong>: Chen Henry Wu, Yinhe Zheng, Xiaoxi Mao, Minlie Huang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07779.pdf">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Lei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, Jie Zhou</li>
<li><strong>Comments</strong>: EMNLP 2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07954.pdf">Improving Unsupervised Question Answering via Summarization-Informed Question Generation</a></p>
<ul>
<li><strong>Author</strong>: Chenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer Foster, Xin Jiang, Qun Liu</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.12907.pdf">Directed Acyclic Graph Network for Conversational Emotion Recognition</a></p>
<ul>
<li><strong>Author</strong>: Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan</li>
<li><strong>Comments</strong>: ACL-IJCNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03910.pdf">A Recipe For Arbitrary Text Style Transfer with Large Language Models</a></p>
<ul>
<li><strong>Author</strong>: Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, Jason Wei</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="14-sep-21----15-sep-21">14 Sep 21 &ndash; 15 Sep 21</h2>
<p>更新于  23 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.06950.pdf">Automatically Exposing Problems with Neural Dialog Models</a></p>
<ul>
<li><strong>Author</strong>: Dian Yu, Kenji Sagae</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07152.pdf">Incorporating Residual and Normalization Layers into Analysis of Masked Language Models</a></p>
<ul>
<li><strong>Author</strong>: Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07293.pdf">Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</a></p>
<ul>
<li><strong>Author</strong>: Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07364.pdf">Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU</a></p>
<ul>
<li><strong>Author</strong>: Patrick Kahardipraja, Brielen Madureira, David Schlangen</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04882.pdf">Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation</a></p>
<ul>
<li><strong>Author</strong>: Yunlong Liang, Fandong Meng, Ying Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04882.pdf">Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation</a></p>
<ul>
<li><strong>Author</strong>: Yunlong Liang, Fandong Meng, Ying Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.06232.pdf">Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters</a></p>
<ul>
<li><strong>Author</strong>: Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan Liu, Genta Indra Winata, Andrea Madotto, Dan Su, Pascale Fung</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="13-sep-21----14-sep-21">13 Sep 21 &ndash; 14 Sep 21</h2>
<p>更新于  22 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.06379.pdf">Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation</a></p>
<ul>
<li><strong>Author</strong>: Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06466.pdf">Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Shiyang Li, Semih Yavuz, Wenhu Chen, Xifeng Yan</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06513.pdf">Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation</a></p>
<ul>
<li><strong>Author</strong>: Chujie Zheng, Minlie Huang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06524.pdf">Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks</a></p>
<ul>
<li><strong>Author</strong>: Yao Qiu, Jinchao Zhang, Jie Zhou</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06717.pdf">Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward</a></p>
<ul>
<li><strong>Author</strong>: Zhe Hu, Zhiwei Cao, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Jinsong Su, Hua Wu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06747.pdf">Adaptive Information Seeking for Open-Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03477.pdf">Document Graph for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Mingzhou Xu, Liangyou Li, Derek F. Wong, Qun Liu,Lidia S. Chao</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01378.pdf">Contrastive Explanations for Model Interpretability</a></p>
<ul>
<li><strong>Author</strong>: Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08455.pdf">Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding</a></p>
<ul>
<li><strong>Author</strong>: Nouha Dziri, Andrea Madotto, Osmar Zaiane, Avishek Joey Bose</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08771.pdf">Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Mozhdeh Gheini, Xiang Ren, Jonathan May</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="10-sep-21----13-sep-21">10 Sep 21 &ndash; 13 Sep 21</h2>
<p>更新于  22 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.05052.pdf">Entity-Based Knowledge Conflicts in Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, Sameer Singh</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05056.pdf">Speaker Turn Modeling for Dialogue Act Classification</a></p>
<ul>
<li><strong>Author</strong>: Zihao He, Leili Tavabi, Kristina Lerman, Mohammad Soleymani</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05406.pdf">Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora</a></p>
<ul>
<li><strong>Author</strong>: Pengda Si, Yao Qiu, Jinchao Zhang, Yiru Wang, Jie Zhou, Yujiu Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05424.pdf">Pairwise Supervised Contrastive Learning of Sentence Representations</a></p>
<ul>
<li><strong>Author</strong>: Dejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05487.pdf">Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05644.pdf">SHAPE: Shifted Absolute Position Embedding for Transformers</a></p>
<ul>
<li><strong>Author</strong>: Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, Kentaro Inui</li>
<li><strong>Comments</strong>: EMNLP 2021 short paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05729.pdf">CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation</a></p>
<ul>
<li><strong>Author</strong>: Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, Xipeng Qiu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05794.pdf">Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions</a></p>
<ul>
<li><strong>Author</strong>: Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeffrey Dalton, Mikhail Burtsev</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05808.pdf">Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</a></p>
<ul>
<li><strong>Author</strong>: Priyanka Sen, Amir Saffari, Armin Oliya</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.03760.pdf">DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement</a></p>
<ul>
<li><strong>Author</strong>: Tianda Li, Jia-Chen Gu, Xiaodan Zhu, Quan Liu, Zhen-Hua Ling, Zhiming Su, Si Wei</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.00055.pdf">Sentence Bottleneck Autoencoders from Transformer Language Models</a></p>
<ul>
<li><strong>Author</strong>: Ivan Montero, Nikolaos Pappas, Noah A. Smith</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="9-sep-21----10-sep-21">9 Sep 21 &ndash; 10 Sep 21</h2>
<p>更新于  20 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.04562.pdf">TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling</a></p>
<ul>
<li><strong>Author</strong>: Huiyuan Xie, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Ann Copestake</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p>[An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next](An Exploratory Study on Long Dialogue Summarization: What Works and What's Next)</p>
<ul>
<li><strong>Author</strong>: Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, Dragomir Radev</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04673.pdf">DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</a></p>
<ul>
<li><strong>Author</strong>: Zeqiu Wu, Bo-Ru Lu, Hannaneh Hajishirzi, Mari Ostendorf</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04867.pdf">Studying word order through iterative shuffling</a></p>
<ul>
<li><strong>Author</strong>: Nikolay Malkin, Sameera Lanka, Pranav Goel, Nebojsa Jojic</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04994.pdf">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization</a></p>
<ul>
<li><strong>Author</strong>: Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.14388.pdf">Universal Sentence Representation Learning with Conditional Masked Language Model</a></p>
<ul>
<li><strong>Author</strong>: Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06644.pdf">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little</a></p>
<ul>
<li><strong>Author</strong>: Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06979.pdf">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</a></p>
<ul>
<li><strong>Author</strong>: Kexin Wang, Nils Reimers, Iryna Gurevych</li>
<li><strong>Comments</strong>: EMNLP 2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08821.pdf">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a></p>
<ul>
<li><strong>Author</strong>: Tianyu Gao, Xingcheng Yao, Danqi Chen</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-sep-21----9-sep-21">8 Sep 21 &ndash; 9 Sep 21</h2>
<p>更新于  20 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.04008.pdf">Graph Based Network with Contextualized Representations of Turns in Dialogue</a></p>
<ul>
<li><strong>Author</strong>: Bongseok Lee, Yong Suk Choi</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08164.pdf">Editing Factual Knowledge in Language Models</a></p>
<ul>
<li><strong>Author</strong>: Nicola De Cao, Wilker Aziz, Ivan Titov</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04084.pdf">Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems</a></p>
<ul>
<li><strong>Author</strong>: Yicheng Zou, Zhihua Liu, Xingwu Hu, Qi Zhang</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04096.pdf">A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Shilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren∗ , Longhui Zhang, Shujuan Yin</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04212.pdf">Efficient Nearest Neighbor Language Models</a></p>
<ul>
<li><strong>Author</strong>: Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04223.pdf">KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs</a></p>
<ul>
<li><strong>Author</strong>: Yinquan Lu, Haonan Lu, Guirong Fu, Qun Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04332.pdf">PPT: Pre-trained Prompt Tuning for Few-shot Learning</a></p>
<ul>
<li><strong>Author</strong>: Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.11090.pdf">Position Information in Transformers: An Overview</a></p>
<ul>
<li><strong>Author</strong>: Philipp Dufter, Martin Schmitt, Hinrich Schütze</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08202.pdf">Q2 : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="7-sep-21----8-sep-21">7 Sep 21 &ndash; 8 Sep 21</h2>
<p>更新于  19 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2108.13990.pdf">Effective Sequence-to-Sequence Dialogue State Tracking</a></p>
<ul>
<li><strong>Author</strong>: Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03772.pdf">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension</a></p>
<ul>
<li><strong>Author</strong>: Yiyang Li, Hai Zhao</li>
<li><strong>Comments</strong>: EMNLP 2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03564.pdf">NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task —— Next Sentence Prediction</a></p>
<ul>
<li><strong>Author</strong>: Yi Sun, Yu Zheng, Chao Hao, Hangping Qiu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03645.pdf">Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach</a></p>
<ul>
<li><strong>Author</strong>: Víctor M. Sánchez-Cartagena, Miquel Esplà-Gomis Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="6-sep-21----7-sep-21">6 Sep 21 &ndash; 7 Sep 21</h2>
<p>更新于  19 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.02882.pdf">Integrating Regular Expressions with Neural Networks via DFA</a></p>
<ul>
<li><strong>Author</strong>: Shaobo Li, Qun Liu, Xin Jiang, Yichun Yin, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Lifeng Shang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02950.pdf">Paraphrase Generation as Unsupervised Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Chun Fan, Yufei Tian, Yuxian Meng, Nanyun Peng, Xiaofei Sun, Fei Wu, Jiwei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03079.pdf">GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation</a></p>
<ul>
<li><strong>Author</strong>: Derek Chen, Zhou Yu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03155.pdf">PAUSE: Positive and Annealed Unlabeled Sentence Embedding</a></p>
<ul>
<li><strong>Author</strong>: Lele Cao, Emil Larsson, Vilhelm von Ehrenheim, Dhiana Deva Cavalcanti Rocha, Anna Martin, Sonja Horn</li>
<li><strong>Comments</strong>: EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.03199.pdf">Unsupervised Conversation Disentanglement through Co-Training</a></p>
<ul>
<li><strong>Author</strong>: Hui Liu, Zhan Shi, Xiaodan Zhu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.00234.pdf">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers</a></p>
<ul>
<li><strong>Author</strong>: Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo</li>
<li><strong>Comments</strong>: EMNLP 2021 Findings</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-sep-21----6-sep-21">3 Sep 21 &ndash; 6 Sep 21</h2>
<p>更新于  16 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.01819.pdf">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</a></p>
<ul>
<li><strong>Author</strong>: Atsuki Yamaguchi, George Chrysostomou, Katerina Margatina, Nikolaos Aletras</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.01958.pdf">SIDECONTROL: Controlled Open-domain Dialogue Generation via Additive Side Networks</a></p>
<ul>
<li><strong>Author</strong>: Wanyu Du, Yangfeng Ji</li>
<li><strong>Comments</strong>: EMNLP2021 (Findings)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02247.pdf">STaCK: Sentence Ordering with Temporal Commonsense Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02492.pdf">DIALOGLM: Pre-trained Model for Long Dialogue Understanding and Summarization</a></p>
<ul>
<li><strong>Author</strong>: Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02513.pdf">Proto: A Neural Cocktail for Generating Appealing Conversations</a></p>
<ul>
<li><strong>Author</strong>: Sougata Saha, Souvik Das, Elizabeth Soper, Erin Pacquetet, Rohini K. Srihari</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02550.pdf">You should evaluate your language model on marginal likelihood over tokenisations</a></p>
<ul>
<li><strong>Author</strong>: Kris Cao, Laura Rimell</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.14913.pdf">Transformer Feed-Forward Layers Are Key-Value Memories</a></p>
<ul>
<li><strong>Author</strong>: Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-sep-21----3-sep-21">2 Sep 21 &ndash; 3 Sep 21</h2>
<p>更新于  7 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2109.01330.pdf">Detecting Speaker Personas from Conversational Texts</a></p>
<ul>
<li><strong>Author</strong>: Jia-Chen Gu, Zhen-Hua Ling, Yu Wu, Quan Liu, Zhigang Chen, Xiaodan Zhu</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.01396.pdf">Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT</a></p>
<ul>
<li><strong>Author</strong>: Elena Voita, Rico Sennrich, Ivan Titov</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2109.01652"><strong>Finetuned Language Models Are Zero-Shot Learners</strong></a></p>
<ul>
<li><strong>Author</strong>: Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.10333.pdf">CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation</a></p>
<ul>
<li><strong>Author</strong>: Wenchang Ma, Ryuichi Takanobu, Minlie Huang</li>
<li><strong>Comments</strong>:  EMNLP 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.14682.pdf">CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade</a></p>
<ul>
<li><strong>Author</strong>: Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.08102.pdf">Affective Decoding for Empathetic Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Chengkun Zeng, Guanyi Chen, Chenghua Lin, Ruizhe Li, Zhigang Chen</li>
<li><strong>Comments</strong>: Long paper accepted to INLG 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-sep-21----2-sep-21">1 Sep 21 &ndash; 2 Sep 21</h2>
<p>更新于  6 Sep 2021</p>
<p><a href="https://arxiv.org/pdf/2104.04466.pdf">Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking</a></p>
<ul>
<li><strong>Author</strong>: Weizhe Lin, Bo-Hsiang Tseng, Bill Byrne</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.09574.pdf">Probing Commonsense Explanation in Dialogue Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Pei Zhou, Pegah Jandaghi, Hyundong Cho, Bill Yuchen Lin, Jay Pujara, Xiang Ren</li>
<li><strong>Comments</strong>: EMNLP 2021-Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.09084.pdf">Fastformer: Additive Attention Can Be All You Need</a></p>
<ul>
<li><strong>Author</strong>: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, Xing Xie</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.09193.pdf">Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer</a></p>
<ul>
<li><strong>Author</strong>: Chuhan Wu, Fangzhao Wu, Tao Qi, Binxing Jiao, Daxin Jiang, Yongfeng Huang, Xing Xie</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.09355.pdf">One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles</a></p>
<ul>
<li><strong>Author</strong>: Zhengyi Ma, Zhicheng Dou, Yutao Zhu, Hanxun Zhong, Ji-Rong Wen</li>
<li><strong>Comments</strong>: SIGIR 2021</li>
</ul>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
