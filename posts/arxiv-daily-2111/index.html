<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Nov 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Nov 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2111/" />
<meta property="article:published_time" content="2021-11-09T22:46:50+08:00" />
<meta property="article:modified_time" content="2021-11-09T22:46:50+08:00" />


    <title>
  Arxiv Daily | Nov 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2111/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Nov 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-11-09T22:46:50&#43;08:00'>
                November 9, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              5-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 11 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="17-nov-21----18-nov-21">17 Nov 21 &ndash; 18 Nov 21</h2>
<p>更新于  19 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.09388.pdf">Minimum Bayes Risk Decoding with Neural Metrics of Translation Quality</a></p>
<ul>
<li><strong>Author</strong>: Markus Freitag, David Grangier, Qijun Tan, Bowen Liang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.09509.pdf">How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN</a></p>
<ul>
<li><strong>Author</strong>: R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.09543.pdf">DeBERTaV3: Improving DeBERTa using ELECTRAStyle Pre-Training with Gradient-Disentangled Embedding Sharing</a></p>
<ul>
<li><strong>Author</strong>: Pengcheng He, Jianfeng Gao, Weizhu Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.09714.pdf">You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling</a></p>
<ul>
<li><strong>Author</strong>: Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh</li>
<li><strong>Comments</strong>: ICML (2021)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.06387.pdf">Rationales for Sequential Predictions</a></p>
<ul>
<li><strong>Author</strong>: Keyon Vafa, Yuntian Deng, David M. Blei, Alexander M. Rush</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-nov-21----17-nov-21">16 Nov 21 &ndash; 17 Nov 21</h2>
<p>更新于  18 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.08808.pdf">User Response and Sentiment Prediction for Automatic Dialogue Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Sarik Ghazarian, Behnam Hedayatnia, Alexandros Papangelis, Yang Liu, Dilek Hakkani-Tur</li>
<li><strong>Comments</strong>: EMNLP 2021 Evaluations and Assessments of Neural Conversation Systems Workshop</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01371.pdf">Convex Aggregation for Opinion Summarization</a></p>
<ul>
<li><strong>Author</strong>: Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, Wang-Chiew Tan</li>
<li><strong>Comments</strong>: Findings of EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08768.pdf">Constrained Language Models Yield Few-Shot Semantic Parsers</a></p>
<ul>
<li><strong>Author</strong>: Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-nov-21----16-nov-21">15 Nov 21 &ndash; 16 Nov 21</h2>
<p>更新于  17 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.08609.pdf">Document AI: Benchmarks, Models and Applications</a></p>
<ul>
<li><strong>Author</strong>: Lei Cui, Yiheng Xu, Tengchao Lv, Furu Wei</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.08647.pdf">DataCLUE: A Benchmark Suite for Data-centric NLP</a></p>
<ul>
<li><strong>Author</strong>: Liang Xu, Jiacheng Liu, Xiang Pan, Xiaojing Lu, Xiaofeng Hou</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.08546.pdf">Interpreting Language Models Through Knowledge Graph Extraction</a></p>
<ul>
<li><strong>Author</strong>: Vinitra Swamy, Angelika Romanou, Martin Jaggi</li>
<li><strong>Comments</strong>: NeurIPS 2021: eXplainable AI for Debugging and Diagnosis Workshop</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="12-nov-21----15-nov-21">12 Nov 21 &ndash; 15 Nov 21</h2>
<p>更新于  16 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.07180.pdf">Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Yizhen Zhang, Minkyu Choi, Kuan Han, Zhongming Liu</li>
<li><strong>Comments</strong>: Neurips 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.02080.pdf">An Explanation of In-context Learning as Implicit Bayesian Inference</a></p>
<ul>
<li><strong>Author</strong>: Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="11-nov-21----12-nov-21">11 Nov 21 &ndash; 12 Nov 21</h2>
<p>更新于  15 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.06832.pdf">Speeding Up Entmax</a></p>
<ul>
<li><strong>Author</strong>: Maxat Tezekbayev, Vassilina Nikoulina, Matthias Gallé, Zhenisbek Assylbekov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.06464.pdf">Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication</a></p>
<ul>
<li><strong>Author</strong>: Łukasz Kuciński, Tomasz Korbak, Paweł Kołodziej, Piotr Miłoś</li>
<li><strong>Comments</strong>:  NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.02738.pdf">Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica</a></p>
<ul>
<li><strong>Author</strong>: Shirley Anugrah Hayati, Dongyeop Kang, Lyle Ungar</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="10-nov-21----11-nov-21">10 Nov 21 &ndash; 11 Nov 21</h2>
<p>更新于  13 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.06103.pdf">Towards Robust Knowledge Graph Embedding via Multi-task Reinforcement Learning</a></p>
<ul>
<li><strong>Author</strong>: Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, Chao Li, Hui Xiong, Qing He, Yongjun Xu</li>
<li><strong>Comments</strong>: TKDE</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="9-nov-21----10-nov-21">9 Nov 21 &ndash; 10 Nov 21</h2>
<p>更新于  13 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.05407.pdf">Learning Logic Rules for Document-level Relation Extraction</a></p>
<ul>
<li><strong>Author</strong>: Dongyu Ru, Changzhi Sun, Jiangtao Feng, Lin Qiu, Hao Zhou, Weinan Zhang, Yong Yu, Lei Li</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.05825.pdf">A Two-Stage Approach towards Generalization in Knowledge Base Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Srinivas Ravishankar, June Thai, Ibrahim Abdelaziz, Nandana Mihidukulasooriya, Tahira Naseem, Pavan Kapanipathi, Gaetano Rossilleo, Achille Fokoue</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.12637.pdf">Oh My Mistake!: Toward Realistic Dialogue State Tracking including Turnback Utterances</a></p>
<ul>
<li><strong>Author</strong>: Takyoung Kim, Yukyung Lee, Hoonsang Yoon, Pilsung Kang, Junseong Bang, Misuk Kim</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-nov-21----9-nov-21">8 Nov 21 &ndash; 9 Nov 21</h2>
<p>更新于  13 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.04933.pdf">DSBERT:Unsupervised Dialogue Structure learning with BERT</a></p>
<ul>
<li><strong>Author</strong>: Bingkun Chen, Shaobing Dai, Yang Li, Shenghua Zheng, Lei Liao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.05204.pdf">Reason first, then respond: Modular Generation for Knowledge-infused Dialogue</a></p>
<ul>
<li><strong>Author</strong>: Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, Jason Weston</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.05193.pdf">A Survey on Green Deep Learning</a></p>
<ul>
<li><strong>Author</strong>: Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, Lei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.10685.pdf">Controllable Generation from Pre-trained Language Models via Inverse Prompting</a></p>
<ul>
<li><strong>Author</strong>: Xu Zou, Da Yin, Qingyang Zhong, Ming Ding, Hongxia Yang, Zhilin Yang, Jie Tang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.03143.pdf">CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings</a></p>
<ul>
<li><strong>Author</strong>: Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, Alex Rogozhnikov</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="5-nov-21----8-nov-21">5 Nov 21 &ndash; 8 Nov 21</h2>
<p>更新于  12 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.04130.pdf">NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework</a></p>
<ul>
<li><strong>Author</strong>: Xingcheng Yao, Yanan Zheng, Xiaocong Yang, Zhilin Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.04198.pdf">TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Yixuan Su, Fangyu Liu, Zaiqiao Meng, Lei Shu, Ehsan Shareghi, Nigel Collier</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08762.pdf">Case-Based Reasoning for Natural Language Queries over Knowledge Bases</a></p>
<ul>
<li><strong>Author</strong>: Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="4-nov-21----5-nov-21">4 Nov 21 &ndash; 5 Nov 21</h2>
<p>更新于  10 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.03642.pdf">Grounded Graph Decoding Improves Compositional Generalization in Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Yu Gai, Paras Jain, Wendi Zhang, Joseph E. Gonzalez, Dawn Song, Ion Stoica</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.01828.pdf">Emergent Discrete Communication in Semantic Spaces</a></p>
<ul>
<li><strong>Author</strong>: Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, Julie Shah</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-nov-21----4-nov-21">3 Nov 21 &ndash; 4 Nov 21</h2>
<p>更新于  10 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.02570.pdf">CLUES: Few-Shot Learning Evaluation in Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Subhabrata Mukherjee, Xiaodong Liu, Guoqing Zheng, Saghar Hosseini, Hao Cheng Greg Yang, Christopher Meek, Ahmed Hassan Awadallah, Jianfeng Gao</li>
<li><strong>Comments</strong>: NeurIPS 2021 Datasets and Benchmarks Track</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.02643.pdf">Response Generation with Context-Aware Prompt Learning</a></p>
<ul>
<li><strong>Author</strong>: Xiaodong Gu, Kang Min Yoo, Sang-Woo Lee</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.02878.pdf">Unsupervised and Distributional Detection of Machine-Generated Text</a></p>
<ul>
<li><strong>Author</strong>: Matthias Gallé, Jos Rozen, Germán Kruszewski, Hady Elsahar</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-nov-21----3-nov-21">2 Nov 21 &ndash; 3 Nov 21</h2>
<p>更新于  10 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2104.08698.pdf"><strong>A Simple and Effective Positional Encoding for Transformers</strong></a></p>
<ul>
<li><strong>Author</strong>: Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, Yin-Wen Chang, Chun-Sung Ferng</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.01540.pdf">Luna: Linear Unified Nested Attention</a></p>
<ul>
<li><strong>Author</strong>: Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, Luke Zettlemoyer</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.12566.pdf">Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding</a></p>
<ul>
<li><strong>Author</strong>: Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-nov-21----2-nov-21">1 Nov 21 &ndash; 2 Nov 21</h2>
<p>更新于  10 Nov 2021</p>
<p><a href="https://arxiv.org/pdf/2111.01322.pdf">Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP</a></p>
<ul>
<li><strong>Author</strong>: Trapit Bansal, Karthick Gunasekaran, Tong Wang, Tsendsuren Munkhdalai, Andrew McCallum</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
