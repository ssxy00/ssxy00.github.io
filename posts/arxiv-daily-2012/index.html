<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Dec 2020"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Dec 2020" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2012/" />
<meta property="article:published_time" content="2020-12-03T10:15:28+08:00" />
<meta property="article:modified_time" content="2020-12-03T10:15:28+08:00" />


    <title>
  Arxiv Daily | Dec 2020 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2012/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Dec 2020</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-12-03T10:15:28&#43;08:00'>
                December 3, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              16-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2020 年 12 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="21-dec-20----22-dec-20">21 Dec 20 &ndash; 22 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.11635.pdf">A Distributional Approach to Controlled Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Muhammad Khalifa, Hady Elsahar, Marc Dymetman</li>
<li><strong>Comments</strong>: Under review at ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11820.pdf">Recognizing Emotion Cause in Conversations</a></p>
<ul>
<li><strong>Author</strong>: Soujanya Poria, Navonil Majumder, Devamanyu Hazarika, Deepanway Ghosal, Rishabh Bhardwaj, Samson Yu Bai Jian, Romila Ghosh, Niyati Chhaya, Alexander Gelbukh, Rada Mihalcea</li>
</ul>
<p>&lsquo;We introduce the task of <em>recognizing emotion cause in conversations</em> with an accompanying dataset named RECCON.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11881.pdf">Undivided Attention: Are Intermediate Layers Necessary for BERT?</a></p>
<ul>
<li><strong>Author</strong>: Sharath Nittur Sridhar, Anthony Sarah</li>
</ul>
<p>这篇工作研究了 BERT 的 FFN 层。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11937.pdf">Learning to Retrieve Entity-Aware Knowledge and Generate Responses with Copy Mechanism for Task-Oriented Dialogue Systems</a></p>
<ul>
<li><strong>Author</strong>: Chao-Hong Tan, Xiaoyu Yang, Zi’ou Zheng, Yufei Feng, Tianda Li, Jia-Chen Gu, Quan Liu, Dan Liu, Zhen-Hua Ling, Xiaodan Zhu</li>
<li><strong>Comments</strong>: AAAI 2021, Workshop on DSTC 9</li>
</ul>
<p>DSTC9 Track1 的参赛模型。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11995.pdf">Pre-Training a Language Model Without Human Language</a></p>
<ul>
<li><strong>Author</strong>: Cheng-Han Chiang, Hung-yi Lee</li>
<li><strong>Comments</strong>: work in progress</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08987.pdf">Discovering New Intents with Deep Aligned Clustering</a></p>
<ul>
<li><strong>Author</strong>: Hanlei Zhang, Hua Xu, Ting-En Lin, Rui Lv</li>
<li><strong>Comments</strong>: AAAI 2021 (Main Track, Long Paper)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.02217.pdf">Hopfield Networks is All You Need</a></p>
<ul>
<li><strong>Author</strong>: Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="18-dec-20----21-dec-20">18 Dec 20 &ndash; 21 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.10586.pdf">Finding Sparse Structure for Domain Specific Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Jianze Liang, Chengqi Zhao, Mingxuan Wang, Xipeng Qiu, Lei Li</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.10813.pdf">Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection</a></p>
<ul>
<li><strong>Author</strong>: Yikang Li, Pulkit Goel, Varsha Kuppur Rajendra, Har Simrat Singh, Jonathan Francis, Kaixin Ma, Eric Nyberg, Alessandro Oltramari</li>
<li><strong>Comments</strong>: AAAI-CSKG 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11099.pdf">A Graph Reasoning Network for Multi-turn Response Selection via Customized Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Yongkang Liu, Shi Feng, Daling Wang, Kaisong Song, Feiliang Ren, Yifei Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11357.pdf">Self-attention Comparison Module for Boosting Performance on Retrieval-based Open-Domain Dialog Systems</a></p>
<ul>
<li><strong>Author</strong>: Tian Lan, Xian-Ling Mao, Zhipeng Zhao, Wei Wei, Heyan Huang</li>
</ul>
<p>&lsquo;Intuitively, better decisions could be made when the models can get access to the comparison information among all the candidate responses.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.11014.pdf">KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA</a></p>
<ul>
<li><strong>Author</strong>: Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.10188.pdf"><strong>Residual Energy-Based Models for Text</strong></a></p>
<ul>
<li><strong>Author</strong>: Anton Bakhtin, Yuntian Deng, Sam Gross,􏰕 Myle Ott􏰕, Marc’Aurelio Ranzato,􏰕 Arthur Szlam</li>
<li><strong>Comments</strong>: long journal version</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.01474.pdf">SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration</a></p>
<ul>
<li><strong>Author</strong>: Mengzuo Huang, Feng Li, Wuhe Zou, Weidong Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="17-dec-20----18-dec-20">17 Dec 20 &ndash; 18 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.10033.pdf">Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning</a></p>
<ul>
<li><strong>Author</strong>: Jerry Zikun Chen, Shi Yu, Haoran Wang</li>
<li><strong>Comments</strong>: Workshop on the 9th Dialog System TechnologyChallenge (DSTC-9), AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-dec-20----17-dec-20">16 Dec 20 &ndash; 17 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.09446.pdf">Unsupervised Learning of Discourse Structures using a Tree Autoencoder</a></p>
<ul>
<li><strong>Author</strong>: Patrick Huber, Giuseppe Carenini</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.09823.pdf">Continual Lifelong Learning in Natural Language Processing: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Magdalena Biesialska, Katarzyna Biesialska, Marta R. Costa-jussà</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.01622.pdf">Adversarial Language Games for Advanced Natural Language Intelligence</a></p>
<ul>
<li><strong>Author</strong>: Yuan Yao, Haoxi Zhong, Zhengyan Zhang, Xu Han, Xiaozhi Wang, Kai Zhang, Chaojun Xiao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>In this work, we propose a challenging adversarial language game called <a href="https://github.com/thunlp/AdversarialTaboo"><em>Adversarial Taboo</em></a>.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.10464.pdf">Fluent Response Generation for Conversational Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Ashutosh Baheti, Alan Ritter, Kevin Small</li>
<li><strong>Comments</strong>: ACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.07185.pdf">Critical Thinking for Language Models</a></p>
<ul>
<li><strong>Author</strong>: Gregor Betz, Christian Voigt, Kyle Richardson</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.12539.pdf">Topic-Aware Multi-turn Dialogue Modeling</a></p>
<ul>
<li><strong>Author</strong>: Yi Xu, Hai Zhao, Zhuosheng Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-dec-20----16-dec-20">15 Dec 20 &ndash; 16 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.08561.pdf"><strong>Pre-Training Transformers as Energy-Based Cloze Models</strong></a></p>
<ul>
<li><strong>Author</strong>: Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning</li>
</ul>
<p>&lsquo;It assigns a scalar energy score to each input token indicating how likely it is given its context.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08695.pdf">DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition</a></p>
<ul>
<li><strong>Author</strong>: Weizhou Shen, Junqing Chen, Xiaojun Quan, Zhixian Xie</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08789.pdf">Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training</a></p>
<ul>
<li><strong>Author</strong>: Chen Xing, Wencong Xiao, Yong Li, Wei Lin</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08790.pdf">Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference</a></p>
<ul>
<li><strong>Author</strong>: Yichao Zhou, Yu Yan, Rujun Han, J. Harry Caufield, Kai-Wei Chang, Yizhou Sun, Peipei Ping, Wei Wang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p><em>logics</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08883.pdf">Multi-type Disentanglement without Adversarial Training</a></p>
<ul>
<li><strong>Author</strong>: Lei Sha, Thomas Lukasiewicz</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.09005.pdf">Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Neuro-Symbolic Algorithms</a></p>
<ul>
<li><strong>Author</strong>: Claudio Pinhanez, Paulo Cavalin, Victor Ribeiro, Heloisa Candello, Julio Nogima, Ana Appel, Mauro Pichiliani, Maira Gatti de Bayser, Melina Guerra, Henrique Ferreira, Gabriel Malfatti</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1911.02215.pdf"><strong>Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information</strong></a></p>
<ul>
<li><strong>Author</strong>: Qiu Ran, Yankai Lin, Peng Li, Jie Zhou</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14996.pdf">Segatron: Segment-Aware Transformer for Language Modeling and Understanding</a></p>
<ul>
<li><strong>Author</strong>: He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>&lsquo;we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.04703.pdf">Do Response Selection Models Really Know What’s Next? Utterance Manipulation Strategies For Multi-turn Response Selection</a></p>
<ul>
<li><strong>Author</strong>: Taesun Whang, Dongyub Lee, Dongsuk Oh, Chanhee Lee, Kijong Han, Dong-hun Lee, Saebyeok Lee</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.12719.pdf">Stylized Dialogue Response Generation Using Stylized Unpaired Texts</a></p>
<ul>
<li><strong>Author</strong>: Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, Minlie Huang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="14-dec-20----15-dec-20">14 Dec 20 &ndash; 15 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.08377.pdf">CARE: Commonsense-Aware Emotional Response Generation with Latent Concepts</a></p>
<ul>
<li><strong>Author</strong>: Peixiang Zhong, Di Wang, Pengfei Li, Chen Zhang, Hao Wang, Chunyan Miao</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08383.pdf">Keyword-Guided Neural Conversational Model</a></p>
<ul>
<li><strong>Author</strong>: Peixiang Zhong, Yong Liu, Hao Wang, Chunyan Miao</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08478.pdf">Nested Named Entity Recognition with Partially-Observed TreeCRFs</a></p>
<ul>
<li><strong>Author</strong>: Yao Fu, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p><em>来自阿里巴巴达摩院的一篇工作，听报告的时候感觉挺厉害的。</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08492.pdf">Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks</a></p>
<ul>
<li><strong>Author</strong>: Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, Yan Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.03863.pdf">Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Kaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan Bisk, Eric Nyberg, Alessandro Oltramari</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="11-dec-20----14-dec-20">11 Dec 20 &ndash; 14 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.06717.pdf">Mapping the Timescale Organization of Neural Language Models</a></p>
<ul>
<li><strong>Author</strong>: Hsiang-Yun Sherry Chien, Jinhan Zhang, Christopher. J. Honey</li>
<li><strong>Comments</strong>: submitted to ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2012.07280">Contrastive Learning with Adversarial Perturbations for Conditional Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Seanie Lee, Dong Bok Lee, Sung Ju Hwang</li>
<li><strong>Comments</strong>: under review</li>
</ul>
<p>这篇工作关注的是 seq2seq model 以 teacher forcing 方式训练时的 exposure bias 问题。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.07300.pdf">Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and Context-Aware Auto-Encoders</a></p>
<ul>
<li><strong>Author</strong>: Yicheng Zou, Jun Lin, Lujun Zhao, Yangyang Kang, Zhuoren Jiang, Changlong Sun, Qi Zhang, Xuanjing Huang, Xiaozhong Liu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p><em>是不是可以利用 chat summarization 方法来处理对话历史呢？</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.07335.pdf">LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Hao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Guiquan Liu, Kaikui Liu, Xiaolong Li</li>
</ul>
<p>这篇工作关注的是 BERT 蒸馏：&lsquo;we propose a knowledge distillation method LRC- BERT based on contrastive learning to fit the output of the intermediate layer from the angular distance aspect.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.07410.pdf"><strong>Reasoning in Dialog: Improving Response Generation by Context Reading Comprehension</strong></a></p>
<ul>
<li><strong>Author</strong>: Xiuying Chen, Zhi Cui, Jiayi Zhang, Chen Wei, Jianwei Cui, Bin Wang, Dongyan Zhao, Rui Yan</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2012.07463">Parameter-Efficient Transfer Learning with Diff Pruning</a></p>
<ul>
<li><strong>Author</strong>: Demi Guo, Alexander M. Rush, Yoon Kim</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.07412.pdf">Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings</a></p>
<ul>
<li><strong>Author</strong>: Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, David Wipf</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.07805.pdf">Extracting Training Data from Large Language Models</a></p>
<ul>
<li><strong>Author</strong>: Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Opera, Colin Raffel</li>
</ul>
<p>&lsquo;We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model’s training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.13542.pdf">Faster Depth-Adaptive Transformers</a></p>
<ul>
<li><strong>Author</strong>: Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, Jinan Xu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>&lsquo;In this paper, we get rid of the halting unit and estimate the required depths in advance, which yields a faster depth-adaptive model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.06504.pdf">Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue</a></p>
<ul>
<li><strong>Author</strong>: Longxiang Liu, Zhuosheng Zhang, Hai Zhao, Xi Zhou, Xiang Zhou</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="10-dec-20----11-dec-20">10 Dec 20 &ndash; 11 Dec 20</h2>
<p>更新于 23 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.05983.pdf">Towards Neural Programming Interfaces</a></p>
<ul>
<li><strong>Author</strong>: Zachary C. Brown, Nathaniel Robinson, David Wingate, Nancy Fulda</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06048.pdf">Reinforced Multi-Teacher Selection for Knowledge Distillation</a></p>
<ul>
<li><strong>Author</strong>: Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, Daxin Jiang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>这篇工作考虑的问题是：在知识蒸馏中，有多个教师的情况下，如何设置每个教师网络蒸馏时的权重。他们利用强化学习 (RL) 为不同的训练样本设置不同的教师网络权重。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06153.pdf">Improving Task-Agnostic BERT Distillation with Layer Mapping Search</a></p>
<ul>
<li><strong>Author</strong>: Xiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu</li>
</ul>
<p>这篇工作考虑的是 BERT 蒸馏问题。之前的工作已经表明了在蒸馏时加入 layer-level 的监督是很有帮助的，但是采取的 layer mapping strategy 比较简单 (e.g., uniform or last-layer)。这篇工作提出用遗传算法 (generic algorithm, GA) 来搜索最优的 layer mapping strategy。实验表明这一做法可以稳定提升表现。</p>
<p><em>关注一下搜索到的 optimal layer mapping strategy</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06460.pdf">Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer</a></p>
<ul>
<li><strong>Author</strong>: Marko Vidoni, Ivan Vulić, Goran Glavaš</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06561.pdf">Comprehension and Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Pavel Naumov, Kevin Ros</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p><em>logics</em></p>
<p>​</p>
<p>​</p>
<h2 id="9-dec-20----10-dec-20">9 Dec 20 &ndash; 10 Dec 20</h2>
<p>更新于 14 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.05395.pdf">Infusing Finetuning with Semantic Dependencies</a></p>
<ul>
<li><strong>Author</strong>: Zhaofeng Wu, Hao Peng, Noah A. Smith</li>
<li><strong>Comments</strong>: TACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.05414.pdf">Rewriter-Evaluator Framework for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Yangming Li, Kaisheng Yao</li>
</ul>
<p>This paper:</p>
<ul>
<li>At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process.</li>
<li>We also propose a prioritized gradient descent (PGD) method that facilitates training the rewriter and the evaluator jointly.</li>
<li>Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models.</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-dec-20----9-dec-20">8 Dec 20 &ndash; 9 Dec 20</h2>
<p>更新于 14 Dec 2020</p>
<p><a href="">Diluted Near-Optimal Expert Demonstrations for Guiding Dialogue Stochastic Policy Optimisation</a></p>
<ul>
<li><strong>Author</strong>: Thibault Cordier, Tanguy Urvoy, Lina M. Rojas-Barahona, Fabrice Lefèvre</li>
<li><strong>Comments</strong>: Human in the Loop Dialogue Systems Workshop, NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04698.pdf">Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text</a></p>
<ul>
<li><strong>Author</strong>: Nishtha Madaan, Inkit Padhi, Naveen Panwar, Diptikalyan Saha</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04882.pdf">Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation</a></p>
<ul>
<li><strong>Author</strong>: Yunlong Liang, Fandong Meng, Ying Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>人类的对话中情绪的表达是 multi-source 的：对话历史，表情，对话者的个性等等，但是这些信息在当前的对话模型工作中并没有被充分利用。这篇工作提出了一个关注 emotion 的对话模型，用 heterogeneous graph neural network 来编码 multi-source 的对话内容，然后用一个 Emotion-Personality-Aware Decoder 来生成 content 和 emotion 都恰当的回复。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.12467.pdf">The Depth-to-Width Interplay in Self-Attention</a></p>
<ul>
<li><strong>Author</strong>: Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, Amnon Shashua</li>
<li><strong>Comments</strong>: NeurIPS 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="7-dec-20----8-dec-20">7 Dec 20 &ndash; 8 Dec 20</h2>
<p>更新于 9 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.04276.pdf">Revisiting Iterative Back-Translation from the Perspective of Compositional Generalization</a></p>
<ul>
<li><strong>Author</strong>: Yinuo Guo, Hualei Zhu, Zeqi Lin, Bei Chen, Jian-Guang Lou, Dongmei Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
<li><strong>Keywords</strong>: compositional generalization ((i.e., the capacity to understand and produce unseen combinations of seen components), iterative back-translation, curriculum iterative back-translation</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04080.pdf">A Taxonomy of Empathetic Response Intents in Human Social Conversations</a></p>
<ul>
<li><strong>Author</strong>: Anuradha Welivita, Pearl Pu</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>这篇工作做了一些 empathetic dialogue generation 方面标注和分析的工作。</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04443.pdf">Extractive Opinion Summarization in Quantized Transformer Spaces</a></p>
<ul>
<li><strong>Author</strong>: Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, Mirella Lapata</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="4-dec-20----7-dec-20">4 Dec 20 &ndash; 7 Dec 20</h2>
<p>更新于 9 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.02943.pdf">Cross-Domain Sentiment Classification with In-Domain Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Tian Li, Xiang Chen, Shanghang Zhang, Zhen Dong, Kurt Keutzer</li>
<li><strong>Comments</strong>: NeurIPS 2020 Workshop on Self-supervised Learning</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02952.pdf">Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation</a></p>
<ul>
<li><strong>Author</strong>: Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, Soroush Vosoughi</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>&lsquo;In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02975.pdf"><strong>Reciprocal Supervised Learning Improves Neural Machine Translation</strong></a></p>
<ul>
<li><strong>Author</strong>: Minkai Xu, Mingxuan Wang, Zhouhan Lin, Hao Zhou, Weinan Zhang, Lei Li</li>
</ul>
<p>self-training 用在 NMT 的一篇工作</p>
<p>&lsquo;RSL first exploits individual models to generate pseudo parallel data, and then cooperatively trains each model on the combined synthetic corpus.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03468.pdf">An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data</a></p>
<ul>
<li><strong>Author</strong>: Lili Wang, Chongyang Gao, Jason Wei, Weicheng Ma, Ruibo Liu, Soroush Vosoughi</li>
<li><strong>Comments</strong>: In proceedings of the 6th Workshop on Noisy User-generated Text (W-NUT) at EMNLP 2020</li>
</ul>
<p>在 text clustering on noisy Twitter data 任务上比较了目前常见的 text representation 方法</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03502.pdf">Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive Meeting Summarization</a></p>
<ul>
<li><strong>Author</strong>: Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Ting Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03539.pdf">UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2</a></p>
<ul>
<li><strong>Author</strong>: Yunyi Yang,Yunhao Li, Xiaojun Quan</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03551.pdf">KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning</a></p>
<ul>
<li><strong>Author</strong>: Bin He, Xin Jiang, Jinghui Xiao, Qun Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03573.pdf">PPKE: Knowledge Representation Learning by Path-based Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Bin He, Di Zhou, Jing Xie, Jinghui Xiao, Xin Jiang, Qun Liu</li>
</ul>
<p>&lsquo;In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02813.pdf">Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</a></p>
<ul>
<li><strong>Author</strong>: Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov</li>
</ul>
<p>&lsquo;In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can (1) quickly perform new tasks in a target modality (i.e. meta-learning) and (2) doing so while being trained on a different source modality.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.13964.pdf">CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Yusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, Maosong Sun</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-dec-20----4-dec-20">3 Dec 20 &ndash; 4 Dec 20</h2>
<p>更新于 7 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.02553.pdf">DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Qi Jia, Hongru Huang, Kenny Q. Zhu</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.09853.pdf">Knowledge-Driven Distractor Generation for Cloze-Style Multiple Choice Questions</a></p>
<ul>
<li><strong>Author</strong>: Siyu Ren, Kenny Q. Zhu</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02462.pdf">Fine-tuning BERT for Low-Resource Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Daniel Grießhaber, Johannes Maucher, Ngoc Thang Vu</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-dec-20----3-dec-20">2 Dec 20 &ndash; 3 Dec 20</h2>
<p>更新于 7 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.01631.pdf">Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Word Embeddings and the Implications to Representation Learning</a></p>
<ul>
<li><strong>Author</strong>: Wei Zhang, Murray Campbell, Yang Yu, Sadhana Kumaravel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01707.pdf">Question Answering over Knowledge Bases by Leveraging Semantic Parsing and Neuro-Symbolic Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander Gray, Ramon Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, Francois Luus, Ndivhuwo Makondo, Nandana Mihindukulasooriya, Tahira Naseem, Sumit Neelam, Lucian Popa, Revanth Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G P Shrivatsa Bhargav, Mo Yu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01775.pdf">DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances</a></p>
<ul>
<li><strong>Author</strong>: Xiaodong Gu, Kang Min Yoo,  Jung-Woo Ha</li>
</ul>
<p><em>（看完了来更新）</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01786.pdf">Self-Explaining Structures Improve NLP Models</a></p>
<ul>
<li><strong>Author</strong>: Zijun Sun, Chun Fan, Qinghong Han, Xiaofei Sun, Yuxian Meng, Fei Wu, Jiwei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01941.pdf">On Extending NLP Techniques from the Categorical to the Latent Space: KL Divergence, Zipf’s Law, and Similarity Search</a></p>
<ul>
<li><strong>Author</strong>: Adam Hare, Yu Chen, Yinan Liu, Zhenming Liu, Christopher G. Brinton</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02030.pdf"><strong>Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks</strong></a></p>
<ul>
<li><strong>Author</strong>: Ileana Rugina, Rumen Dangovski, Li Jing, Preslav Nakov, Marin Soljačić</li>
</ul>
<p>&lsquo;Through attention pruning, we find that about 90% of the attention computation can be reduced for language modelling and about 50% for machine translation and prediction with BERT on GLUE tasks.&rsquo;</p>
<p>&lsquo;We discovered important distinctions between self- and cross-attention patterns.&rsquo;</p>
<p>​</p>
<p>​</p>
<h2 id="1-dec-20----2-dec-20">1 Dec 20 &ndash; 2 Dec 20</h2>
<p>更新于 4 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00893.pdf">Evaluating Explanations: How much do explanations from the teacher aid students?</a></p>
<ul>
<li><strong>Author</strong>: Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00955.pdf">How Can We Know When Language Models Know?</a></p>
<ul>
<li><strong>Author</strong>: Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2012.00958">Interactive Teaching for Conversational AI</a></p>
<ul>
<li><strong>Author</strong>: Qing Ping, Feiyang Niu, Govind Thattai, Joel Chengottusseriyil, Qiaozi Gao, Aishwarya Reganti, Prashanth Rajagopal, Gokhan Tur, Dilek Hakkani-Tur, and Prem Natarajan</li>
<li><strong>Comments</strong>: Human in the Loop Dialogue Systems Workshop @NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01266.pdf">Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains</a></p>
<ul>
<li><strong>Author</strong>: Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang</li>
</ul>
<p>' We first leverage a crossdomain learning process to train the meta-teacher on multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01300.pdf">Learning from others&rsquo; mistakes: Avoiding dataset biases without modeling them</a></p>
<ul>
<li><strong>Author</strong>: Victor Sanh, Thomas Wolf, Yonatan Belinkov, Alexander M. Rush</li>
</ul>
<p>&lsquo;Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.10598.pdf">Shapeshifter Networks: Decoupling Layers from Parameters for Scalable and Effective Deep Learning</a></p>
<ul>
<li><strong>Author</strong>: Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko</li>
</ul>
<p>&lsquo;We evaluate SSNs using seven network architectures across diverse tasks that include image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1% of the parameters.&rsquo;</p>
<p>​</p>
<p>​</p>
<h2 id="30-nov-20----1-dec-20">30 Nov 20 &ndash; 1 Dec 20</h2>
<p>更新于 3 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00052.pdf">Systematically Exploring Redundancy Reduction in Summarizing Long Documents</a></p>
<ul>
<li><strong>Author</strong>: Wen Xiao, Giuseppe Carenini</li>
<li><strong>Comments</strong>: AACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00124.pdf">Extreme Model Compression for On-device Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Kanthashree Mysore Sathyendra, Samridhi Choudhary, Leah Nicolich-Henkin</li>
<li><strong>Comments</strong>:  Long paper at COLING 2020</li>
</ul>
<p>模型压缩</p>
<p>&lsquo;Our approach achieves a compression rate of 97.4% with less than 3.7% degradation in predictive performance.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00190.pdf">Towards a Unified Framework for Emotion Analysis</a></p>
<ul>
<li><strong>Author</strong>: Sven Buechel, Luise Modersohn, Udo Hahn</li>
</ul>
<p>&lsquo;Experiments on 14 datasets indicate that EMOCODER learns an interpretable language-independent representation of emotions, allows seamless absorption of stateof-the-art models, and maintains strong prediction quality, even when tested on unseen combinations of domains and label formats.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00363.pdf">Modifying Memories in Transformer Models</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar</li>
</ul>
<p>&lsquo;In this paper, we propose a new task of explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00366.pdf">An Enhanced Knowledge Injection Model for Commonsense Generation</a></p>
<ul>
<li><strong>Author</strong>: Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuanjing Huang, Nan Duan, Ruofei Zhang</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>&lsquo;We retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00413.pdf">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></p>
<ul>
<li><strong>Author</strong>: Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun</li>
</ul>
<p>超大规模中文预训练语言模型</p>
<ul>
<li>2.6 billion parameters</li>
<li>100GB Chinese training data</li>
<li>Transformer-based autoregressive language model</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00708.pdf">Mutual Information Constraints for Monte-Carlo Objectives</a></p>
<ul>
<li><strong>Author</strong>: Gábor Melis, András György, Phil Blunsom</li>
</ul>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
