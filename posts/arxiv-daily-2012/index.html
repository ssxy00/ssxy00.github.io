<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Dec 2020"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Dec 2020" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2012/" />
<meta property="article:published_time" content="2020-12-03T10:15:28+08:00" />
<meta property="article:modified_time" content="2020-12-03T10:15:28+08:00" />


    <title>
  Arxiv Daily | Dec 2020 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2012/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Dec 2020</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-12-03T10:15:28&#43;08:00'>
                December 3, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              7-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2020 年 12 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="4-dec-20----7-dec-20">4 Dec 20 &ndash; 7 Dec 20</h2>
<p>更新于 9 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.02943.pdf">Cross-Domain Sentiment Classification with In-Domain Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Tian Li, Xiang Chen, Shanghang Zhang, Zhen Dong, Kurt Keutzer</li>
<li><strong>Comments</strong>: NeurIPS 2020 Workshop on Self-supervised Learning</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02952.pdf">Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation</a></p>
<ul>
<li><strong>Author</strong>: Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, Soroush Vosoughi</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>&lsquo;In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02975.pdf"><strong>Reciprocal Supervised Learning Improves Neural Machine Translation</strong></a></p>
<ul>
<li><strong>Author</strong>: Minkai Xu, Mingxuan Wang, Zhouhan Lin, Hao Zhou, Weinan Zhang, Lei Li</li>
</ul>
<p>self-training 用在 NMT 的一篇工作</p>
<p>&lsquo;RSL first exploits individual models to generate pseudo parallel data, and then cooperatively trains each model on the combined synthetic corpus.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03468.pdf">An Empirical Survey of Unsupervised Text Representation Methods on Twitter Data</a></p>
<ul>
<li><strong>Author</strong>: Lili Wang, Chongyang Gao, Jason Wei, Weicheng Ma, Ruibo Liu, Soroush Vosoughi</li>
<li><strong>Comments</strong>: In proceedings of the 6th Workshop on Noisy User-generated Text (W-NUT) at EMNLP 2020</li>
</ul>
<p>在 text clustering on noisy Twitter data 任务上比较了目前常见的 text representation 方法</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03502.pdf">Dialogue Discourse-Aware Graph Convolutional Networks for Abstractive Meeting Summarization</a></p>
<ul>
<li><strong>Author</strong>: Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, Ting Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03539.pdf">UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2</a></p>
<ul>
<li><strong>Author</strong>: Yunyi Yang,Yunhao Li, Xiaojun Quan</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03551.pdf">KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning</a></p>
<ul>
<li><strong>Author</strong>: Bin He, Xin Jiang, Jinghui Xiao, Qun Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03573.pdf">PPKE: Knowledge Representation Learning by Path-based Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Bin He, Di Zhou, Jing Xie, Jinghui Xiao, Xin Jiang, Qun Liu</li>
</ul>
<p>&lsquo;In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02813.pdf">Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</a></p>
<ul>
<li><strong>Author</strong>: Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov</li>
</ul>
<p>&lsquo;In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can (1) quickly perform new tasks in a target modality (i.e. meta-learning) and (2) doing so while being trained on a different source modality.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.13964.pdf">CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Yusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, Maosong Sun</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-dec-20----4-dec-20">3 Dec 20 &ndash; 4 Dec 20</h2>
<p>更新于 7 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.02553.pdf">DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Qi Jia, Hongru Huang, Kenny Q. Zhu</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.09853.pdf">Knowledge-Driven Distractor Generation for Cloze-Style Multiple Choice Questions</a></p>
<ul>
<li><strong>Author</strong>: Siyu Ren, Kenny Q. Zhu</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02462.pdf">Fine-tuning BERT for Low-Resource Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Daniel Grießhaber, Johannes Maucher, Ngoc Thang Vu</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-dec-20----3-dec-20">2 Dec 20 &ndash; 3 Dec 20</h2>
<p>更新于 7 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.01631.pdf">Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Word Embeddings and the Implications to Representation Learning</a></p>
<ul>
<li><strong>Author</strong>: Wei Zhang, Murray Campbell, Yang Yu, Sadhana Kumaravel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01707.pdf">Question Answering over Knowledge Bases by Leveraging Semantic Parsing and Neuro-Symbolic Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Pavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Ravishankar, Salim Roukos, Alexander Gray, Ramon Astudillo, Maria Chang, Cristina Cornelio, Saswati Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, Sairam Gurajada, Hima Karanam, Naweed Khan, Dinesh Khandelwal, Young-Suk Lee, Yunyao Li, Francois Luus, Ndivhuwo Makondo, Nandana Mihindukulasooriya, Tahira Naseem, Sumit Neelam, Lucian Popa, Revanth Reddy, Ryan Riegel, Gaetano Rossiello, Udit Sharma, G P Shrivatsa Bhargav, Mo Yu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01775.pdf">DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances</a></p>
<ul>
<li><strong>Author</strong>: Xiaodong Gu, Kang Min Yoo,  Jung-Woo Ha</li>
</ul>
<p><em>（看完了来更新）</em></p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01786.pdf">Self-Explaining Structures Improve NLP Models</a></p>
<ul>
<li><strong>Author</strong>: Zijun Sun, Chun Fan, Qinghong Han, Xiaofei Sun, Yuxian Meng, Fei Wu, Jiwei Li</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01941.pdf">On Extending NLP Techniques from the Categorical to the Latent Space: KL Divergence, Zipf’s Law, and Similarity Search</a></p>
<ul>
<li><strong>Author</strong>: Adam Hare, Yu Chen, Yinan Liu, Zhenming Liu, Christopher G. Brinton</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.02030.pdf"><strong>Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks</strong></a></p>
<ul>
<li><strong>Author</strong>: Ileana Rugina, Rumen Dangovski, Li Jing, Preslav Nakov, Marin Soljačić</li>
</ul>
<p>&lsquo;Through attention pruning, we find that about 90% of the attention computation can be reduced for language modelling and about 50% for machine translation and prediction with BERT on GLUE tasks.&rsquo;</p>
<p>&lsquo;We discovered important distinctions between self- and cross-attention patterns.&rsquo;</p>
<p>​</p>
<p>​</p>
<h2 id="1-dec-20----2-dec-20">1 Dec 20 &ndash; 2 Dec 20</h2>
<p>更新于 4 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00893.pdf">Evaluating Explanations: How much do explanations from the teacher aid students?</a></p>
<ul>
<li><strong>Author</strong>: Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00955.pdf">How Can We Know When Language Models Know?</a></p>
<ul>
<li><strong>Author</strong>: Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2012.00958">Interactive Teaching for Conversational AI</a></p>
<ul>
<li><strong>Author</strong>: Qing Ping, Feiyang Niu, Govind Thattai, Joel Chengottusseriyil, Qiaozi Gao, Aishwarya Reganti, Prashanth Rajagopal, Gokhan Tur, Dilek Hakkani-Tur, and Prem Natarajan</li>
<li><strong>Comments</strong>: Human in the Loop Dialogue Systems Workshop @NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01266.pdf">Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains</a></p>
<ul>
<li><strong>Author</strong>: Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang</li>
</ul>
<p>' We first leverage a crossdomain learning process to train the meta-teacher on multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01300.pdf">Learning from others&rsquo; mistakes: Avoiding dataset biases without modeling them</a></p>
<ul>
<li><strong>Author</strong>: Victor Sanh, Thomas Wolf, Yonatan Belinkov, Alexander M. Rush</li>
</ul>
<p>&lsquo;Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.10598.pdf">Shapeshifter Networks: Decoupling Layers from Parameters for Scalable and Effective Deep Learning</a></p>
<ul>
<li><strong>Author</strong>: Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko</li>
</ul>
<p>&lsquo;We evaluate SSNs using seven network architectures across diverse tasks that include image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1% of the parameters.&rsquo;</p>
<p>​</p>
<p>​</p>
<h2 id="30-nov-20----1-dec-20">30 Nov 20 &ndash; 1 Dec 20</h2>
<p>更新于 3 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00052.pdf">Systematically Exploring Redundancy Reduction in Summarizing Long Documents</a></p>
<ul>
<li><strong>Author</strong>: Wen Xiao, Giuseppe Carenini</li>
<li><strong>Comments</strong>: AACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00124.pdf">Extreme Model Compression for On-device Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Kanthashree Mysore Sathyendra, Samridhi Choudhary, Leah Nicolich-Henkin</li>
<li><strong>Comments</strong>:  Long paper at COLING 2020</li>
</ul>
<p>模型压缩</p>
<p>&lsquo;Our approach achieves a compression rate of 97.4% with less than 3.7% degradation in predictive performance.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00190.pdf">Towards a Unified Framework for Emotion Analysis</a></p>
<ul>
<li><strong>Author</strong>: Sven Buechel, Luise Modersohn, Udo Hahn</li>
</ul>
<p>&lsquo;Experiments on 14 datasets indicate that EMOCODER learns an interpretable language-independent representation of emotions, allows seamless absorption of stateof-the-art models, and maintains strong prediction quality, even when tested on unseen combinations of domains and label formats.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00363.pdf">Modifying Memories in Transformer Models</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar</li>
</ul>
<p>&lsquo;In this paper, we propose a new task of explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00366.pdf">An Enhanced Knowledge Injection Model for Commonsense Generation</a></p>
<ul>
<li><strong>Author</strong>: Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuanjing Huang, Nan Duan, Ruofei Zhang</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>&lsquo;We retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00413.pdf">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></p>
<ul>
<li><strong>Author</strong>: Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun</li>
</ul>
<p>超大规模中文预训练语言模型</p>
<ul>
<li>2.6 billion parameters</li>
<li>100GB Chinese training data</li>
<li>Transformer-based autoregressive language model</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00708.pdf">Mutual Information Constraints for Monte-Carlo Objectives</a></p>
<ul>
<li><strong>Author</strong>: Gábor Melis, András György, Phil Blunsom</li>
</ul>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
