<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Dec 2020"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Dec 2020" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2012/" />
<meta property="article:published_time" content="2020-12-03T10:15:28+08:00" />
<meta property="article:modified_time" content="2020-12-03T10:15:28+08:00" />


    <title>
  Arxiv Daily | Dec 2020 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2012/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Dec 2020</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2020-12-03T10:15:28&#43;08:00'>
                December 3, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              3-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2020 年 12 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="1-dec-20----2-dec-20">1 Dec 20 &ndash; 2 Dec 20</h2>
<p>更新于 4 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00893.pdf">Evaluating Explanations: How much do explanations from the teacher aid students?</a></p>
<ul>
<li><strong>Author</strong>: Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00955.pdf">How Can We Know When Language Models Know?</a></p>
<ul>
<li><strong>Author</strong>: Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/abs/2012.00958">Interactive Teaching for Conversational AI</a></p>
<ul>
<li><strong>Author</strong>: Qing Ping, Feiyang Niu, Govind Thattai, Joel Chengottusseriyil, Qiaozi Gao, Aishwarya Reganti, Prashanth Rajagopal, Gokhan Tur, Dilek Hakkani-Tur, and Prem Natarajan</li>
<li><strong>Comments</strong>: Human in the Loop Dialogue Systems Workshop @NeurIPS 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01266.pdf">Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains</a></p>
<ul>
<li><strong>Author</strong>: Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang</li>
</ul>
<p>' We first leverage a crossdomain learning process to train the meta-teacher on multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.01300.pdf">Learning from others&rsquo; mistakes: Avoiding dataset biases without modeling them</a></p>
<ul>
<li><strong>Author</strong>: Victor Sanh, Thomas Wolf, Yonatan Belinkov, Alexander M. Rush</li>
</ul>
<p>&lsquo;Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.10598.pdf">Shapeshifter Networks: Decoupling Layers from Parameters for Scalable and Effective Deep Learning</a></p>
<ul>
<li><strong>Author</strong>: Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko</li>
</ul>
<p>&lsquo;We evaluate SSNs using seven network architectures across diverse tasks that include image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1% of the parameters.&rsquo;</p>
<p>​</p>
<p>​</p>
<h2 id="30-nov-20----1-dec-20">30 Nov 20 &ndash; 1 Dec 20</h2>
<p>更新于 3 Dec 2020</p>
<p><a href="https://arxiv.org/pdf/2012.00052.pdf">Systematically Exploring Redundancy Reduction in Summarizing Long Documents</a></p>
<ul>
<li><strong>Author</strong>: Wen Xiao, Giuseppe Carenini</li>
<li><strong>Comments</strong>: AACL 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00124.pdf">Extreme Model Compression for On-device Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Kanthashree Mysore Sathyendra, Samridhi Choudhary, Leah Nicolich-Henkin</li>
<li><strong>Comments</strong>:  Long paper at COLING 2020</li>
</ul>
<p>模型压缩</p>
<p>&lsquo;Our approach achieves a compression rate of 97.4% with less than 3.7% degradation in predictive performance.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00190.pdf">Towards a Unified Framework for Emotion Analysis</a></p>
<ul>
<li><strong>Author</strong>: Sven Buechel, Luise Modersohn, Udo Hahn</li>
</ul>
<p>&lsquo;Experiments on 14 datasets indicate that EMOCODER learns an interpretable language-independent representation of emotions, allows seamless absorption of stateof-the-art models, and maintains strong prediction quality, even when tested on unseen combinations of domains and label formats.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00363.pdf">Modifying Memories in Transformer Models</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar</li>
</ul>
<p>&lsquo;In this paper, we propose a new task of explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00366.pdf">An Enhanced Knowledge Injection Model for Commonsense Generation</a></p>
<ul>
<li><strong>Author</strong>: Zhihao Fan, Yeyun Gong, Zhongyu Wei, Siyuan Wang, Yameng Huang, Jian Jiao, Xuanjing Huang, Nan Duan, Ruofei Zhang</li>
<li><strong>Comments</strong>: COLING 2020</li>
</ul>
<p>&lsquo;We retrieve prototypes from external knowledge to assist the understanding of the scenario for better description generation.&rsquo;</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00413.pdf">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></p>
<ul>
<li><strong>Author</strong>: Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun</li>
</ul>
<p>超大规模中文预训练语言模型</p>
<ul>
<li>2.6 billion parameters</li>
<li>100GB Chinese training data</li>
<li>Transformer-based autoregressive language model</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00708.pdf">Mutual Information Constraints for Monte-Carlo Objectives</a></p>
<ul>
<li><strong>Author</strong>: Gábor Melis, András György, Phil Blunsom</li>
</ul>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
