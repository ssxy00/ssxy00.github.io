<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Apr 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Apr 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2104/" />
<meta property="article:published_time" content="2021-04-06T08:45:54+08:00" />
<meta property="article:modified_time" content="2021-04-06T08:45:54+08:00" />


    <title>
  Arxiv Daily | Apr 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2104/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Apr 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-04-06T08:45:54&#43;08:00'>
                April 6, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              15-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 4 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="14-apr-21----15-apr-21">14 Apr 21 &ndash; 15 Apr 21</h2>
<p>更新于 20 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.07064.pdf">Reformulating Sentence Ordering as Conditional Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Somnath Basu Roy Chowdhury, Faeze Brahman, Snigdha Chaturvedi</li>
</ul>
<p>The input is a set of shuffled sentences with sentence-specific markers and output is a sequence of position markers of the ordered text.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07094.pdf">Static Embeddings as Efficient Knowledge Bases?</a></p>
<ul>
<li><strong>Author</strong>: Philipp Dufter, Nora Kassner, Hinrich Schütze</li>
<li><strong>Comments</strong>: NAACL2021 CRV</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07143.pdf">An Interpretability Illusion for BERT</a></p>
<ul>
<li><strong>Author</strong>: Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viegas, Martin Wattenberg</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07155.pdf">Disentangling Representations of Text by Masking Transformers</a></p>
<ul>
<li><strong>Author</strong>: Xiongyi Zhang, Jan-Willem van de Meent, Byron C. Wallace</li>
</ul>
<p>Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07284.pdf">Consistency Training with Virtual Adversarial Discrete Perturbation</a></p>
<ul>
<li><strong>Author</strong>: Jungsoo Park, Gyuwan Kim, Jaewoo Kang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07412.pdf">XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07545.pdf">Hierarchical Learning for Generation with Long Source Sequences</a></p>
<ul>
<li><strong>Author</strong>: Tobias Rohde, Xiaoxia Wu, Yinhan Liu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07567.pdf"><strong>Retrieval Augmentation Reduces Hallucination in Conversation</strong></a></p>
<ul>
<li><strong>Author</strong>: Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07578.pdf">Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Matteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang, Yoon Kim, SueYeon Chung</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07639.pdf">Demystify Optimization Challenges in Multilingual Transformers</a></p>
<ul>
<li><strong>Author</strong>: Xian Li, Hongyu Gong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.13284.pdf">Pchatbot: A Large-Scale Dataset for Personalized Chatbot</a></p>
<ul>
<li><strong>Author</strong>: Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Yutao Zhu, Zhanliang Liu, Zhicheng Dou, Ji-Rong Wen</li>
</ul>
<p>Chinese Dialogue Dataset</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12527.pdf">Retrieve, Read, Rerank, then Iterate: Answering Open-Domain Questions of Varying Reasoning Steps from Text</a></p>
<ul>
<li><strong>Author</strong>: Peng Qi, Haejun Lee, Oghenetegiri “TG” Sido, Christopher D. Manning</li>
</ul>
<p>We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents—in an iterative fashion.</p>
<p>​</p>
<p>​</p>
<h2 id="13-apr-21----14-apr-21">13 Apr 21 &ndash; 14 Apr 21</h2>
<p>更新于 17 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.06599.pdf">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</a></p>
<ul>
<li><strong>Author</strong>: Guanghui Qin, Jason Eisner</li>
<li><strong>Comments</strong>:  NAACL-HLT 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06644.pdf"><strong>Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little</strong></a></p>
<ul>
<li><strong>Author</strong>: Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela</li>
</ul>
<p>Overall, our results show that purely distributional information largely explains the success of pretraining, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06828.pdf">Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley, Julian McAuley</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.07012.pdf">Sparse Attention with Linear Units</a></p>
<ul>
<li><strong>Author</strong>: Biao Zhang, Ivan Titov, Rico Sennrich</li>
</ul>
<p>In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.15720.pdf"><strong>Progressive Generation of Long Text with Pretrained Language Models</strong></a></p>
<ul>
<li><strong>Author</strong>: Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P. Xing, Zhiting Hu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.03648.pdf"><strong>A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks</strong></a></p>
<ul>
<li><strong>Author</strong>: Nikunj Saunshi, Sadhika Malladi, Sanjeev Arora</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="12-apr-21----13-apr-21">12 Apr 21 &ndash; 13 Apr 21</h2>
<p>更新于 16 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.05824.pdf">Evaluating Saliency Methods for Neural Language Models</a></p>
<ul>
<li><strong>Author</strong>: Shuoyang Ding, Philipp Koehn</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05837.pdf">Relational world knowledge representation in contextual language models: A review</a></p>
<ul>
<li><strong>Author</strong>: Tara Safavi, Danai Koutra</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05847.pdf">Targeted Adversarial Training for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Lis Pereira, Xiaodong Liu, Hao Cheng, Hoifung Poon, Jianfeng Gao, Ichiro Kobayashi</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05882.pdf">Discourse Probing of Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Fajri Koto, Jey Han Lau,  Timothy Baldwin</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05904.pdf">DIRECTPROBE: Studying Representations without Classifiers</a></p>
<ul>
<li><strong>Author</strong>: Yichu Zhou, Vivek Srikumar</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06039.pdf">MultiModalQA: Complex Question Answering over Text, Tables and Images</a></p>
<ul>
<li><strong>Author</strong>: Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06245.pdf">Understanding Hard Negatives in Noise Contrastive Estimation</a></p>
<ul>
<li><strong>Author</strong>: Wenzheng Zhang, Karl Stratos</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06378.pdf">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06387.pdf">EXPLAINABOARD: An Explainable Leaderboard for NLP</a></p>
<ul>
<li><strong>Author</strong>: Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang, Junqi Dai, Yixin Liu, Zihuiwen Ye, Graham Neubig</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.03607.pdf">TuringAdvice: A Generative and Dynamic Evaluation of Language Use</a></p>
<ul>
<li><strong>Author</strong>: Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali Farhadi, Yejin Choi</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>We propose TuringAdvice, a new challenge task and dataset for language understanding models.</p>
<p>The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00619.pdf">Probing Contextual Language Models for Common Ground with Visual Representations</a></p>
<ul>
<li><strong>Author</strong>: Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations?</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.09654.pdf">Generative Imagination Elevates Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Quanyu Long, Mingxuan Wang, Lei Li</li>
</ul>
<p>ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the “imagined representation” to produce a target translation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12148.pdf">ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Dongling Xiao, Yukun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12562.pdf">On the Transformer Growth for Progressive BERT Training</a></p>
<ul>
<li><strong>Author</strong>: Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, Jiawei Han</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In light of our analyses, the proposed method CompoundGrow speeds up BERT pretraining by 73.6% and 82.2% for the base and large models respectively, while achieving comparable performances.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.10598.pdf">Neural Parameter Allocation Search</a></p>
<ul>
<li><strong>Author</strong>: Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko</li>
</ul>
<p>This paper removes these restrictions with a novel task called Neural Parameter Allocation Search (NPAS), where the goal is to generate weights for a network using a given parameter budget. NPAS requires new techniques to morph available parameters to fit any architecture. To address this new task we introduce Shapeshifter Networks (SSNs), which automatically learns where and how to share parameters between all layers in a network, even between layers of varying sizes and operations.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.04898.pdf">Open-Domain Question Answering Goes Conversational via Question Rewriting</a></p>
<ul>
<li><strong>Author</strong>: Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, Srinivas Chappidi</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="9-apr-21----12-apr-21">9 Apr 21 &ndash; 12 Apr 21</h2>
<p>更新于 15 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.04692.pdf">Not All Attention Is All You Need</a></p>
<ul>
<li><strong>Author</strong>: Hongqiu Wu, Hai Zhao, Min Zhang</li>
</ul>
<p>In this paper, we focus on pre-trained language models with self-pruning training design on taskspecific tuning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04748.pdf">Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management</a></p>
<ul>
<li><strong>Author</strong>: Zhengxu Hou, Bang Liu, Ruihui Zhao, Zijing Ou, Yafei Liu, Xi Chen, Yefeng Zheng</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a threelevel hierarchy: domain, act, and slot.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04907.pdf">Disentangled Contrastive Learning for Learning Robust Textual Representations</a></p>
<ul>
<li><strong>Author</strong>: Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang, Huajun Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04909.pdf">EDGE: Enriching Knowledge Graph Embeddings with External Text</a></p>
<ul>
<li><strong>Author</strong>: Saed Rezayi, Handong Zhao, Sungchul Kim, Ryan A. Rossi, Nedim Lipka, Sheng Li</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04923.pdf">Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog</a></p>
<ul>
<li><strong>Author</strong>: Arun Babu, Akshat Shrivastava, Armen Aghajanyan, Ahmed Aly, Angela Fan, Marjan Ghazvininejad</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04946.pdf">UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost</a></p>
<ul>
<li><strong>Author</strong>: Zhen Wu, Lijun Wu, Qi Meng, Yingce Xia, Shufang Xie, Tao Qin, Xinyu Dai, Tie-Yan Liu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>Specifically, we propose an approach named UniDrop to unite three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05115.pdf">Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: James Y. Huang, Kuan-Hao Huang, Kai-Wei Chang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05196.pdf">STYLEPTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</a></p>
<ul>
<li><strong>Author</strong>: Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this paper, we introduce a large-scale benchmark, STYLEPTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05216.pdf">Contextualized Knowledge-aware Attentive Neural Network: Enhancing Answer Selection with Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Wai Lam, Ying Shen</li>
<li><strong>Comments</strong>: TIOS</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05218.pdf">FUDGE: Controlled Text Generation With Future Discriminators</a></p>
<ul>
<li><strong>Author</strong>: Kevin Yang, Dan Klein</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05240.pdf">Factual Probing Is [MASK] : Learning vs. Learning to Recall</a></p>
<ul>
<li><strong>Author</strong>: Zexuan Zhong, Dan Friedman, Danqi Chen</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05336.pdf"><strong>Machine Translation Decoding beyond Beam Search</strong></a></p>
<ul>
<li><strong>Author</strong>: Rémi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05361.pdf">The Great Misalignment Problem in Human Evaluation of NLP Methods</a></p>
<ul>
<li><strong>Author</strong>: Mika Hämäläinen, Khalid Alnajjar</li>
<li><strong>Comments</strong>: Workshop on Human Evaluation of NLP Systems at EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05489.pdf">Continual Learning for Text Classification with Information Disentanglement Based Regularization</a></p>
<ul>
<li><strong>Author</strong>: Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang, Diyi Yang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05514.pdf">Self-Training with Weak Supervision</a></p>
<ul>
<li><strong>Author</strong>: Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, Ahmed Hassan Awadallah</li>
<li><strong>Comments</strong>: NAACL 2021 (Long Paper)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05604.pdf">Semantic Frame Forecast</a></p>
<ul>
<li><strong>Author</strong>: Chieh-Yang Huang, Ting-Hao (Kenneth) Huang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05694.pdf">On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies</a></p>
<ul>
<li><strong>Author</strong>: Tianyi Zhang, Tatsunori B. Hashimoto</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04676.pdf">Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis</a></p>
<ul>
<li><strong>Author</strong>: Xutan Peng, Guanyi Chen, Chenghua Lin, Mark Stevenson</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.05062.pdf">Achieving Model Robustness through Discrete Adversarial Training</a></p>
<ul>
<li><strong>Author</strong>: Maor Ivgi, Jonathan Berant</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.05221.pdf">Compression of Deep Learning Models for Text: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Manish Gupta, Puneet Agrawal</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.09396.pdf">Neural Machine Translation without Embeddings</a></p>
<ul>
<li><strong>Author</strong>: Uri Shaham, Omer Levy</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.06925.pdf">DA-Transformer: Distance-aware Transformer</a></p>
<ul>
<li><strong>Author</strong>: Chuhan Wu, Fangzhao Wu, Yongfeng Huang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12623.pdf">Unsupervised Multi-hop Question Answering by Question Generation</a></p>
<ul>
<li><strong>Author</strong>: Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang Wang</li>
<li><strong>Comments</strong>: NAACL 2021 (long paper)</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12820.pdf">“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses</a></p>
<ul>
<li><strong>Author</strong>: Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.15674.pdf">ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</a></p>
<ul>
<li><strong>Author</strong>: Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.11230.pdf">Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents</a></p>
<ul>
<li><strong>Author</strong>: Mohammad Kachuee, Hao Yuan, Young-Bum Kim, Sungjin Lee</li>
<li><strong>Comments</strong>:  NAACL-HLT 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-apr-21----9-apr-21">8 Apr 21 &ndash; 9 Apr 21</h2>
<p>更新于 13 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.04128.pdf">An Empirical Comparison of Instance Attribution Methods for NLP</a></p>
<ul>
<li><strong>Author</strong>: Pouya Pezeshkpour, Sarthak Jain, Byron C. Wallace, Sameer Singh</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04434.pdf">Larger-Context Tagging: When and Why Does It Work?</a></p>
<ul>
<li><strong>Author</strong>: Jinlan Fu, Liangjing Feng, Qi Zhang, Xuanjing Huang, Pengfei Liu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04470.pdf">Did they answer? Subjective acts and intents in conversational discourse</a></p>
<ul>
<li><strong>Author</strong>: Elisa Ferracane, Greg Durrett, Junyi Jessy Li, Katrin Erk</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.04488.pdf">Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks</a></p>
<ul>
<li><strong>Author</strong>: Hanjie Chen, Song Feng, Jatin Ganhotra, Hui Wan, Chulaka Gunasekara, Sachindra Joshi, Yangfeng Ji</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.05298.pdf">SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine Teaching</a></p>
<ul>
<li><strong>Author</strong>: Baolin Peng, Chunyuan Li, Jinchao, Li Shahin Shayandeh, Lars Liden, Jianfeng Gao</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.02252.pdf">KILT: a Benchmark for Knowledge Intensive Language Tasks</a></p>
<ul>
<li><strong>Author</strong>: Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, Sebastian Riedel</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.08014.pdf">GSum: A General Framework for Guided Neural Abstractive Summarization</a></p>
<ul>
<li><strong>Author</strong>: Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.07432.pdf">Target Guided Emotion Aware Chat Machine</a></p>
<ul>
<li><strong>Author</strong>: Wei Wei, Jiayi Liu, Xianling Mao, Guibin Guo, Feida Zhu, Pan Zhou, Yuchong Hu, Shanshan Feng</li>
<li><strong>Comments</strong>: TIOS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="7-apr-21----8-apr-21">7 Apr 21 &ndash; 8 Apr 21</h2>
<p>更新于 10 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.03474.pdf">Revisiting Simple Neural Probabilistic Language Models</a></p>
<ul>
<li><strong>Author</strong>: Simeng Sun, Mohit Iyyer</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.03514.pdf">Low-Complexity Probing via Finding Subnetworks</a></p>
<ul>
<li><strong>Author</strong>: Steven Cao, Victor Sanh, Alexander M. Rush</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.03869.pdf">Probing BERT in Hyperbolic Spaces</a></p>
<ul>
<li><strong>Author</strong>: Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, Liping Jing</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12919.pdf">Causal Effects of Linguistic Properties</a></p>
<ul>
<li><strong>Author</strong>: Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch, Dhanya Sridhar</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="6-apr-21----7-apr-21">6 Apr 21 &ndash; 7 Apr 21</h2>
<p>更新于 9 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.02851.pdf">Interpreting A Pre-trained Model Is A Key For Model Architecture Optimization: A Case Study On Wav2Vec 2.0</a></p>
<ul>
<li><strong>Author</strong>: Liu Chen, Meysam Asgari</li>
</ul>
<p>对 attention heatmap 做了可视化来分析其合理性，值得借鉴</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2007.07834.pdf">INFOXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</a></p>
<ul>
<li><strong>Author</strong>: Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, Ming Zhou</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this work, we present an informationtheoretic framework that formulates crosslingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.11524.pdf">slimIPL: Language-Model-Free Iterative Pseudo-Labeling</a></p>
<ul>
<li><strong>Author</strong>: Tatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, Ronan Collobert</li>
</ul>
<p>Speech Recognition</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.03737.pdf">Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals</a></p>
<ul>
<li><strong>Author</strong>: Gaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, Ji-Rong Wen</li>
<li><strong>Comments</strong>: WSDM 2021</li>
</ul>
<p>In our approach, the student network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network.</p>
<p>​</p>
<p>​</p>
<h2 id="5-apr-21----6-apr-21">5 Apr 21 &ndash; 6 Apr 21</h2>
<p>更新于 8 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.02112.pdf">Efficient Attentions for Long Document Summarization</a></p>
<ul>
<li><strong>Author</strong>: Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang</li>
<li><strong>Comments</strong>: NAACL 2021 long paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.02205.pdf">Attention Head Masking for Inference Time Content Selection in Abstractive Summarization</a></p>
<ul>
<li><strong>Author</strong>: Shuyang Cao, Lu Wang</li>
<li><strong>Comments</strong>: NAACL 2021 short paper</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.02145.pdf">What Will it Take to Fix Benchmarking in Natural Language Understanding?</a></p>
<ul>
<li><strong>Author</strong>: Samuel R. Bowman, George E. Dahl</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this position paper, we lay out four criteria that we argue NLU benchmarks should meet.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.02484.pdf">OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation</a></p>
<ul>
<li><strong>Author</strong>: Petr Marek, Vishal Ishwar Naik, Vincent Auvray, Anuj Goyal</li>
<li><strong>Comments</strong>: NAACL 2021 Industry track</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.02689.pdf">A Student-Teacher Architecture for Dialog Domain Adaptation under the Meta-Learning Setting</a></p>
<ul>
<li><strong>Author</strong>: Kun Qian, Wei Wei, Zhou Yu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.02704.pdf">Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, Furu Wei</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14614.pdf">Knowledge Injection into Dialogue Generation via Language Models</a></p>
<ul>
<li><strong>Author</strong>: Yi-Lin Tuan, Wei Wei, William Yang Wang</li>
</ul>
<p>模型和优化目标有点像 CVAE</p>
<p>​</p>
<p>​</p>
<h2 id="2-apr-21----5-apr-21">2 Apr 21 &ndash; 5 Apr 21</h2>
<p>更新于 7 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.01264.pdf">Attention Forcing for Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Qingyun Dou, Yiting Lu, Potsawee Manakul, Xixin Wu, Mark J. F. Gales</li>
</ul>
<p>This paper introduces attention forcing for NMT. This approach guides the model with the generated output history and reference attention, and can reduce the training-inference mismatch without a schedule or a classifier.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01477.pdf">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</a></p>
<ul>
<li><strong>Author</strong>: Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar</li>
</ul>
<p>BERT Probing</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01569.pdf">Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks</a></p>
<ul>
<li><strong>Author</strong>: Endri Kacupaj, Joan Plepi, Kuldeep Singh, Harsh Thakkar, Jens Lehmann, Maria Maleshkova</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01724.pdf">Inference Time Style Control for Summarization</a></p>
<ul>
<li><strong>Author</strong>: Shuyang Cao, Lu Wang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01767.pdf">WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</a></p>
<ul>
<li><strong>Author</strong>: Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, Nan Duan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01853.pdf">Rethinking Perturbations in Encoder-Decoders for Fast Training</a></p>
<ul>
<li><strong>Author</strong>: Sho Takase, Shun Kiyono</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>We compare several perturbations in sequence-to-sequence problems with respect to computational time.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01940.pdf">What’s the best place for an AI conference, Vancouver or : Why completing comparative questions is difficult</a></p>
<ul>
<li><strong>Author</strong>: Avishai Zagoury, Einat Minkov, Idan Szpektor, William W. Cohen</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.07968.pdf">SKIM: Few-Shot Conversational Semantic Parsers with Formal Dialogue Contexts</a></p>
<ul>
<li><strong>Author</strong>: Giovanni Campagna, Sina J. Semnani, Ryan Kearns, Lucas Jun Koba Sato, Silei Xu, Monica S. Lam</li>
</ul>
<p>This paper proposes to replace the utterances before the current turn with a formal representation, which is used as the context in a semantic parser mapping the current user utterance to its formal meaning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12770.pdf">Conversational Semantic Parsing for Dialog State Tracking</a></p>
<ul>
<li><strong>Author</strong>: Jianpeng Cheng et al.</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.01403.pdf">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a></p>
<ul>
<li><strong>Author</strong>: Beliz Gunel, Jingfei Du, Alexis Conneau, Ves Stoyanov</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.06779.pdf">Few Shot Dialogue State Tracking using Meta-learning</a></p>
<ul>
<li><strong>Author</strong>: Saket Dingliwal, Shuyang Gao, Sanchit Agarwal, Chien-Wei Lin, Tagyoung Chung, Dilek Hakkani-Tur</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-apr-21----2-apr-21">1 Apr 21 &ndash; 2 Apr 21</h2>
<p>更新于 6 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.00773.pdf">MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Fanghua Ye, Jarana Manotumruksa, Emine Yilmaz</li>
</ul>
<p>This work introduces <a href="https://github.com/smartyfh/MultiWOZ2.4">MultiWOZ 2.4</a> , in which we refine all annotations in the validation set and test set on top of MultiWOZ 2.1.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.00743.pdf">Towards General Purpose Vision Systems</a></p>
<ul>
<li><strong>Author</strong>: Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem</li>
</ul>
<p>In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1910.10488.pdf">Injecting Hierarchy with U-Net Transformers</a></p>
<ul>
<li><strong>Author</strong>: David Donahue, Vladislav Lialin, Anna Rumshisky</li>
</ul>
<p>In the present work, we introduce hierarchical processing into the Transformer model, taking inspiration from the UNet architecture.</p>
<p>This work experiments on Cornell Movie Dialogues corpus and the PersonaChat dataset.</p>
<p>​</p>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
