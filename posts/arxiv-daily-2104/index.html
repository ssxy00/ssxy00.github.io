<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Apr 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Apr 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2104/" />
<meta property="article:published_time" content="2021-04-06T08:45:54+08:00" />
<meta property="article:modified_time" content="2021-04-06T08:45:54+08:00" />


    <title>
  Arxiv Daily | Apr 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2104/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Apr 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-04-06T08:45:54&#43;08:00'>
                April 6, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              3-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 4 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="2-apr-21----5-apr-21">2 Apr 21 &ndash; 5 Apr 21</h2>
<p>更新于 7 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.01264.pdf">Attention Forcing for Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Qingyun Dou, Yiting Lu, Potsawee Manakul, Xixin Wu, Mark J. F. Gales</li>
</ul>
<p>This paper introduces attention forcing for NMT. This approach guides the model with the generated output history and reference attention, and can reduce the training-inference mismatch without a schedule or a classifier.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01477.pdf">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</a></p>
<ul>
<li><strong>Author</strong>: Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar</li>
</ul>
<p>BERT Probing</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01569.pdf">Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks</a></p>
<ul>
<li><strong>Author</strong>: Endri Kacupaj, Joan Plepi, Kuldeep Singh, Harsh Thakkar, Jens Lehmann, Maria Maleshkova</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01724.pdf">Inference Time Style Control for Summarization</a></p>
<ul>
<li><strong>Author</strong>: Shuyang Cao, Lu Wang</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01767.pdf">WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach</a></p>
<ul>
<li><strong>Author</strong>: Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, Nan Duan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01853.pdf">Rethinking Perturbations in Encoder-Decoders for Fast Training</a></p>
<ul>
<li><strong>Author</strong>: Sho Takase, Shun Kiyono</li>
<li><strong>Comments</strong>: NAACL-HLT 2021</li>
</ul>
<p>We compare several perturbations in sequence-to-sequence problems with respect to computational time.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.01940.pdf">What’s the best place for an AI conference, Vancouver or : Why completing comparative questions is difficult</a></p>
<ul>
<li><strong>Author</strong>: Avishai Zagoury, Einat Minkov, Idan Szpektor, William W. Cohen</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.07968.pdf">SKIM: Few-Shot Conversational Semantic Parsers with Formal Dialogue Contexts</a></p>
<ul>
<li><strong>Author</strong>: Giovanni Campagna, Sina J. Semnani, Ryan Kearns, Lucas Jun Koba Sato, Silei Xu, Monica S. Lam</li>
</ul>
<p>This paper proposes to replace the utterances before the current turn with a formal representation, which is used as the context in a semantic parser mapping the current user utterance to its formal meaning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12770.pdf">Conversational Semantic Parsing for Dialog State Tracking</a></p>
<ul>
<li><strong>Author</strong>: Jianpeng Cheng et al.</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.01403.pdf">Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning</a></p>
<ul>
<li><strong>Author</strong>: Beliz Gunel, Jingfei Du, Alexis Conneau, Ves Stoyanov</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.06779.pdf">Few Shot Dialogue State Tracking using Meta-learning</a></p>
<ul>
<li><strong>Author</strong>: Saket Dingliwal, Shuyang Gao, Sanchit Agarwal, Chien-Wei Lin, Tagyoung Chung, Dilek Hakkani-Tur</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-apr-21----2-apr-21">1 Apr 21 &ndash; 2 Apr 21</h2>
<p>更新于 6 Apr 2021</p>
<p><a href="https://arxiv.org/pdf/2104.00773.pdf">MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Fanghua Ye, Jarana Manotumruksa, Emine Yilmaz</li>
</ul>
<p>This work introduces <a href="https://github.com/smartyfh/MultiWOZ2.4">MultiWOZ 2.4</a> , in which we refine all annotations in the validation set and test set on top of MultiWOZ 2.1.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.00743.pdf">Towards General Purpose Vision Systems</a></p>
<ul>
<li><strong>Author</strong>: Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem</li>
</ul>
<p>In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1910.10488.pdf">Injecting Hierarchy with U-Net Transformers</a></p>
<ul>
<li><strong>Author</strong>: David Donahue, Vladislav Lialin, Anna Rumshisky</li>
</ul>
<p>In the present work, we introduce hierarchical processing into the Transformer model, taking inspiration from the UNet architecture.</p>
<p>This work experiments on Cornell Movie Dialogues corpus and the PersonaChat dataset.</p>
<p>​</p>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
