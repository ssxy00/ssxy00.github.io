<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Mar 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Mar 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2103/" />
<meta property="article:published_time" content="2021-03-09T18:58:11+08:00" />
<meta property="article:modified_time" content="2021-03-09T18:58:11+08:00" />


    <title>
  Arxiv Daily | Mar 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2103/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Mar 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-03-09T18:58:11&#43;08:00'>
                March 9, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              15-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 3 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="17-mar-21----18-mar-21">17 Mar 21 &ndash; 18 Mar 21</h2>
<p>更新于 20 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.10291.pdf">Smoothing and Shrinking the Sparse Seq2Seq Search Space</a></p>
<ul>
<li><strong>Author</strong>: Ben Peters, André F. T. Martins</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.10360.pdf"><strong>All NLP Tasks Are Generation Tasks: A General Pretraining Framework</strong></a></p>
<ul>
<li><strong>Author</strong>: Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang</li>
</ul>
<p>venience for model development and selection. We propose a novel pretraining framework GLM (General Language Model) to address this challenge. Compared to previous work, our architecture has three major benefits: (1) it performs well on classification, unconditional generation, and conditional generation tasks with one single pretrained model; (2) it outperforms BERT-like models on classification due to improved pretrain-finetune consistency; (3) it naturally handles variable-length blank filling which is crucial for many downstream tasks.</p>
<p>看标题不知道跟 T5 的做法是否相关</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.10385.pdf"><strong>GPT Understands, Too</strong></a></p>
<ul>
<li><strong>Author</strong>: Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang</li>
</ul>
<p>While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.09857.pdf">Value-aware Approximate Attention</a></p>
<ul>
<li><strong>Author</strong>: Ankit Gupta, Jonathan Berant</li>
</ul>
<p>In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.10282.pdf">Modeling the Second Player in Distributionally Robust Optimization</a></p>
<ul>
<li><strong>Author</strong>: Paul Michel, Tatsunori Hashimoto, Graham Neubig</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12872.pdf">Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation</a></p>
<ul>
<li><strong>Author</strong>: Mrigank Raman, Aaron Chan, Siddhant Agarwal, Peifeng Wang, Hansen Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.03539.pdf">UBAR: Towards Fully End-to-End Task-Oriented Dialog System with GPT-2</a></p>
<ul>
<li><strong>Author</strong>: Yunyi Yang, Yunhao Li, Xiaojun Quan</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>UBAR is acquired by fine-tuning the large pretrained unidirectional language model GPT-2 on the sequence of the entire dialog session which is composed of user utterance, belief state, database result, system act, and system response of every dialog turn.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.04698.pdf">Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text</a></p>
<ul>
<li><strong>Author</strong>: Nishtha Madaan, Inkit Padhi, Naveen Panwar, Diptikalyan Saha</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06717.pdf">Mapping the Timescale Organization of Neural Language Models</a></p>
<ul>
<li><strong>Author</strong>: Hsiang-Yun Sherry Chien, Jinhan Zhang, Christopher. J. Honey</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-mar-21----17-mar-21">16 Mar 21 &ndash; 17 Mar 21</h2>
<p>更新于 20 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.09534.pdf">Dialogue History Matters! Personalized Response Selection in Multi-turn Retrieval-based Chatbots</a></p>
<ul>
<li><strong>Author</strong>: Juntao Li, Chang Liu, Chongyang Tao, Zhangming Chan, Dongyan Zhao, Min Zhang, Rui Yan</li>
<li><strong>Comments</strong>: Accepted by ACM Transactions on Information Systems</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.09548.pdf">ENCONTER: Entity Constrained Progressive Sequence Generation via Insertion-based Transformer</a></p>
<ul>
<li><strong>Author</strong>: Lee-Hsun Hsieh, Yang-Yin Lee, Ee-Peng Lim</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.03706.pdf">Learning to Recombine and Resample Data for Compositional Generalization</a></p>
<ul>
<li><strong>Author</strong>: Ekin Akyürek, Afra Feyza Akyürek, Jacob Andreas</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.00593.pdf">MixKD: Towards Efficient Distillation of Large-scale Language Models</a></p>
<ul>
<li><strong>Author</strong>: Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, Lawrence Carin</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-mar-21----16-mar-21">15 Mar 21 &ndash; 16 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.08809.pdf">Robustly Optimized and Distilled Training for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Haytham ElFadeel, Stan Peshterliev</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.08862.pdf">Gumbel-Attention for Multi-modal Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Pengbo Liu, Hailong Cao, Tiejun Zhao</li>
</ul>
<p>Multi-modal machine translation (MMT) improves translation quality by introducing visual information. However, the existing MMT model ignores the problem that the image will bring information irrelevant to the text, causing much noise to the model and affecting the translation quality. In this paper, we propose a novel Gumbel-Attention for multi-modal machine translation, which selects the text-related parts of the image features.</p>
<p>​</p>
<p>​</p>
<h2 id="12-mar-21----15-mar-21">12 Mar 21 &ndash; 15 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.07552.pdf">Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning</a></p>
<ul>
<li><strong>Author</strong>: Jason Wei,Chengyu Huang, Soroush Vosoughi, Yu Cheng, Shiqi Xu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07601.pdf">Approximating How Single Head Attention Learns</a></p>
<ul>
<li><strong>Author</strong>: Charlie Snell, Ruiqi Zhong, Dan Klein, Jacob Steinhardt</li>
</ul>
<p>关于 attention 的研究</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07649.pdf">Improving Diversity of Neural Text Generation via Inverse Probability Weighting</a></p>
<ul>
<li><strong>Author:</strong> Xinran Zhang, Maosong Sun, Jiafeng Liu, Xiaobing Li</li>
</ul>
<p>关于的是 text generation 中的 diversity 问题</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07766.pdf">Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann</li>
<li><strong>Comments</strong>: ESWC'2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07771.pdf">ParaQA: A Question Answering Dataset with Paraphrase Responses for Single-Turn Conversation</a></p>
<ul>
<li><strong>Author</strong>: Endri Kacupaj, Barshana Banerjee, Kuldeep Singh, Jens Lehmann</li>
<li><strong>Comments</strong>: ESWC'2021</li>
</ul>
<p>这篇工作提出了一个数据集 ParaQA:  contains 5000 question-answer pairs with a minimum of two and a maximum of eight unique paraphrased responses for each question.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.08490.pdf">Multi-view Subword Regularization</a></p>
<ul>
<li><strong>Author</strong>: Xinyi Wang, Sebastian Ruder, Graham Neubig</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>本文关注的问题是，在 multilingual 场景下，standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.08545.pdf">A Study of Automatic Metrics for the Evaluation of Natural Language Explanations</a></p>
<ul>
<li><strong>Author</strong>: Miruna-Adriana Clinciu, Arash Eshghi, Helen Hastie</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>We investigate which of the NLG evaluation measures map well to explanations.</p>
<p>We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1910.03833.pdf">Word Embedding Visualization Via Dictionary Learning</a></p>
<ul>
<li><strong>Author</strong>: Juexiao Zhang, Yubei Chen, Brian Cheung, Bruno A Olshausen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.15595.pdf"><strong>Rethinking Positional Encoding in Language Pre-training</strong></a></p>
<ul>
<li><strong>Author</strong>: Guolin Ke, Di He, Tie-Yan Liu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.01466.pdf">Taking Notes on the Fly Helps BERT Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Qiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, Tie-Yan Liu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12272.pdf">Pre-training with Meta Learning for Chinese Word Segmentation</a></p>
<ul>
<li><strong>Author</strong>: Zhen Ke, Liang Shi, Songtao Sun, Erli Meng, Bin Wang, Xipeng Qiu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this paper, we propose a CWS-specific pre-trained model METASEG, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12688.pdf">Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>English Wijidata KG -&gt; natural text</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.08067.pdf">Hierarchical Transformer for Task Oriented Dialog Systems</a></p>
<ul>
<li><strong>Author</strong>: Bishal Santra, Potnuru Anusha, Pawan Goyal</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06874.pdf">CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</a></p>
<ul>
<li><strong>Author</strong>: Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting</li>
</ul>
<p>In this paper, we present CANINE, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy with soft inductive biases in place of hard token boundaries.</p>
<p>​</p>
<p>​</p>
<h2 id="11-mar-21----12-mar-21">11 Mar 21 &ndash; 12 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="">Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models</a></p>
<ul>
<li><strong>Author</strong>: Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, Xia Hu</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this work, we show that the shortcut learning behavior can be explained by the long-tailed phenomenon.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.07170.pdf">Constrained Text Generation with Global Guidance – Case Study on CommonGen</a></p>
<ul>
<li><strong>Author</strong>: Yixian Lu, Liwen Zhang, Wenjuan Han, Yue Zhang, Kewei Tu</li>
</ul>
<p>In this paper, we consider using reinforcement learning to address the limitation, measuring global constraints including fluency, common sense and concept coverage with a comprehensive score, which serves as the reward for reinforcement learning.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06410.pdf">MEDIASUM: A Large-scale Media Interview Dataset for Dialogue Summarization</a></p>
<ul>
<li><strong>Author</strong>: Chenguang Zhu, Yang Liu, Jie Mei, Michael Zeng</li>
</ul>
<p>这篇文章提出了一个数据集 MEDIASUM: a largescale media interview dataset consisting of 463.6K transcripts with abstractive summaries.</p>
<p>Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains.</p>
<p>​</p>
<p>​</p>
<h2 id="10-mar-21----11-mar-21">10 Mar 21 &ndash; 11 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.06332.pdf">Hurdles to Progress in Long-form Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Kalpesh Krishna, Aurko Roy, Mohit Iyyer</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06370.pdf">Causal-aware Safe Policy Improvement for Task-oriented dialogue</a></p>
<ul>
<li><strong>Author</strong>: Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, Caiming Xiong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06434.pdf">Topical Language Generation using Transformers</a></p>
<ul>
<li><strong>Author</strong>: Rohola Zandie, Mohammad H. Mahoor</li>
<li><strong>Comments</strong>: Accepted in the Journal of Natural Language Engineering</li>
</ul>
<p>This paper presents a novel approach for Topical Language Generation (TLG) by combining a pre-trained LM with topic modeling information. We cast the problem using Bayesian probability formulation with topic probabilities as a prior, LM probabilities as the likelihood, and topical language generation probability as the posterior.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06500.pdf">Conversational Answer Generation and Factuality for Reading Comprehension Question-Answering</a></p>
<ul>
<li><strong>Author</strong>: Stan Peshterliev, Barlas Oguz, Debojeet Chatterjee, Hakan Inan, Vikas Bhardwaj</li>
</ul>
<p>In this work, we investigate conversational answer generation for QA. We propose AnswerBART, an end-to-end generative RC model which combines answer generation from multiple passages with passage ranking and answerability.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.06648.pdf">Domain State Tracking for a Simplified Dialogue System</a></p>
<ul>
<li><strong>Author</strong>: Hyunmin Jeon, Gary Geunbae Lee</li>
</ul>
<p>In this paper, we present DoTS (Domain State Tracking for a Simplified Dialogue System), a task-oriented dialogue system that uses a <strong>simplified input context</strong> instead of the entire dialogue history. However, neglecting the dialogue history can result in a loss of contextual information from previous conversational turns. To address this issue, DoTS tracks the domain state in addition to the belief state and uses it for the input context.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.05987.pdf">Revisiting Few-sample BERT Fine-tuning</a></p>
<ul>
<li><strong>Author</strong>: Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, Yoav Artzi</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2008.08995.pdf">AutoKG: Constructing Virtual Knowledge Graphs from Unstructured Documents for Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Seunghak Yu, Tianxing He, James Glass</li>
</ul>
<p>unstructured documents -&gt; KG -&gt; QA</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.10229.pdf">An Empirical Study on Neural Keyphrase Generation</a></p>
<ul>
<li><strong>Author</strong>: Rui Meng, Xingdi Yuan, Tong Wang, Sanqiang Zhao, Adam Trischler, Daqing He</li>
<li><strong>Comments</strong>: NAACL 2021</li>
</ul>
<p>In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.11934.pdf">mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer</a></p>
<ul>
<li><strong>Author</strong>: Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.05327.pdf">BERTese: Learning to Speak to BERT</a></p>
<ul>
<li><strong>Author</strong>: Adi Haviv, Jonathan Berant, Amir Globerson</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>In this work, we propose a method for automatically rewriting queries into “BERTese”, a paraphrase query that is directly optimized towards better knowledge extraction.</p>
<p>​</p>
<p>​</p>
<h2 id="9-mar-21----10-mar-21">9 Mar 21 &ndash; 10 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.05825.pdf">ELLA: Exploration through Learned Language Abstraction</a></p>
<ul>
<li><strong>Author</strong>: Suvir Mirchandani, Siddharth Karamcheti, Dorsa Sadigh</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.05951.pdf">Self-Learning for Zero Shot Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Surafel M. Lakew, Matteo Negri, Marco Turchi</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.00529.pdf">VinVL: Revisiting Visual Representations in Vision-Language Models</a></p>
<ul>
<li><strong>Author</strong>: Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao</li>
<li><strong>Comments</strong>: CVPR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-mar-21----9-mar-21">8 Mar 21 &ndash; 9 Mar 21</h2>
<p>更新于 17 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.05070.pdf">Text Simplification by Tagging</a></p>
<ul>
<li><strong>Author</strong>: Kostiantyn Omelianchuk, Vipul Raheja, Oleksandr Skurzhanskyi</li>
<li><strong>Comments</strong>: BEA @ EACL 2021</li>
</ul>
<p>Text Simplification is the task of rewriting text into a form that is easier to read and understand while preserving its underlying meaning and information.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.05284.pdf">Open-book Video Captioning with Retrieve-Copy-Generate Network</a></p>
<ul>
<li><strong>Author</strong>: Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng, Weiming Hu</li>
<li><strong>Comments</strong>: CVPR 2021</li>
</ul>
<p>Open-book Video Captioning: generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.03535.pdf">CoCon: A Self-Supervised Approach for Controlled Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang, Jie Fu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>Here, we propose Content-Conditioner (CoCon) to control an LM’s output text with a content input, at a fine-grained level.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.06207.pdf">Contrastive Triple Extraction with Generative Transformer</a></p>
<ul>
<li><strong>Author</strong>: Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, Huajun Chen</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.14794.pdf"><strong>Rethinking Attention with Performers</strong></a></p>
<ul>
<li><strong>Author</strong>: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="5-mar-21----8-mar-21">5 Mar 21 &ndash; 8 Mar 21</h2>
<p>更新于 18 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.04350.pdf">Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees</a></p>
<ul>
<li><strong>Author</strong>: Jiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, Yunhai Tong</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14602.pdf">Look at the First Sentence: Position Bias in Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, Jaewoo Kang</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2005.00719.pdf">Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?</a></p>
<ul>
<li><strong>Author</strong>: Abhilasha Ravichander, Yonatan Belinkov, Eduard Hovy</li>
<li><strong>Comments</strong>: EACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2006.10369.pdf"><strong>Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation</strong></a></p>
<ul>
<li><strong>Author</strong>: Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, Noah A. Smith</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01345.pdf">Emotion Dynamics in Movie Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Will E. Hipson, Saif M. Mohammad</li>
</ul>
<p>Emotion dynamics is a framework for measuring how an individual’s emotions change over time.</p>
<p>​</p>
<p>​</p>
<h2 id="4-mar-21----5-mar-21">4 Mar 21 &ndash; 5 Mar 21</h2>
<p>更新于 18 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.03457.pdf">IOT: Instance-wise Layer Reordering for Transformer Structures</a></p>
<ul>
<li><strong>Author</strong>: Jinhua Zhu, Lijun Wu, Yingce Xia, Shufang Xie, Tao Qin, Wengang Zhou, Houqiang Li, Tie-Yan Liu</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>We observe that different data samples actually favor different orders of the layers. Based on this observation, in this work, we break the assumption of the fixed layer order in Transformer and introduce instance-wise layer reordering into model structure.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.03583.pdf">Graph-Based Tri-Attention Network for Answer Ranking in CQA</a></p>
<ul>
<li><strong>Author</strong>: Wei Zhang, Zeyuan Chen, Chao Dong, Wen Wang, Hongyuan Zha, Jianyong Wang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.02548.pdf">NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven Conversation</a></p>
<ul>
<li><strong>Author</strong>: Xiaoyang Wang, Chen Li, Jianqiao Zhao, Dong Yu</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>中文对话数据集</p>
<p>In this paper, we propose a Chinese multi-turn topic-driven conversation dataset, NaturalConv, which allows the participants to chat anything they want as long as any element from the topic is mentioned and the topic shift is smooth. Our corpus contains 19.9K conversations from six domains, and 400K utterances with an average turn number of 20.1.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.08492.pdf">Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks</a></p>
<ul>
<li><strong>Author</strong>: Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, Yan Zhang</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-mar-21----4-mar-21">3 Mar 21 &ndash; 4 Mar 21</h2>
<p>更新于 18 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.03125.pdf"><strong>Advances in Multi-turn Dialogue Comprehension: A Survey</strong></a></p>
<ul>
<li><strong>Author</strong>: Zhuosheng Zhang, Hai Zhao</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-mar-21----3-mar-21">2 Mar 21 &ndash; 3 Mar 21</h2>
<p>更新于 18 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.02141.pdf">CogNet: Bridging Linguistic Knowledge, World Knowledge and Commonsense Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Chenhao Wang, Yubo Chen, Zhipeng Xue, Yang Zhou, Jun Zhao</li>
<li><strong>Comments</strong>: AAAI 2021 Demonstrations</li>
</ul>
<p>At present, CogNet integrates 1,000+ semantic frames from linguistic KBs, 20,000,000+ frame instances from world KBs, as well as 90,000+ commonsense assertions from commonsense KBs.</p>
<p>cense. The demo and data are available at <a href="http://cognet.top/">http://cognet.top/</a>.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.02143.pdf">Random Feature Attention</a></p>
<ul>
<li><strong>Author</strong>: Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.02205.pdf">Gradual Fine-Tuning for Low-Resource Domain Adaptation</a></p>
<ul>
<li><strong>Author</strong>: Haoran Xu, Seth Ebner, Mahsa Yarmohammadi, Aaron Steven White, Benjamin Van Durme, Kenton Murray</li>
</ul>
<p>We demonstrate that gradually finetuning in a multi-stage process can yield substantial further gains and can be applied without modifying the model or learning objective.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.02262.pdf">Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>Meta Learning + Curriculum Learning</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.07839.pdf">Text Generation by Learning from Demonstrations</a></p>
<ul>
<li><strong>Author</strong>: Richard Yuanzhe Pang, He He</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>We frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.03752.pdf">CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Yusheng Su, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Peng Li, Jie Zhou, Maosong Sun</li>
</ul>
<p>Given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled instances and original labeled instances to help PLMs capture crucial task-related semantic features.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.00293.pdf">A Simple But Effective Approach to n-shot Task-Oriented Dialogue Augmentation</a></p>
<ul>
<li><strong>Author</strong>: Taha Aksu, Nancy F. Chen, Min-Yen Kan, Liu Zhengyuan</li>
</ul>
<p>Our framework uses the simple idea that each turn-pair in a task-oriented dialogue has a certain function and exploits this idea to mix them creating new dialogues.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2007.03848.pdf">Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers</a></p>
<ul>
<li><strong>Author</strong>: Shijie Geng, Peng Gao, Moitreya Chatterjee, Chiori Hori, Jonathan Le Roux, Yongfeng Zhang, Hongsheng Li, Anoop Cherian</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content.</p>
<p>Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization.</p>
<p>​</p>
<p>​</p>
<h2 id="1-mar-21----2-mar-21">1 Mar 21 &ndash; 2 Mar 21</h2>
<p>更新于 18 Mar 2021</p>
<p><a href="https://arxiv.org/pdf/2103.01287.pdf">A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues</a></p>
<ul>
<li><strong>Author</strong>: Ziming Li, Dookun Park, Julia Kiseleva, Young-Bum Kim, Sungjin Lee</li>
</ul>
<p>We assume the user has an initial interaction budget for a conversation based on the task complexity, and each dialogue turn has a cost. When the task is completed or the budget has been run out, the user will quit the interaction.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01378.pdf">Contrastive Explanations for Model Interpretability</a></p>
<ul>
<li><strong>Author</strong>: Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg</li>
</ul>
<p>​</p>
<p><a href="">Towards Efficiently Diversifying Dialogue Generation via Embedding Augmentation</a></p>
<ul>
<li><strong>Author</strong>: Yu Cao, Liang Ding, Zhiliang Tian, Meng Fang</li>
<li><strong>Comments</strong>: ICASSP2021</li>
</ul>
<p>在 PersonaChat 和 DailyDialog 上做了实验</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01819.pdf">The Rediscovery Hypothesis: Language Models Need to Meet Linguistics</a></p>
<ul>
<li><strong>Author</strong>: Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova, Matthias Gallé, Zhenisbek Assylbekov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2004.14992.pdf">How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking</a></p>
<ul>
<li><strong>Author</strong>: Nicola De Cao, Michael Schlichtkrull, Wilker Aziz, Ivan Titov</li>
<li><strong>Comments</strong>: EMNLP 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.00823.pdf">M6: A Chinese Multimodal Pretrainer</a></p>
<ul>
<li><strong>Author</strong>: Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang</li>
</ul>
<p>中文 T5</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.06561.pdf">Comprehension and Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Pavel Naumov, Kevin Ros</li>
<li><strong>Comments</strong>: AAAI 2021</li>
</ul>
<p>The ability of an agent to comprehend a sentence is tightly connected to the agent’s prior experiences and background knowledge. The paper suggests to interpret comprehension as a modality and proposes a complete bimodal logical system that describes an interplay between comprehension and knowledge modalities.</p>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2103.01209.pdf">Generative Adversarial Transformers</a></p>
<ul>
<li><strong>Author</strong>: Drew A. Hudson, C. Lawrence Zitnick</li>
</ul>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
