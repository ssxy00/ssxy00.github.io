<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Xinyi Shen">
    <meta name="description" content="arxiv Daily Reading">
    <meta name="keywords" content="NLPer">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Arxiv Daily | Dec 2021"/>
<meta name="twitter:description" content="arxiv Daily Reading"/>

    <meta property="og:title" content="Arxiv Daily | Dec 2021" />
<meta property="og:description" content="arxiv Daily Reading" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ssxy00.github.io/posts/arxiv-daily-2112/" />
<meta property="article:published_time" content="2021-12-02T19:52:03+08:00" />
<meta property="article:modified_time" content="2021-12-02T19:52:03+08:00" />


    <title>
  Arxiv Daily | Dec 2021 · Xinyi Shen
</title>

    
      <link rel="canonical" href="http://ssxy00.github.io/posts/arxiv-daily-2112/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" integrity="sha256-gsmEoJAws/Kd3CjuOQzLie5Q3yshhvmo7YNtBG7aaEY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="/css/coder.min.f01c647a0d25b40da992a37c3376291185eed8a50ced8c26cc2c0bcfe38c97df.css" integrity="sha256-8Bxkeg0ltA2pkqN8M3YpEYXu2KUM7YwmzCwLz&#43;OMl98=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="generator" content="Hugo 0.62.2" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Xinyi Shen
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>



      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Arxiv Daily | Dec 2021</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-12-02T19:52:03&#43;08:00'>
                December 2, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              15-minute read
            </span>
          </div>
          <div class="authors">
    <i class="fa fa-user" aria-hidden="true"></i>
      <a href="/authors/xinyi-shen/">Xinyi Shen</a></div>
          
          
        </div>
      </header>

      <div>
        
        <p>这篇博客记录的是 2021 年 12 月内刷到的 <a href="https://arxiv.org/list/cs.CL/recent">Arxiv cs.CL</a> 感兴趣的文献。</p>
<h2 id="22-dec-21----23-dec-21">22 Dec 21 &ndash; 23 Dec 21</h2>
<p>更新于  25 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.12389.pdf">S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation</a></p>
<ul>
<li><strong>Author</strong>: Chen Liang, Chong Yang, Jing Xu, Junyang Huang, Yongliang Wang, Yang Dong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.12731.pdf">ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation</a></p>
<ul>
<li><strong>Author</strong>: Shuohuan Wang et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.00400.pdf">Sub-Character Tokenization for Chinese Pretrained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Chenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao Qi, Xiaozhi Wang, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="21-dec-21----22-dec-21">21 Dec 21 &ndash; 22 Dec 21</h2>
<p>更新于  25 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.11632.pdf">Diformer: Directional Transformer for Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Minghan Wang, Jiaxin Guo, Yuxia Wang, Daimeng Wei, Hengchao Shang, Chang Su, Yimeng Chen, Yinglu Li, Min Zhang, Shimin Tao, Hao Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11640.pdf">Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Jiaxin Guo, Minghan Wang, Daimeng Wei, Hengchao Shang, Yuxia Wang, Zongyao Li, Zhengzhe Yu, Zhanglin Wu, Yimeng Chen, Chang Su, Min Zhang, Lizhi Lei, Shimin Tao, Hao Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11642.pdf">Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models</a></p>
<ul>
<li><strong>Author</strong>: Zhengzhe Yu, Jiaxin Guo, Minghan Wang, Daimeng Wei, Hengchao Shang, Zongyao Li, Zhanglin Wu, Yuxia Wang, Yimeng Chen, Chang Su, Min Zhang, Lizhi Lei, Shimin Tao, Hao Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11668.pdf">How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?</a></p>
<ul>
<li><strong>Author</strong>: Xinshuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11718.pdf">Hybrid Curriculum Learning for Emotion Recognition in Conversation</a></p>
<ul>
<li><strong>Author</strong>: Lin Yang, Yi Shen, Yue Mao, Longjun Cai</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11911.pdf">Towards Interactive Language Modeling</a></p>
<ul>
<li><strong>Author</strong>: Maartje ter Hoeve, Evgeny Kharitonov, Dieuwke Hupkes, Emmanuel Dupoux</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04366.pdf">Towards a Unified View of Parameter-Efficient Transfer Learning</a></p>
<ul>
<li><strong>Author</strong>: Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="20-dec-21----21-dec-21">20 Dec 21 &ndash; 21 Dec 21</h2>
<p>更新于  22 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.11202.pdf">Contrast and Generation Make BART a Good Dialogue Emotion Recognizer</a></p>
<ul>
<li><strong>Author</strong>: Shimin Li, Hang Yan, Xipeng Qiu</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11446.pdf">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a></p>
<ul>
<li><strong>Author</strong>: Jack W. Rae et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.11070.pdf">An Inference Approach To Question Answering Over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Aayushee Gupta, Annervaz K M, Ambedkar Dukkipati, Shubhashis Sengupta</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.02486.pdf">Towards General Natural Language Understanding with Probabilistic Worldbuilding</a></p>
<ul>
<li><strong>Author</strong>: Abulhair Saparov, Tom M. Mitchell</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.14207.pdf">How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI</a></p>
<ul>
<li><strong>Author</strong>: Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, Peter Clark</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="17-dec-21----20-dec-21">17 Dec 21 &ndash; 20 Dec 21</h2>
<p>更新于  21 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.10424.pdf">Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction</a></p>
<ul>
<li><strong>Author</strong>: Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.10508.pdf">Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP</a></p>
<ul>
<li><strong>Author</strong>: Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, Samson Tan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.10543.pdf">Spiral Language Modeling</a></p>
<ul>
<li><strong>Author</strong>: Yong Cao, Yukun Feng, Shaohui Kuang, Gu Xu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.10668.pdf">Few-shot Learning with Multilingual Language Models</a></p>
<ul>
<li><strong>Author</strong>: Xi Victoria Lin et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.10684.pdf">Efficient Large Scale Language Modeling with Mixtures of Experts</a></p>
<ul>
<li><strong>Author</strong>: Mikel Artetxe et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.07176.pdf">SAS: Self-Augmentation Strategy for Language Model Pre-training</a></p>
<ul>
<li><strong>Author</strong>: Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, Ying Nian Wu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.07140.pdf">On the Universality of Deep Contextual Language Models</a></p>
<ul>
<li><strong>Author</strong>: Shaily Bhatt, Poonam Goyal, Sandipan Dandapat, Monojit Choudhary, Sunayana Sitaram</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.02080.pdf">An Explanation of In-context Learning as Implicit Bayesian Inference</a></p>
<ul>
<li><strong>Author</strong>: Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="16-dec-21----17-dec-21">16 Dec 21 &ndash; 17 Dec 21</h2>
<p>更新于  21 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.09174.pdf">Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and Explanations</a></p>
<ul>
<li><strong>Author</strong>: Hui Shi, Sicun Gao, Yuandong Tian, Xinyun Chen, Jishen Zhao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.09669.pdf">Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations</a></p>
<ul>
<li><strong>Author</strong>: Siddhant Arora, Danish Pruthi, Norman Sadeh, William W. Cohen, Zachary C. Lipton, Graham Neubig</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.09153.pdf">An Empirical Investigation of the Role of Pre-training in Lifelong Learning</a></p>
<ul>
<li><strong>Author</strong>: Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2012.00893.pdf">Evaluating Explanations: How much do explanations from the teacher aid students?</a></p>
<ul>
<li><strong>Author</strong>: Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</li>
<li><strong>Comments</strong>: TACL 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="15-dec-21----16-dec-21">15 Dec 21 &ndash; 16 Dec 21</h2>
<p>更新于  19 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.08558.pdf">CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning</a></p>
<ul>
<li><strong>Author</strong>: Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Gaurav Singh Tomar</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08560.pdf">Block-Skim: Efficient Question Answering for Transformer</a></p>
<ul>
<li><strong>Author</strong>: Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, Minyi Guo, Yuhao Zhu</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08583.pdf">Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Ian Porada, Alessandro Sordoni, Jackie Chi Kit Cheung</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08608.pdf">QuALITY: Question Answering with Long Input Texts, Yes!</a></p>
<ul>
<li><strong>Author</strong>: Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel R. Bowman</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08615.pdf">Commonsense Knowledge-Augmented Pretrained Language Models for Causal Reasoning Classification</a></p>
<ul>
<li><strong>Author</strong>: Pedram Hosseini, David A. Broniatowski, Mona Diab</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08619.pdf">Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge</a></p>
<ul>
<li><strong>Author</strong>: Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Donghoon Shin, Seungryong Kim, Heuiseok Lim</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08633.pdf">Learning To Retrieve Prompts for In-Context Learning</a></p>
<ul>
<li><strong>Author</strong>: Ohad Rubin, Jonathan Herzig, Jonathan Berant</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08688.pdf">Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks</a></p>
<ul>
<li><strong>Author</strong>: Akari Asai, Matt Gardner, Hannaneh Hajishirzi</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08709.pdf">DOCmT5: Document-Level Pretraining of Multilingual Language Models</a></p>
<ul>
<li><strong>Author</strong>: Chia-Hsuan Lee, Aditya Siddhant, Viresh Ratnakar, Melvin Johnson</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08713.pdf">CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning</a></p>
<ul>
<li><strong>Author</strong>: Xiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08777.pdf">Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Avi Caciularu, Ido Dagan, Jacob Goldberger, Arman Cohan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08802.pdf">UNIREX: A Unified Learning Framework for Language Model Rationale Extraction</a></p>
<ul>
<li><strong>Author</strong>: Aaron Chan, Maziar Sanjabi, Lambert Mathias, Liang Tan, Shaoliang Nie, Xiaochang Peng, Xiang Ren, Hamed Firooz</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08812.pdf">Ditch the Gold Standard: Re-evaluating Conversational Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Huihan Li, Tianyu Gao, Manan Goenka, Danqi Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08907.pdf">Inherently Explainable Reinforcement Learning in Natural Language</a></p>
<ul>
<li><strong>Author</strong>: Xiangyu Peng, Mark O. Riedl, Prithviraj Ammanabrolu</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="14-dec-21----15-dec-21">14 Dec 21 &ndash; 15 Dec 21</h2>
<p>更新于  19 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.07924.pdf">Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation</a></p>
<ul>
<li><strong>Author</strong>: Yu Li, Baolin Peng, Yelong Shen, Yi Mao, Lars Liden, Zhou Yu, Jianfeng Gao</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.07916.pdf">LongT5: Efficient Text-To-Text Transformer for Long Sequences</a></p>
<ul>
<li><strong>Author</strong>: Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08152.pdf">Faster Nearest Neighbor Machine Translation</a></p>
<ul>
<li><strong>Author</strong>: Shuhe Wangr, Jiwei Li, Yuxian Meng, Rongbin Ouyang, Guoyin Wang, Xiaoya Li, Tianwei Zhang, Shi Zong</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08266.pdf">KGR4 : Retrieval, Retrospect, Refine and Rethink for Commonsense Generation</a></p>
<ul>
<li><strong>Author</strong>: Xin Liu, Dayiheng Liu, Baosong Yang, Haibo Zhang, Junwei Ding, Wenqing Yao, Weihua Luo, Haiying Zhang, Jinsong Su</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08326.pdf">Is “my favorite new movie” my favorite movie? Probing the Understanding of Recursive Noun Phrases</a></p>
<ul>
<li><strong>Author</strong>: Qing Lyu, Hua Zheng, Daoxin Li, Li Zhang, Marianna Apidianaki, Chris Callison-Burch</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.08342.pdf">DG2 : Data Augmentation Through Document Grounded Dialogue Generation</a></p>
<ul>
<li><strong>Author</strong>: Qingyang Wu, Song Feng, Derek Chen, Sachindra Joshi, Luis A. Lastras, Zhou Yu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.07899.pdf">Large Dual Encoders Are Generalizable Retrievers</a></p>
<ul>
<li><strong>Author</strong>: Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, Yinfei Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.01156.pdf">Challenges in Generalization in Open Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Linqing Liu, Patrick Lewis, Sebastian Riedel, Pontus Stenetorp</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.02794.pdf">Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, Brenden M. Lake</li>
<li><strong>Comments</strong>:  NeurIPS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="13-dec-21----14-dec-21">13 Dec 21 &ndash; 14 Dec 21</h2>
<p>更新于  17 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.07194.pdf">MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhang, Luis Fernando D’Haro, Thomas Friedrichs, Haizhou Li</li>
<li><strong>Comments</strong>: AAAI2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.07210.pdf">Simple Local Attentions Remain Competitive for Long-Context Tasks</a></p>
<ul>
<li><strong>Author</strong>: Wenhan Xiong, Barlas Oğuz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Wen-tau Yih, Yashar Mehdad</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.07327.pdf">Model Uncertainty–Aware Knowledge Amalgamation for Pre-Trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Lei Li, Yankai Lin, Xuancheng Ren, Guangxiang Zhao, Peng Li, Jie Zhou, Xu Sun</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.07381.pdf">You Only Need One Model for Open-domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, Kyoung-Gu Woo</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="10-dec-21----13-dec-21">10 Dec 21 &ndash; 13 Dec 21</h2>
<p>更新于  14 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.05785.pdf">TempoQR: Temporal Question Reasoning over Knowledge Graphs</a></p>
<ul>
<li><strong>Author</strong>: Costas Mavromatis, Prasanna Lakkur Subramanyam, Vassilis N. Ioannidis, Soji Adeshina, Phillip R. Howard, Tetiana Grinberg, Nagib Hakim, George Karypis</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.05787.pdf">Representation Learning for Conversational Data using Discourse Mutual Information Maximization</a></p>
<ul>
<li><strong>Author</strong>: Bishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, Pawan Goyal</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.05843.pdf">Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an Identity</a></p>
<ul>
<li><strong>Author</strong>: Kurt Shuster, Jack Urbanek, Arthur Szlam, Jason Weston</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06295.pdf">Towards More Efficient Insertion Transformer with Fractional Positional Encoding</a></p>
<ul>
<li><strong>Author</strong>: Zhisong Zhang, Yizhe Zhang, Bill Dolan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06318.pdf">Contextualized Scene Imagination for Generative Commonsense Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Peifeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06346.pdf">VALUENET: A New Dataset for Human Value Driven Dialogue System</a></p>
<ul>
<li><strong>Author</strong>: Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, Song-Chun Zhu</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06386.pdf">Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification</a></p>
<ul>
<li><strong>Author</strong>: Yinhua Piao, Sangseon Lee, Dohoon Lee, Sun Kim</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06494.pdf">Native Chinese Reader: A Dataset Towards Native-Level Chinese Machine Reading Comprehension</a></p>
<ul>
<li><strong>Author</strong>: Shusheng Xu, Yichen Liu, Xiaoyu Yi, Siyuan Zhou, Huizi Li, Yi Wu</li>
<li><strong>Comments</strong>: NeurIPS 2021 Track on Datasets and Benchmarks</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06510.pdf">Do Data-based Curricula Work?</a></p>
<ul>
<li><strong>Author</strong>: Maxim K. Surkov, Vladislav D. Mosin, Ivan P. Yamshchikov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06723.pdf">Understanding and Improving the Exemplar-based Generation for Open-domain Conversation</a></p>
<ul>
<li><strong>Author</strong>: Seungju Han, Beomsu Kim, Seokjun Seo, Enkhbayar Erdenee, Buru Chang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06749.pdf">Step-unrolled Denoising Autoencoders for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Nikolay Savinov, Junyoung Chung, Mikołaj Binkowski, Erich Elsen, Aaron van den Oord</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06837.pdf">Sparse Interventions in Language Models with Differentiable Masking</a></p>
<ul>
<li><strong>Author</strong>: Nicola De Cao, Leon Schmid, Dieuwke Hupkes, Ivan Titov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.06905.pdf">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a></p>
<ul>
<li><strong>Author</strong>: Nan Du et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.15135.pdf">Explanation-Based Human Debugging of NLP Models: A Survey</a></p>
<ul>
<li><strong>Author</strong>: Piyawat Lertvittayakumjorn, Francesca Toni</li>
<li><strong>Comments</strong>: TACL</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.06597.pdf">RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling</a></p>
<ul>
<li><strong>Author</strong>: Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan</li>
<li><strong>Comments</strong>: AAAI-22</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.13876.pdf">Rationale-Inspired Natural Language Explanations with Commonsense</a></p>
<ul>
<li><strong>Author</strong>: Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, Julian McAuley</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.04645.pdf">CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems</a></p>
<ul>
<li><strong>Author</strong>: Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang, Qun Liu</li>
<li><strong>Comments</strong>: AAAI2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06961.pdf">Language Modelling via Learning to Rank</a></p>
<ul>
<li><strong>Author</strong>: Arvid Frydenlund, Gagandeep Singh, Frank Rudzicz</li>
<li><strong>Comments</strong>: AAAI22</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.07658.pdf">Calculating Question Similarity is Enough: A New Method for KBQA Tasks</a></p>
<ul>
<li><strong>Author</strong>: Hanyu Zhao, Sha Yuan, Jiahong Leng, Xiang Pan, Guoqiang Wang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.08207.pdf">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></p>
<ul>
<li><strong>Author</strong>: Victor Sanh et al.</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="9-dec-21----10-dec-21">9 Dec 21 &ndash; 10 Dec 21</h2>
<p>更新于  13 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.05717.pdf">Discourse-Aware Prompt Design for Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Marjan Ghazvininejad, Vladimir Karpukhin, Asli Celikyilmaz</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.05359.pdf">Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences</a></p>
<ul>
<li><strong>Author</strong>: Yifan Chen, Qi Zeng, Dilek Hakkani-Tur, Di Jin, Heng Ji, Yun Yang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.05587.pdf">Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation</a></p>
<ul>
<li><strong>Author</strong>: Tianyi Liu, Zuxuan Wu, Wenhan Xiong, Jingjing Chen, Yu-Gang Jiang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.04198.pdf">TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning</a></p>
<ul>
<li><strong>Author</strong>: Yixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan, Lei Shu, Ehsan Shareghi, Nigel Collier</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.04169.pdf">Iterative Decoding for Compositional Generalization in Transformers</a></p>
<ul>
<li><strong>Author</strong>: Luana Ruiz, Joshua Ainslie, Santiago Ontañón</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="8-dec-21----9-dec-21">8 Dec 21 &ndash; 9 Dec 21</h2>
<p>更新于  10 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2012.13577.pdf">LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification</a></p>
<ul>
<li><strong>Author</strong>: Jiangjie Chen, Qiaoben Bao, Changzhi Sun, Xinbo Zhang, Jiaze Chen, Hao Zhou, Yanghua Xiao, Lei Li</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.01040.pdf">Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling</a></p>
<ul>
<li><strong>Author</strong>: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang</li>
<li><strong>Comments</strong>:  ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.09543.pdf">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</a></p>
<ul>
<li><strong>Author</strong>: Pengcheng He, Jianfeng Gao, Weizhu Chen</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="7-dec-21----8-dec-21">7 Dec 21 &ndash; 8 Dec 21</h2>
<p>更新于  9 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.04426.pdf">Improving language models by retrieving from trillions of tokens</a></p>
<ul>
<li><strong>Author</strong>: Sebastian Borgeaud et al.</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08793.pdf">SALKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Aaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal, Tanishq Gupta, Xiang Ren</li>
<li><strong>Comments</strong>:  NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.02643.pdf">Response Generation with Context-Aware Prompt Learning</a></p>
<ul>
<li><strong>Author</strong>: Xiaodong Gu, Kang Min Yoo, Sang-Woo Lee</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="6-dec-21----7-dec-21">6 Dec 21 &ndash; 7 Dec 21</h2>
<p>更新于  8 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.03473.pdf">Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation</a></p>
<ul>
<li><strong>Author</strong>: Thong Nguyen, Luu Anh Tuan</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.03638.pdf">Scaling Structured Inference with Randomization</a></p>
<ul>
<li><strong>Author</strong>: Yao Fu, Mirella Lapata</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2102.12060.pdf">Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing</a></p>
<ul>
<li><strong>Author</strong>: Sarah Wiegreffe, Ana Marasović</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.02192.pdf">Long-Short Transformer: Efficient Transformers for Language and Vision</a></p>
<ul>
<li><strong>Author</strong>: Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, Bryan Catanzaro</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="3-dec-21----6-dec-21">3 Dec 21 &ndash; 6 Dec 21</h2>
<p>更新于  7 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.02505.pdf">Causal Distillation for Language Models</a></p>
<ul>
<li><strong>Author</strong>: Zhengxuan Wu, Atticus Geiger, Josh Rozner, Elisa Kreiss, Hanson Lu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.02706.pdf">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning</a></p>
<ul>
<li><strong>Author</strong>: Zixuan Ke, Bing Liu, Nianzu Ma, Hu Xu, Lei Shu</li>
<li><strong>Comments</strong>:  NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.02732.pdf">JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Yueqing Sun, Qi Shi, Le Qi, Yu Zhang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.02997.pdf">Language Semantics Interpretation with an Interaction-based Recurrent Neural Networks</a></p>
<ul>
<li><strong>Author</strong>: Shaw-Hwa Lo, Yiqiao Yin</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.03154.pdf">VAE based Text Style Transfer with Pivot Words Enhancement Learning</a></p>
<ul>
<li><strong>Author</strong>: Haoran Xu, Sixing Lu, Zhongkai Sun, Chengyuan Ma, Chenlei Guo</li>
<li><strong>Comments</strong>: Accepted at The eighteenth International Conference on Natural Language Processing</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.03204.pdf">Quantifying Adaptability in Pre-trained Language Models with 500 Tasks</a></p>
<ul>
<li><strong>Author</strong>: Belinda Z. Li, Jane Yu, Madian Khabsa, Luke Zettlemoyer, Alon Halevy, Jacob Andreas</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.03254.pdf">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</a></p>
<ul>
<li><strong>Author</strong>: Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.05346.pdf">End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering</a></p>
<ul>
<li><strong>Author</strong>: Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama</li>
<li><strong>Comments</strong>:  NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.11879.pdf">Hybrid Autoregressive Inference for Scalable Multi-hop Explanation Regeneration</a></p>
<ul>
<li><strong>Author</strong>: Marco Valentino, Mokanarangan Thayaparan, Deborah Ferreira, André Freitas</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2107.13377.pdf">Learning to solve complex tasks by growing knowledge culturally across generations</a></p>
<ul>
<li><strong>Author</strong>: Michael Henry Tessler, Jason Madeano, Pedro A. Tsividis, Noah D. Goodman, Joshua B. Tenenbaum</li>
<li><strong>Comments</strong>: NeurIPS 2021 Cooperative AI Workshop</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.05739.pdf">CEM: Commonsense-aware Empathetic Response Generation</a></p>
<ul>
<li><strong>Author</strong>: Sahand Sabour, Chujie Zheng, Minlie Huang</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2109.13059.pdf">Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations</a></p>
<ul>
<li><strong>Author</strong>: Fangyu Liu, Yunlong Jiao, Jordan Massiah, Emine Yilmaz, Serhii Havrylov</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.00653.pdf">SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL</a></p>
<ul>
<li><strong>Author</strong>: Ruichu Cai, Jinjie Yuan, Boyan Xu, Zhifeng Hao</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2111.14210.pdf">Emergent Graphical Conventions in a Visual Communication Game</a></p>
<ul>
<li><strong>Author</strong>: Shuwen Qiu, Sirui Xie, Lifeng Fan, Tao Gao, Song-Chun Zhu, Yixin Zhu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00653.pdf">Variational Learning for Unsupervised Knowledge Grounded Dialogs</a></p>
<ul>
<li><strong>Author</strong>: Mayank Mishra, Dhiraj Madan, Gaurav Pandey, Danish Contractor</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="2-dec-21----3-dec-21">2 Dec 21 &ndash; 3 Dec 21</h2>
<p>更新于  6 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.01753.pdf">Probing Linguistic Information For Logical Inference In Pre-trained Language Models</a></p>
<ul>
<li><strong>Author</strong>: Zeming Chen, Qiyue Gao</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.01716.pdf">Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research</a></p>
<ul>
<li><strong>Author</strong>: Bernard Koch, Emily Denton, Alex Hanna, Jacob G. Foster</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2009.05160.pdf">Rank over Class: The Untapped Potential of Ranking in Natural Language Processing</a></p>
<ul>
<li><strong>Author</strong>: Amir Atapour-Abarghouei, Stephen Bonner, Andrew Stephen McGough</li>
<li><strong>Comments</strong>: IEEE BigData 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00503.pdf">Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph</a></p>
<ul>
<li><strong>Author</strong>: Liyan Xu, Xuchao Zhang, Bo Zong, Yanchi Liu, Wei Cheng, Jingchao Ni, Haifeng Chen, Liang Zhao, Jinho D. Choi</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2101.05779.pdf">Structured Prediction as Translation between Augmented Natural Languages</a></p>
<ul>
<li><strong>Author</strong>: Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, Stefano Soatto</li>
<li><strong>Comments</strong>: ICLR 2021</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="1-dec-21----2-dec-21">1 Dec 21 &ndash; 2 Dec 21</h2>
<p>更新于  3 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.01047.pdf">DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding</a></p>
<ul>
<li><strong>Author</strong>: Taolin Zhang, Chengyu Wang, Nan Hu, Minghui Qiu, Chengguang Tang, Xiaofeng He, Jun Huang</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.01404.pdf">LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training</a></p>
<ul>
<li><strong>Author</strong>: Ningyu Zhang, Hongbin Ye, Jiacheng Yang, Shumin Deng, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang, Huajun Chen</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00791.pdf">Controlling Conditional Language Models with Distributional Policy Gradients</a></p>
<ul>
<li><strong>Author</strong>: Tomasz Korbak, Hady Elsahar, Germán Kruszewski, Marc Dymetman</li>
<li><strong>Comments</strong>: CtrlGen: Controllable Generative Modeling in Language and Vision Workshop at NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00967.pdf">Relational Graph Learning for Grounded Video Description Generation</a></p>
<ul>
<li><strong>Author</strong>: Wenqiao Zhang, Xin Eric Wang, Siliang Tang, Haizhou Shi, Haocheng Shi, Jun Xiao,Yueting Zhuang, William Yang Wang</li>
<li><strong>Comments</strong>: ACM MM 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.01040.pdf">EngineKGI: Closed-Loop Knowledge Graph Inference</a></p>
<ul>
<li><strong>Author</strong>: Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/1910.08293.pdf">ALOHA: Artificial Learning of Human Attributes for Dialogue Agents</a></p>
<ul>
<li><strong>Author</strong>: Aaron W. Li, Veronica Jiang, Steven Y. Feng, Julia Sprague, Wei Zhou, Jesse Hoey</li>
<li><strong>Comments</strong>: AAAI 2020</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2010.12784.pdf">Deep Clustering of Text Representations for Supervision-free Probing of Syntax</a></p>
<ul>
<li><strong>Author</strong>: Vikram Gupta, Haoyue Shi, Kevin Gimpel, Mrinmaya Sachan</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.06669.pdf">NAREOR: The Narrative Reordering Problem</a></p>
<ul>
<li><strong>Author</strong>: Varun Gangal, Steven Y. Feng, Malihe Alikhani, Teruko Mitamura, Eduard Hovy</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2105.03075.pdf">A Survey of Data Augmentation Approaches for NLP</a></p>
<ul>
<li><strong>Author</strong>: Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, Eduard Hovy</li>
<li><strong>Comments</strong>: ACL 2021 Findings</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2108.06643.pdf">SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation</a></p>
<ul>
<li><strong>Author</strong>: Steven Y. Feng, Jessica Huynh, Chaitanya Narisetty, Eduard Hovy, Varun Gangal</li>
<li><strong>Comments</strong>:  INLG 2021 [Best Long Paper]</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.02782.pdf">How BPE Affects Memorization in Transformers</a></p>
<ul>
<li><strong>Author</strong>: Eugene Kharitonov, Marco Baroni, Dieuwke Hupkes</li>
</ul>
<p>​</p>
<p>​</p>
<h2 id="30-nov-21----1-dec-21">30 Nov 21 &ndash; 1 Dec 21</h2>
<p>更新于  2 Dec 2021</p>
<p><a href="https://arxiv.org/pdf/2112.00503.pdf">Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph</a></p>
<ul>
<li><strong>Author</strong>: Liyan Xu, Xuchao Zhang, Bo Zong, Yanchi Liu, Wei Cheng, Jingchao Ni, Haifeng Chen, Liang Zhao, Jinho D. Choi</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00578.pdf">Systematic Generalization with Edge Transformers</a></p>
<ul>
<li><strong>Author</strong>: Leon Bergen, Timothy J. O’Donnell, Dzmitry Bahdanau</li>
<li><strong>Comments</strong>: NeurIPS 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00653.pdf">Variational Learning for Unsupervised Knowledge Grounded Dialogs</a></p>
<ul>
<li><strong>Author</strong>: Mayank Mishra, Dhiraj Madan, Gaurav Pandey, Danish Contractor</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2112.00712.pdf">STEM: Unsupervised STructural EMbedding for Stance Detection</a></p>
<ul>
<li><strong>Author</strong>: Ron Korenblum Pick, Vladyslav Kozhukhov, Dan Vilenchik, Oren Tsur</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2011.00740.pdf">Influence Patterns for Explaining Information Flow in BERT</a></p>
<ul>
<li><strong>Author</strong>: Kaiji Lu, Zifan Wang, Piotr Mardziel, Anupam Datta</li>
<li><strong>Comments</strong>: Neurips 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2104.08646.pdf">Competency Problems: On Finding and Removing Artifacts in Language Data</a></p>
<ul>
<li><strong>Author</strong>: Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis Ross, Sameer Singh, Noah A. Smith</li>
<li><strong>Comments</strong>: EMNLP 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2106.12066.pdf">It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning</a></p>
<ul>
<li><strong>Author</strong>: Alexey Tikhonov, Max Ryabinin</li>
<li><strong>Comments</strong>: Findings of ACL 2021</li>
</ul>
<p>​</p>
<p><a href="https://arxiv.org/pdf/2110.06537.pdf">Well-classified Examples are Underestimated in Classification with Deep Neural</a></p>
<ul>
<li><strong>Author</strong>: Guangxiang Zhao, Wenkai Yang, Xuancheng Ren, Lei Li, Xu Sun</li>
<li><strong>Comments</strong>: AAAI 2022</li>
</ul>
<p>​</p>
<p>​</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2020 -
        
        2021
         Xinyi Shen 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
        
      
    </section>
  </footer>


    </main>

    

    

    

    

    

    
  </body>

</html>
